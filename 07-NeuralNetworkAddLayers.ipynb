{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dataset = dsets.MNIST(root='./data', \n",
    "                            train=True, \n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='./data', \n",
    "                           train=False, \n",
    "                           transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "n_iters = 3000\n",
    "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a model class \n",
    "# input->linear function->non linear function(sigmoid)->linear function->Softmax->CrossEntropy\n",
    "class FeedFowardNeuralNetwork(nn.Module): \n",
    "    def __init__(self,input_dim,hidden_dim,output_dim):\n",
    "        super(FeedFowardNeuralNetwork,self).__init__()\n",
    "        # linear function 784->100\n",
    "        self.fc1=nn.Linear(input_dim,hidden_dim)\n",
    "        # Non linearity 1\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        # linear function 100->100\n",
    "        self.fc2=nn.Linear(hidden_dim,hidden_dim)\n",
    "        # Non linearity 2\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        # linear function (readout) 100->10\n",
    "        self.fc3=nn.Linear(hidden_dim,output_dim)\n",
    "        \n",
    "        \n",
    "    def forward(self,x): \n",
    "        out=self.fc1(x)\n",
    "        out=self.relu1(out)\n",
    "        out=self.fc2(out)\n",
    "        out=self.relu2(out)\n",
    "        out=self.fc3(out)\n",
    "        return out \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the model \n",
    "input_dim=28*28 \n",
    "hidden_dim=100\n",
    "output_dim=10 \n",
    "model=FeedFowardNeuralNetwork(input_dim,hidden_dim,output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# instantiate the loss \n",
    "criterion=nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# instatiate the optimize \n",
    "learning_rate=0.1\n",
    "optimizer=torch.optim.SGD(model.parameters(),lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "torch.Size([100, 784])\n",
      "torch.Size([100])\n",
      "torch.Size([100, 100])\n",
      "torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "print(len(list(model.parameters())))\n",
    "print(list(model.parameters())[0].size())\n",
    "print(list(model.parameters())[1].size())\n",
    "print(list(model.parameters())[2].size())\n",
    "print(list(model.parameters())[3].size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1. Loss: 2.295961856842041\n",
      "Iteration: 2. Loss: 2.3094120025634766\n",
      "Iteration: 3. Loss: 2.2979609966278076\n",
      "Iteration: 4. Loss: 2.3090195655822754\n",
      "Iteration: 5. Loss: 2.299222707748413\n",
      "Iteration: 6. Loss: 2.2961041927337646\n",
      "Iteration: 7. Loss: 2.296984910964966\n",
      "Iteration: 8. Loss: 2.286041021347046\n",
      "Iteration: 9. Loss: 2.2845804691314697\n",
      "Iteration: 10. Loss: 2.2931692600250244\n",
      "Iteration: 11. Loss: 2.2816431522369385\n",
      "Iteration: 12. Loss: 2.2820863723754883\n",
      "Iteration: 13. Loss: 2.2977688312530518\n",
      "Iteration: 14. Loss: 2.2785065174102783\n",
      "Iteration: 15. Loss: 2.286782741546631\n",
      "Iteration: 16. Loss: 2.2735514640808105\n",
      "Iteration: 17. Loss: 2.2741024494171143\n",
      "Iteration: 18. Loss: 2.288369655609131\n",
      "Iteration: 19. Loss: 2.261913537979126\n",
      "Iteration: 20. Loss: 2.256812334060669\n",
      "Iteration: 21. Loss: 2.2638347148895264\n",
      "Iteration: 22. Loss: 2.258091688156128\n",
      "Iteration: 23. Loss: 2.253525495529175\n",
      "Iteration: 24. Loss: 2.263747453689575\n",
      "Iteration: 25. Loss: 2.242072343826294\n",
      "Iteration: 26. Loss: 2.238617181777954\n",
      "Iteration: 27. Loss: 2.2395715713500977\n",
      "Iteration: 28. Loss: 2.220460891723633\n",
      "Iteration: 29. Loss: 2.2344212532043457\n",
      "Iteration: 30. Loss: 2.2318408489227295\n",
      "Iteration: 31. Loss: 2.2131528854370117\n",
      "Iteration: 32. Loss: 2.2154603004455566\n",
      "Iteration: 33. Loss: 2.1986501216888428\n",
      "Iteration: 34. Loss: 2.1980197429656982\n",
      "Iteration: 35. Loss: 2.200122356414795\n",
      "Iteration: 36. Loss: 2.1929311752319336\n",
      "Iteration: 37. Loss: 2.1800267696380615\n",
      "Iteration: 38. Loss: 2.1753487586975098\n",
      "Iteration: 39. Loss: 2.1588234901428223\n",
      "Iteration: 40. Loss: 2.164034843444824\n",
      "Iteration: 41. Loss: 2.150467872619629\n",
      "Iteration: 42. Loss: 2.162837028503418\n",
      "Iteration: 43. Loss: 2.1272902488708496\n",
      "Iteration: 44. Loss: 2.1296324729919434\n",
      "Iteration: 45. Loss: 2.126096248626709\n",
      "Iteration: 46. Loss: 2.1070728302001953\n",
      "Iteration: 47. Loss: 2.077420711517334\n",
      "Iteration: 48. Loss: 2.042891025543213\n",
      "Iteration: 49. Loss: 2.056074619293213\n",
      "Iteration: 50. Loss: 2.047966957092285\n",
      "Iteration: 51. Loss: 1.9989922046661377\n",
      "Iteration: 52. Loss: 1.9810127019882202\n",
      "Iteration: 53. Loss: 2.009077787399292\n",
      "Iteration: 54. Loss: 1.975080132484436\n",
      "Iteration: 55. Loss: 1.9575384855270386\n",
      "Iteration: 56. Loss: 1.9643745422363281\n",
      "Iteration: 57. Loss: 1.8665080070495605\n",
      "Iteration: 58. Loss: 1.8972336053848267\n",
      "Iteration: 59. Loss: 1.8330047130584717\n",
      "Iteration: 60. Loss: 1.8392736911773682\n",
      "Iteration: 61. Loss: 1.7894047498703003\n",
      "Iteration: 62. Loss: 1.7800228595733643\n",
      "Iteration: 63. Loss: 1.7766139507293701\n",
      "Iteration: 64. Loss: 1.7299498319625854\n",
      "Iteration: 65. Loss: 1.6891560554504395\n",
      "Iteration: 66. Loss: 1.726497769355774\n",
      "Iteration: 67. Loss: 1.631386160850525\n",
      "Iteration: 68. Loss: 1.6593807935714722\n",
      "Iteration: 69. Loss: 1.5875234603881836\n",
      "Iteration: 70. Loss: 1.6069958209991455\n",
      "Iteration: 71. Loss: 1.5712385177612305\n",
      "Iteration: 72. Loss: 1.4505921602249146\n",
      "Iteration: 73. Loss: 1.536737322807312\n",
      "Iteration: 74. Loss: 1.4175903797149658\n",
      "Iteration: 75. Loss: 1.337053656578064\n",
      "Iteration: 76. Loss: 1.3601640462875366\n",
      "Iteration: 77. Loss: 1.340592384338379\n",
      "Iteration: 78. Loss: 1.284590244293213\n",
      "Iteration: 79. Loss: 1.3192819356918335\n",
      "Iteration: 80. Loss: 1.3129663467407227\n",
      "Iteration: 81. Loss: 1.2566951513290405\n",
      "Iteration: 82. Loss: 1.2167658805847168\n",
      "Iteration: 83. Loss: 1.2158615589141846\n",
      "Iteration: 84. Loss: 1.1927827596664429\n",
      "Iteration: 85. Loss: 1.1172105073928833\n",
      "Iteration: 86. Loss: 1.1526755094528198\n",
      "Iteration: 87. Loss: 1.1191641092300415\n",
      "Iteration: 88. Loss: 1.0049397945404053\n",
      "Iteration: 89. Loss: 1.0394500494003296\n",
      "Iteration: 90. Loss: 1.0493544340133667\n",
      "Iteration: 91. Loss: 1.0301412343978882\n",
      "Iteration: 92. Loss: 0.9817649126052856\n",
      "Iteration: 93. Loss: 0.9871378540992737\n",
      "Iteration: 94. Loss: 0.918070375919342\n",
      "Iteration: 95. Loss: 0.9882773756980896\n",
      "Iteration: 96. Loss: 0.913242518901825\n",
      "Iteration: 97. Loss: 0.9234511852264404\n",
      "Iteration: 98. Loss: 0.8599714040756226\n",
      "Iteration: 99. Loss: 0.9086583852767944\n",
      "Iteration: 100. Loss: 0.7526649236679077\n",
      "Iteration: 101. Loss: 0.8945164680480957\n",
      "Iteration: 102. Loss: 0.9587228894233704\n",
      "Iteration: 103. Loss: 0.8208959698677063\n",
      "Iteration: 104. Loss: 0.8837386965751648\n",
      "Iteration: 105. Loss: 0.84783536195755\n",
      "Iteration: 106. Loss: 0.7214344143867493\n",
      "Iteration: 107. Loss: 0.8296672105789185\n",
      "Iteration: 108. Loss: 0.7893431186676025\n",
      "Iteration: 109. Loss: 0.9584905505180359\n",
      "Iteration: 110. Loss: 0.8105194568634033\n",
      "Iteration: 111. Loss: 0.875916600227356\n",
      "Iteration: 112. Loss: 0.8008741140365601\n",
      "Iteration: 113. Loss: 0.8095566034317017\n",
      "Iteration: 114. Loss: 0.8029081225395203\n",
      "Iteration: 115. Loss: 0.8133724331855774\n",
      "Iteration: 116. Loss: 0.8754326105117798\n",
      "Iteration: 117. Loss: 0.8837832808494568\n",
      "Iteration: 118. Loss: 0.7644441723823547\n",
      "Iteration: 119. Loss: 0.6911144852638245\n",
      "Iteration: 120. Loss: 0.6395520567893982\n",
      "Iteration: 121. Loss: 0.7336559891700745\n",
      "Iteration: 122. Loss: 0.7270857095718384\n",
      "Iteration: 123. Loss: 0.8119662404060364\n",
      "Iteration: 124. Loss: 0.6985114812850952\n",
      "Iteration: 125. Loss: 0.6143220663070679\n",
      "Iteration: 126. Loss: 0.6591202020645142\n",
      "Iteration: 127. Loss: 0.8093926310539246\n",
      "Iteration: 128. Loss: 0.5955504775047302\n",
      "Iteration: 129. Loss: 0.6821956038475037\n",
      "Iteration: 130. Loss: 0.6547634601593018\n",
      "Iteration: 131. Loss: 0.7190167307853699\n",
      "Iteration: 132. Loss: 0.6687215566635132\n",
      "Iteration: 133. Loss: 0.7186089158058167\n",
      "Iteration: 134. Loss: 0.7924649715423584\n",
      "Iteration: 135. Loss: 0.5864251852035522\n",
      "Iteration: 136. Loss: 0.6828485727310181\n",
      "Iteration: 137. Loss: 0.6611204743385315\n",
      "Iteration: 138. Loss: 0.5600850582122803\n",
      "Iteration: 139. Loss: 0.6276360750198364\n",
      "Iteration: 140. Loss: 0.6765300035476685\n",
      "Iteration: 141. Loss: 0.7129582762718201\n",
      "Iteration: 142. Loss: 0.8672052025794983\n",
      "Iteration: 143. Loss: 0.6562848091125488\n",
      "Iteration: 144. Loss: 0.7211482524871826\n",
      "Iteration: 145. Loss: 0.6307440996170044\n",
      "Iteration: 146. Loss: 0.5642566084861755\n",
      "Iteration: 147. Loss: 0.6271312832832336\n",
      "Iteration: 148. Loss: 0.6856881976127625\n",
      "Iteration: 149. Loss: 0.5580832958221436\n",
      "Iteration: 150. Loss: 0.4508025646209717\n",
      "Iteration: 151. Loss: 0.42761629819869995\n",
      "Iteration: 152. Loss: 0.6261466145515442\n",
      "Iteration: 153. Loss: 0.5628938674926758\n",
      "Iteration: 154. Loss: 0.5931108593940735\n",
      "Iteration: 155. Loss: 0.6006119251251221\n",
      "Iteration: 156. Loss: 0.510869562625885\n",
      "Iteration: 157. Loss: 0.5932323932647705\n",
      "Iteration: 158. Loss: 0.5278608202934265\n",
      "Iteration: 159. Loss: 0.4955238699913025\n",
      "Iteration: 160. Loss: 0.5213411450386047\n",
      "Iteration: 161. Loss: 0.5844116806983948\n",
      "Iteration: 162. Loss: 0.6062160134315491\n",
      "Iteration: 163. Loss: 0.5116031169891357\n",
      "Iteration: 164. Loss: 0.36953166127204895\n",
      "Iteration: 165. Loss: 0.5723621249198914\n",
      "Iteration: 166. Loss: 0.5286834239959717\n",
      "Iteration: 167. Loss: 0.42707157135009766\n",
      "Iteration: 168. Loss: 0.40789976716041565\n",
      "Iteration: 169. Loss: 0.5778162479400635\n",
      "Iteration: 170. Loss: 0.6364796161651611\n",
      "Iteration: 171. Loss: 0.5199862122535706\n",
      "Iteration: 172. Loss: 0.5699440240859985\n",
      "Iteration: 173. Loss: 0.4712427854537964\n",
      "Iteration: 174. Loss: 0.6452969908714294\n",
      "Iteration: 175. Loss: 0.4720458984375\n",
      "Iteration: 176. Loss: 0.472331166267395\n",
      "Iteration: 177. Loss: 0.4389156401157379\n",
      "Iteration: 178. Loss: 0.4767742156982422\n",
      "Iteration: 179. Loss: 0.5286004543304443\n",
      "Iteration: 180. Loss: 0.42344731092453003\n",
      "Iteration: 181. Loss: 0.4365383982658386\n",
      "Iteration: 182. Loss: 0.551617443561554\n",
      "Iteration: 183. Loss: 0.5353935360908508\n",
      "Iteration: 184. Loss: 0.45632973313331604\n",
      "Iteration: 185. Loss: 0.6759545207023621\n",
      "Iteration: 186. Loss: 0.5005924701690674\n",
      "Iteration: 187. Loss: 0.5420138835906982\n",
      "Iteration: 188. Loss: 0.3874882757663727\n",
      "Iteration: 189. Loss: 0.46590712666511536\n",
      "Iteration: 190. Loss: 0.47989383339881897\n",
      "Iteration: 191. Loss: 0.36209872364997864\n",
      "Iteration: 192. Loss: 0.5827887058258057\n",
      "Iteration: 193. Loss: 0.5504494905471802\n",
      "Iteration: 194. Loss: 0.3901498019695282\n",
      "Iteration: 195. Loss: 0.528954803943634\n",
      "Iteration: 196. Loss: 0.5537351369857788\n",
      "Iteration: 197. Loss: 0.3637831509113312\n",
      "Iteration: 198. Loss: 0.3898514211177826\n",
      "Iteration: 199. Loss: 0.43537721037864685\n",
      "Iteration: 200. Loss: 0.48462772369384766\n",
      "Iteration: 201. Loss: 0.5529389381408691\n",
      "Iteration: 202. Loss: 0.4443252682685852\n",
      "Iteration: 203. Loss: 0.41783440113067627\n",
      "Iteration: 204. Loss: 0.38836371898651123\n",
      "Iteration: 205. Loss: 0.5396831035614014\n",
      "Iteration: 206. Loss: 0.5865986347198486\n",
      "Iteration: 207. Loss: 0.3923529088497162\n",
      "Iteration: 208. Loss: 0.4560004472732544\n",
      "Iteration: 209. Loss: 0.41748106479644775\n",
      "Iteration: 210. Loss: 0.6170954704284668\n",
      "Iteration: 211. Loss: 0.6398490071296692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 212. Loss: 0.52409827709198\n",
      "Iteration: 213. Loss: 0.5392146706581116\n",
      "Iteration: 214. Loss: 0.3523859679698944\n",
      "Iteration: 215. Loss: 0.33588922023773193\n",
      "Iteration: 216. Loss: 0.47253063321113586\n",
      "Iteration: 217. Loss: 0.5903775095939636\n",
      "Iteration: 218. Loss: 0.46703213453292847\n",
      "Iteration: 219. Loss: 0.44318535923957825\n",
      "Iteration: 220. Loss: 0.5025017857551575\n",
      "Iteration: 221. Loss: 0.4242366552352905\n",
      "Iteration: 222. Loss: 0.3857786953449249\n",
      "Iteration: 223. Loss: 0.5491111278533936\n",
      "Iteration: 224. Loss: 0.5383682250976562\n",
      "Iteration: 225. Loss: 0.46171218156814575\n",
      "Iteration: 226. Loss: 0.6248679161071777\n",
      "Iteration: 227. Loss: 0.4019682705402374\n",
      "Iteration: 228. Loss: 0.43347567319869995\n",
      "Iteration: 229. Loss: 0.36112672090530396\n",
      "Iteration: 230. Loss: 0.5524498224258423\n",
      "Iteration: 231. Loss: 0.47621095180511475\n",
      "Iteration: 232. Loss: 0.3991951644420624\n",
      "Iteration: 233. Loss: 0.39684009552001953\n",
      "Iteration: 234. Loss: 0.34384486079216003\n",
      "Iteration: 235. Loss: 0.38935500383377075\n",
      "Iteration: 236. Loss: 0.38050973415374756\n",
      "Iteration: 237. Loss: 0.5602471828460693\n",
      "Iteration: 238. Loss: 0.5498067736625671\n",
      "Iteration: 239. Loss: 0.4317719340324402\n",
      "Iteration: 240. Loss: 0.37414059042930603\n",
      "Iteration: 241. Loss: 0.4599890112876892\n",
      "Iteration: 242. Loss: 0.3735525906085968\n",
      "Iteration: 243. Loss: 0.511629581451416\n",
      "Iteration: 244. Loss: 0.3803912401199341\n",
      "Iteration: 245. Loss: 0.32642948627471924\n",
      "Iteration: 246. Loss: 0.46182575821876526\n",
      "Iteration: 247. Loss: 0.2919437289237976\n",
      "Iteration: 248. Loss: 0.5255392789840698\n",
      "Iteration: 249. Loss: 0.44623690843582153\n",
      "Iteration: 250. Loss: 0.5976004004478455\n",
      "Iteration: 251. Loss: 0.4265974164009094\n",
      "Iteration: 252. Loss: 0.40206143260002136\n",
      "Iteration: 253. Loss: 0.3777221441268921\n",
      "Iteration: 254. Loss: 0.5062769055366516\n",
      "Iteration: 255. Loss: 0.42349773645401\n",
      "Iteration: 256. Loss: 0.49501559138298035\n",
      "Iteration: 257. Loss: 0.45075497031211853\n",
      "Iteration: 258. Loss: 0.3579314351081848\n",
      "Iteration: 259. Loss: 0.549988865852356\n",
      "Iteration: 260. Loss: 0.39299288392066956\n",
      "Iteration: 261. Loss: 0.5791094303131104\n",
      "Iteration: 262. Loss: 0.4124247431755066\n",
      "Iteration: 263. Loss: 0.38267025351524353\n",
      "Iteration: 264. Loss: 0.411570280790329\n",
      "Iteration: 265. Loss: 0.45312178134918213\n",
      "Iteration: 266. Loss: 0.4723489284515381\n",
      "Iteration: 267. Loss: 0.3292037546634674\n",
      "Iteration: 268. Loss: 0.3116421103477478\n",
      "Iteration: 269. Loss: 0.27761945128440857\n",
      "Iteration: 270. Loss: 0.38331279158592224\n",
      "Iteration: 271. Loss: 0.522810161113739\n",
      "Iteration: 272. Loss: 0.35998713970184326\n",
      "Iteration: 273. Loss: 0.4052424132823944\n",
      "Iteration: 274. Loss: 0.43770766258239746\n",
      "Iteration: 275. Loss: 0.34553033113479614\n",
      "Iteration: 276. Loss: 0.5582522749900818\n",
      "Iteration: 277. Loss: 0.5981698036193848\n",
      "Iteration: 278. Loss: 0.5150518417358398\n",
      "Iteration: 279. Loss: 0.33718571066856384\n",
      "Iteration: 280. Loss: 0.40289393067359924\n",
      "Iteration: 281. Loss: 0.4496958255767822\n",
      "Iteration: 282. Loss: 0.48930495977401733\n",
      "Iteration: 283. Loss: 0.3830447494983673\n",
      "Iteration: 284. Loss: 0.4697616696357727\n",
      "Iteration: 285. Loss: 0.4893941879272461\n",
      "Iteration: 286. Loss: 0.32842952013015747\n",
      "Iteration: 287. Loss: 0.3143031597137451\n",
      "Iteration: 288. Loss: 0.42227473855018616\n",
      "Iteration: 289. Loss: 0.4414081871509552\n",
      "Iteration: 290. Loss: 0.346466600894928\n",
      "Iteration: 291. Loss: 0.41145214438438416\n",
      "Iteration: 292. Loss: 0.354749858379364\n",
      "Iteration: 293. Loss: 0.3781305253505707\n",
      "Iteration: 294. Loss: 0.34828731417655945\n",
      "Iteration: 295. Loss: 0.4820196032524109\n",
      "Iteration: 296. Loss: 0.38390418887138367\n",
      "Iteration: 297. Loss: 0.43842384219169617\n",
      "Iteration: 298. Loss: 0.2675759494304657\n",
      "Iteration: 299. Loss: 0.3480325937271118\n",
      "Iteration: 300. Loss: 0.27056923508644104\n",
      "Iteration: 301. Loss: 0.28380197286605835\n",
      "Iteration: 302. Loss: 0.4055352807044983\n",
      "Iteration: 303. Loss: 0.42742806673049927\n",
      "Iteration: 304. Loss: 0.34738993644714355\n",
      "Iteration: 305. Loss: 0.31340745091438293\n",
      "Iteration: 306. Loss: 0.28552618622779846\n",
      "Iteration: 307. Loss: 0.3834809958934784\n",
      "Iteration: 308. Loss: 0.3440437614917755\n",
      "Iteration: 309. Loss: 0.31471142172813416\n",
      "Iteration: 310. Loss: 0.31434759497642517\n",
      "Iteration: 311. Loss: 0.3117980360984802\n",
      "Iteration: 312. Loss: 0.3486140966415405\n",
      "Iteration: 313. Loss: 0.2691222131252289\n",
      "Iteration: 314. Loss: 0.5061038732528687\n",
      "Iteration: 315. Loss: 0.3377709686756134\n",
      "Iteration: 316. Loss: 0.41748616099357605\n",
      "Iteration: 317. Loss: 0.31087321043014526\n",
      "Iteration: 318. Loss: 0.3442404568195343\n",
      "Iteration: 319. Loss: 0.401265412569046\n",
      "Iteration: 320. Loss: 0.4737165570259094\n",
      "Iteration: 321. Loss: 0.467326819896698\n",
      "Iteration: 322. Loss: 0.6315315365791321\n",
      "Iteration: 323. Loss: 0.4561871290206909\n",
      "Iteration: 324. Loss: 0.31341657042503357\n",
      "Iteration: 325. Loss: 0.2971043884754181\n",
      "Iteration: 326. Loss: 0.3353087604045868\n",
      "Iteration: 327. Loss: 0.38295668363571167\n",
      "Iteration: 328. Loss: 0.2352922409772873\n",
      "Iteration: 329. Loss: 0.5132535696029663\n",
      "Iteration: 330. Loss: 0.3124052584171295\n",
      "Iteration: 331. Loss: 0.275252103805542\n",
      "Iteration: 332. Loss: 0.4449731409549713\n",
      "Iteration: 333. Loss: 0.5702816247940063\n",
      "Iteration: 334. Loss: 0.33522942662239075\n",
      "Iteration: 335. Loss: 0.4683716595172882\n",
      "Iteration: 336. Loss: 0.31645289063453674\n",
      "Iteration: 337. Loss: 0.2772936224937439\n",
      "Iteration: 338. Loss: 0.35400551557540894\n",
      "Iteration: 339. Loss: 0.23179635405540466\n",
      "Iteration: 340. Loss: 0.3525475561618805\n",
      "Iteration: 341. Loss: 0.31938067078590393\n",
      "Iteration: 342. Loss: 0.3204852342605591\n",
      "Iteration: 343. Loss: 0.459977924823761\n",
      "Iteration: 344. Loss: 0.2656346559524536\n",
      "Iteration: 345. Loss: 0.3093544840812683\n",
      "Iteration: 346. Loss: 0.357054203748703\n",
      "Iteration: 347. Loss: 0.4054103493690491\n",
      "Iteration: 348. Loss: 0.38681018352508545\n",
      "Iteration: 349. Loss: 0.4263765811920166\n",
      "Iteration: 350. Loss: 0.4539496600627899\n",
      "Iteration: 351. Loss: 0.34545621275901794\n",
      "Iteration: 352. Loss: 0.39412951469421387\n",
      "Iteration: 353. Loss: 0.5253474712371826\n",
      "Iteration: 354. Loss: 0.411601722240448\n",
      "Iteration: 355. Loss: 0.32699450850486755\n",
      "Iteration: 356. Loss: 0.6221160292625427\n",
      "Iteration: 357. Loss: 0.35736769437789917\n",
      "Iteration: 358. Loss: 0.46339839696884155\n",
      "Iteration: 359. Loss: 0.48636719584465027\n",
      "Iteration: 360. Loss: 0.5056929588317871\n",
      "Iteration: 361. Loss: 0.5052913427352905\n",
      "Iteration: 362. Loss: 0.3501322269439697\n",
      "Iteration: 363. Loss: 0.30719509720802307\n",
      "Iteration: 364. Loss: 0.3873307704925537\n",
      "Iteration: 365. Loss: 0.37846651673316956\n",
      "Iteration: 366. Loss: 0.4760397672653198\n",
      "Iteration: 367. Loss: 0.24892103672027588\n",
      "Iteration: 368. Loss: 0.22976073622703552\n",
      "Iteration: 369. Loss: 0.32712510228157043\n",
      "Iteration: 370. Loss: 0.3985390067100525\n",
      "Iteration: 371. Loss: 0.26785609126091003\n",
      "Iteration: 372. Loss: 0.339629590511322\n",
      "Iteration: 373. Loss: 0.4617866277694702\n",
      "Iteration: 374. Loss: 0.4054449796676636\n",
      "Iteration: 375. Loss: 0.28018292784690857\n",
      "Iteration: 376. Loss: 0.5548936724662781\n",
      "Iteration: 377. Loss: 0.35434749722480774\n",
      "Iteration: 378. Loss: 0.25493812561035156\n",
      "Iteration: 379. Loss: 0.35235652327537537\n",
      "Iteration: 380. Loss: 0.32201698422431946\n",
      "Iteration: 381. Loss: 0.33116066455841064\n",
      "Iteration: 382. Loss: 0.42344239354133606\n",
      "Iteration: 383. Loss: 0.460213840007782\n",
      "Iteration: 384. Loss: 0.31483224034309387\n",
      "Iteration: 385. Loss: 0.2785392701625824\n",
      "Iteration: 386. Loss: 0.4304755926132202\n",
      "Iteration: 387. Loss: 0.299287348985672\n",
      "Iteration: 388. Loss: 0.3083789646625519\n",
      "Iteration: 389. Loss: 0.24470585584640503\n",
      "Iteration: 390. Loss: 0.3773351311683655\n",
      "Iteration: 391. Loss: 0.39464855194091797\n",
      "Iteration: 392. Loss: 0.29081541299819946\n",
      "Iteration: 393. Loss: 0.2874598205089569\n",
      "Iteration: 394. Loss: 0.38678616285324097\n",
      "Iteration: 395. Loss: 0.3031914532184601\n",
      "Iteration: 396. Loss: 0.3057764768600464\n",
      "Iteration: 397. Loss: 0.3441806733608246\n",
      "Iteration: 398. Loss: 0.28586092591285706\n",
      "Iteration: 399. Loss: 0.30908408761024475\n",
      "Iteration: 400. Loss: 0.3914308249950409\n",
      "Iteration: 401. Loss: 0.46934443712234497\n",
      "Iteration: 402. Loss: 0.3383423984050751\n",
      "Iteration: 403. Loss: 0.2941066026687622\n",
      "Iteration: 404. Loss: 0.42512059211730957\n",
      "Iteration: 405. Loss: 0.4815562069416046\n",
      "Iteration: 406. Loss: 0.46136295795440674\n",
      "Iteration: 407. Loss: 0.33158767223358154\n",
      "Iteration: 408. Loss: 0.4187076687812805\n",
      "Iteration: 409. Loss: 0.2320311963558197\n",
      "Iteration: 410. Loss: 0.46340423822402954\n",
      "Iteration: 411. Loss: 0.3989781141281128\n",
      "Iteration: 412. Loss: 0.39882808923721313\n",
      "Iteration: 413. Loss: 0.3974502980709076\n",
      "Iteration: 414. Loss: 0.3910226821899414\n",
      "Iteration: 415. Loss: 0.2736762762069702\n",
      "Iteration: 416. Loss: 0.3531017303466797\n",
      "Iteration: 417. Loss: 0.29346412420272827\n",
      "Iteration: 418. Loss: 0.2694542407989502\n",
      "Iteration: 419. Loss: 0.45312047004699707\n",
      "Iteration: 420. Loss: 0.4681084156036377\n",
      "Iteration: 421. Loss: 0.34334737062454224\n",
      "Iteration: 422. Loss: 0.4587530195713043\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 423. Loss: 0.38746702671051025\n",
      "Iteration: 424. Loss: 0.3171207308769226\n",
      "Iteration: 425. Loss: 0.3048940896987915\n",
      "Iteration: 426. Loss: 0.36534592509269714\n",
      "Iteration: 427. Loss: 0.23228545486927032\n",
      "Iteration: 428. Loss: 0.3460231423377991\n",
      "Iteration: 429. Loss: 0.2580827474594116\n",
      "Iteration: 430. Loss: 0.18930314481258392\n",
      "Iteration: 431. Loss: 0.36882269382476807\n",
      "Iteration: 432. Loss: 0.488783061504364\n",
      "Iteration: 433. Loss: 0.36127251386642456\n",
      "Iteration: 434. Loss: 0.2713894546031952\n",
      "Iteration: 435. Loss: 0.3811684846878052\n",
      "Iteration: 436. Loss: 0.290493905544281\n",
      "Iteration: 437. Loss: 0.3426871597766876\n",
      "Iteration: 438. Loss: 0.5136816501617432\n",
      "Iteration: 439. Loss: 0.3308650553226471\n",
      "Iteration: 440. Loss: 0.38806259632110596\n",
      "Iteration: 441. Loss: 0.3339191675186157\n",
      "Iteration: 442. Loss: 0.38413456082344055\n",
      "Iteration: 443. Loss: 0.27067622542381287\n",
      "Iteration: 444. Loss: 0.21475204825401306\n",
      "Iteration: 445. Loss: 0.58870929479599\n",
      "Iteration: 446. Loss: 0.28163257241249084\n",
      "Iteration: 447. Loss: 0.2624094784259796\n",
      "Iteration: 448. Loss: 0.264499694108963\n",
      "Iteration: 449. Loss: 0.2892209589481354\n",
      "Iteration: 450. Loss: 0.4844418466091156\n",
      "Iteration: 451. Loss: 0.4784047603607178\n",
      "Iteration: 452. Loss: 0.23058971762657166\n",
      "Iteration: 453. Loss: 0.2757127285003662\n",
      "Iteration: 454. Loss: 0.43325528502464294\n",
      "Iteration: 455. Loss: 0.2798053026199341\n",
      "Iteration: 456. Loss: 0.36315450072288513\n",
      "Iteration: 457. Loss: 0.44414234161376953\n",
      "Iteration: 458. Loss: 0.3641608953475952\n",
      "Iteration: 459. Loss: 0.23261240124702454\n",
      "Iteration: 460. Loss: 0.3062034249305725\n",
      "Iteration: 461. Loss: 0.322195827960968\n",
      "Iteration: 462. Loss: 0.47970059514045715\n",
      "Iteration: 463. Loss: 0.28396567702293396\n",
      "Iteration: 464. Loss: 0.3463904559612274\n",
      "Iteration: 465. Loss: 0.2586948275566101\n",
      "Iteration: 466. Loss: 0.2813906967639923\n",
      "Iteration: 467. Loss: 0.34157416224479675\n",
      "Iteration: 468. Loss: 0.4155484735965729\n",
      "Iteration: 469. Loss: 0.21955053508281708\n",
      "Iteration: 470. Loss: 0.18521057069301605\n",
      "Iteration: 471. Loss: 0.318078875541687\n",
      "Iteration: 472. Loss: 0.42838427424430847\n",
      "Iteration: 473. Loss: 0.3763463497161865\n",
      "Iteration: 474. Loss: 0.34518054127693176\n",
      "Iteration: 475. Loss: 0.3452776372432709\n",
      "Iteration: 476. Loss: 0.3445023000240326\n",
      "Iteration: 477. Loss: 0.4226911664009094\n",
      "Iteration: 478. Loss: 0.20854246616363525\n",
      "Iteration: 479. Loss: 0.315123587846756\n",
      "Iteration: 480. Loss: 0.3652767837047577\n",
      "Iteration: 481. Loss: 0.23837172985076904\n",
      "Iteration: 482. Loss: 0.3840232789516449\n",
      "Iteration: 483. Loss: 0.2840714752674103\n",
      "Iteration: 484. Loss: 0.2883017659187317\n",
      "Iteration: 485. Loss: 0.2944032847881317\n",
      "Iteration: 486. Loss: 0.16931255161762238\n",
      "Iteration: 487. Loss: 0.28122228384017944\n",
      "Iteration: 488. Loss: 0.23570793867111206\n",
      "Iteration: 489. Loss: 0.25357815623283386\n",
      "Iteration: 490. Loss: 0.3872954845428467\n",
      "Iteration: 491. Loss: 0.400940477848053\n",
      "Iteration: 492. Loss: 0.35669025778770447\n",
      "Iteration: 493. Loss: 0.30091017484664917\n",
      "Iteration: 494. Loss: 0.1869291365146637\n",
      "Iteration: 495. Loss: 0.2707262933254242\n",
      "Iteration: 496. Loss: 0.3301865756511688\n",
      "Iteration: 497. Loss: 0.3250136077404022\n",
      "Iteration: 498. Loss: 0.22709602117538452\n",
      "Iteration: 499. Loss: 0.27109166979789734\n",
      "Iteration: 500. Loss: 0.34712356328964233\n",
      "Iteration: 501. Loss: 0.1496419459581375\n",
      "Iteration: 502. Loss: 0.26070454716682434\n",
      "Iteration: 503. Loss: 0.2630523443222046\n",
      "Iteration: 504. Loss: 0.26807206869125366\n",
      "Iteration: 505. Loss: 0.33801591396331787\n",
      "Iteration: 506. Loss: 0.22937242686748505\n",
      "Iteration: 507. Loss: 0.3317655622959137\n",
      "Iteration: 508. Loss: 0.3155843913555145\n",
      "Iteration: 509. Loss: 0.3186795115470886\n",
      "Iteration: 510. Loss: 0.33816543221473694\n",
      "Iteration: 511. Loss: 0.30619412660598755\n",
      "Iteration: 512. Loss: 0.39013826847076416\n",
      "Iteration: 513. Loss: 0.3571760058403015\n",
      "Iteration: 514. Loss: 0.6584050059318542\n",
      "Iteration: 515. Loss: 0.33874568343162537\n",
      "Iteration: 516. Loss: 0.4062429666519165\n",
      "Iteration: 517. Loss: 0.460112988948822\n",
      "Iteration: 518. Loss: 0.4147942364215851\n",
      "Iteration: 519. Loss: 0.4081568121910095\n",
      "Iteration: 520. Loss: 0.30796289443969727\n",
      "Iteration: 521. Loss: 0.2267400324344635\n",
      "Iteration: 522. Loss: 0.22062721848487854\n",
      "Iteration: 523. Loss: 0.27354052662849426\n",
      "Iteration: 524. Loss: 0.320637971162796\n",
      "Iteration: 525. Loss: 0.40026405453681946\n",
      "Iteration: 526. Loss: 0.23222482204437256\n",
      "Iteration: 527. Loss: 0.45787638425827026\n",
      "Iteration: 528. Loss: 0.1977136731147766\n",
      "Iteration: 529. Loss: 0.37056609988212585\n",
      "Iteration: 530. Loss: 0.3222898542881012\n",
      "Iteration: 531. Loss: 0.3182353079319\n",
      "Iteration: 532. Loss: 0.2951936423778534\n",
      "Iteration: 533. Loss: 0.1915230005979538\n",
      "Iteration: 534. Loss: 0.3968517780303955\n",
      "Iteration: 535. Loss: 0.2736435830593109\n",
      "Iteration: 536. Loss: 0.2332417517900467\n",
      "Iteration: 537. Loss: 0.20984627306461334\n",
      "Iteration: 538. Loss: 0.40738266706466675\n",
      "Iteration: 539. Loss: 0.1758204847574234\n",
      "Iteration: 540. Loss: 0.33571314811706543\n",
      "Iteration: 541. Loss: 0.353791743516922\n",
      "Iteration: 542. Loss: 0.2779688537120819\n",
      "Iteration: 543. Loss: 0.6068071126937866\n",
      "Iteration: 544. Loss: 0.29181167483329773\n",
      "Iteration: 545. Loss: 0.23964297771453857\n",
      "Iteration: 546. Loss: 0.320261150598526\n",
      "Iteration: 547. Loss: 0.31112372875213623\n",
      "Iteration: 548. Loss: 0.2542823255062103\n",
      "Iteration: 549. Loss: 0.22532981634140015\n",
      "Iteration: 550. Loss: 0.20108796656131744\n",
      "Iteration: 551. Loss: 0.40543627738952637\n",
      "Iteration: 552. Loss: 0.3347471356391907\n",
      "Iteration: 553. Loss: 0.15228521823883057\n",
      "Iteration: 554. Loss: 0.2743775546550751\n",
      "Iteration: 555. Loss: 0.4140250086784363\n",
      "Iteration: 556. Loss: 0.19349923729896545\n",
      "Iteration: 557. Loss: 0.2618441879749298\n",
      "Iteration: 558. Loss: 0.3231005072593689\n",
      "Iteration: 559. Loss: 0.3105916380882263\n",
      "Iteration: 560. Loss: 0.3545127213001251\n",
      "Iteration: 561. Loss: 0.29211655259132385\n",
      "Iteration: 562. Loss: 0.229603573679924\n",
      "Iteration: 563. Loss: 0.4139595031738281\n",
      "Iteration: 564. Loss: 0.24176521599292755\n",
      "Iteration: 565. Loss: 0.28339409828186035\n",
      "Iteration: 566. Loss: 0.34108391404151917\n",
      "Iteration: 567. Loss: 0.3617285490036011\n",
      "Iteration: 568. Loss: 0.24279388785362244\n",
      "Iteration: 569. Loss: 0.23209962248802185\n",
      "Iteration: 570. Loss: 0.27000686526298523\n",
      "Iteration: 571. Loss: 0.3678950369358063\n",
      "Iteration: 572. Loss: 0.3967825770378113\n",
      "Iteration: 573. Loss: 0.2856566905975342\n",
      "Iteration: 574. Loss: 0.24730034172534943\n",
      "Iteration: 575. Loss: 0.4722611606121063\n",
      "Iteration: 576. Loss: 0.2516075372695923\n",
      "Iteration: 577. Loss: 0.19074684381484985\n",
      "Iteration: 578. Loss: 0.2138601839542389\n",
      "Iteration: 579. Loss: 0.39480167627334595\n",
      "Iteration: 580. Loss: 0.2290976494550705\n",
      "Iteration: 581. Loss: 0.4624986946582794\n",
      "Iteration: 582. Loss: 0.27660149335861206\n",
      "Iteration: 583. Loss: 0.27886542677879333\n",
      "Iteration: 584. Loss: 0.20076514780521393\n",
      "Iteration: 585. Loss: 0.22874727845191956\n",
      "Iteration: 586. Loss: 0.37219133973121643\n",
      "Iteration: 587. Loss: 0.21512454748153687\n",
      "Iteration: 588. Loss: 0.4010697901248932\n",
      "Iteration: 589. Loss: 0.24407264590263367\n",
      "Iteration: 590. Loss: 0.1924414187669754\n",
      "Iteration: 591. Loss: 0.22244365513324738\n",
      "Iteration: 592. Loss: 0.26385796070098877\n",
      "Iteration: 593. Loss: 0.2275242656469345\n",
      "Iteration: 594. Loss: 0.4269399642944336\n",
      "Iteration: 595. Loss: 0.3189854919910431\n",
      "Iteration: 596. Loss: 0.2443423867225647\n",
      "Iteration: 597. Loss: 0.32951557636260986\n",
      "Iteration: 598. Loss: 0.33545374870300293\n",
      "Iteration: 599. Loss: 0.2866673767566681\n",
      "Iteration: 600. Loss: 0.2698756456375122\n",
      "Iteration: 601. Loss: 0.22088029980659485\n",
      "Iteration: 602. Loss: 0.182212695479393\n",
      "Iteration: 603. Loss: 0.3575000464916229\n",
      "Iteration: 604. Loss: 0.22292210161685944\n",
      "Iteration: 605. Loss: 0.22702760994434357\n",
      "Iteration: 606. Loss: 0.16064397990703583\n",
      "Iteration: 607. Loss: 0.25804299116134644\n",
      "Iteration: 608. Loss: 0.3841544985771179\n",
      "Iteration: 609. Loss: 0.19728359580039978\n",
      "Iteration: 610. Loss: 0.5452876091003418\n",
      "Iteration: 611. Loss: 0.36797991394996643\n",
      "Iteration: 612. Loss: 0.18235154449939728\n",
      "Iteration: 613. Loss: 0.3247841000556946\n",
      "Iteration: 614. Loss: 0.31392624974250793\n",
      "Iteration: 615. Loss: 0.1114327609539032\n",
      "Iteration: 616. Loss: 0.4767528176307678\n",
      "Iteration: 617. Loss: 0.31741863489151\n",
      "Iteration: 618. Loss: 0.2289152890443802\n",
      "Iteration: 619. Loss: 0.32974016666412354\n",
      "Iteration: 620. Loss: 0.2974495589733124\n",
      "Iteration: 621. Loss: 0.2911843955516815\n",
      "Iteration: 622. Loss: 0.33866560459136963\n",
      "Iteration: 623. Loss: 0.2789551615715027\n",
      "Iteration: 624. Loss: 0.1678881049156189\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 625. Loss: 0.24653130769729614\n",
      "Iteration: 626. Loss: 0.33561062812805176\n",
      "Iteration: 627. Loss: 0.3062237799167633\n",
      "Iteration: 628. Loss: 0.40125054121017456\n",
      "Iteration: 629. Loss: 0.21376147866249084\n",
      "Iteration: 630. Loss: 0.2898998558521271\n",
      "Iteration: 631. Loss: 0.3975120186805725\n",
      "Iteration: 632. Loss: 0.25683584809303284\n",
      "Iteration: 633. Loss: 0.24584345519542694\n",
      "Iteration: 634. Loss: 0.26782599091529846\n",
      "Iteration: 635. Loss: 0.24524062871932983\n",
      "Iteration: 636. Loss: 0.26072898507118225\n",
      "Iteration: 637. Loss: 0.30389198660850525\n",
      "Iteration: 638. Loss: 0.3211441934108734\n",
      "Iteration: 639. Loss: 0.18591050803661346\n",
      "Iteration: 640. Loss: 0.45671990513801575\n",
      "Iteration: 641. Loss: 0.20203159749507904\n",
      "Iteration: 642. Loss: 0.22813577950000763\n",
      "Iteration: 643. Loss: 0.3572346568107605\n",
      "Iteration: 644. Loss: 0.32393860816955566\n",
      "Iteration: 645. Loss: 0.12171108275651932\n",
      "Iteration: 646. Loss: 0.21185527741909027\n",
      "Iteration: 647. Loss: 0.29927897453308105\n",
      "Iteration: 648. Loss: 0.25548678636550903\n",
      "Iteration: 649. Loss: 0.2980586886405945\n",
      "Iteration: 650. Loss: 0.3746713697910309\n",
      "Iteration: 651. Loss: 0.36076459288597107\n",
      "Iteration: 652. Loss: 0.402690052986145\n",
      "Iteration: 653. Loss: 0.24671076238155365\n",
      "Iteration: 654. Loss: 0.4521719217300415\n",
      "Iteration: 655. Loss: 0.17836381494998932\n",
      "Iteration: 656. Loss: 0.19053637981414795\n",
      "Iteration: 657. Loss: 0.24062222242355347\n",
      "Iteration: 658. Loss: 0.21275784075260162\n",
      "Iteration: 659. Loss: 0.2097041755914688\n",
      "Iteration: 660. Loss: 0.24389204382896423\n",
      "Iteration: 661. Loss: 0.2833230495452881\n",
      "Iteration: 662. Loss: 0.20928983390331268\n",
      "Iteration: 663. Loss: 0.2840780019760132\n",
      "Iteration: 664. Loss: 0.17117474973201752\n",
      "Iteration: 665. Loss: 0.26459673047065735\n",
      "Iteration: 666. Loss: 0.32277190685272217\n",
      "Iteration: 667. Loss: 0.3047417104244232\n",
      "Iteration: 668. Loss: 0.38607513904571533\n",
      "Iteration: 669. Loss: 0.31460484862327576\n",
      "Iteration: 670. Loss: 0.30615556240081787\n",
      "Iteration: 671. Loss: 0.2518262565135956\n",
      "Iteration: 672. Loss: 0.26886558532714844\n",
      "Iteration: 673. Loss: 0.24164773523807526\n",
      "Iteration: 674. Loss: 0.22559209167957306\n",
      "Iteration: 675. Loss: 0.20983803272247314\n",
      "Iteration: 676. Loss: 0.2823125123977661\n",
      "Iteration: 677. Loss: 0.24652990698814392\n",
      "Iteration: 678. Loss: 0.22332791984081268\n",
      "Iteration: 679. Loss: 0.2876468300819397\n",
      "Iteration: 680. Loss: 0.3128608465194702\n",
      "Iteration: 681. Loss: 0.266019344329834\n",
      "Iteration: 682. Loss: 0.08737263083457947\n",
      "Iteration: 683. Loss: 0.29312729835510254\n",
      "Iteration: 684. Loss: 0.3198946714401245\n",
      "Iteration: 685. Loss: 0.2768607437610626\n",
      "Iteration: 686. Loss: 0.2511504888534546\n",
      "Iteration: 687. Loss: 0.3016345798969269\n",
      "Iteration: 688. Loss: 0.36860573291778564\n",
      "Iteration: 689. Loss: 0.3495536148548126\n",
      "Iteration: 690. Loss: 0.4438270330429077\n",
      "Iteration: 691. Loss: 0.43908873200416565\n",
      "Iteration: 692. Loss: 0.2085532248020172\n",
      "Iteration: 693. Loss: 0.21202434599399567\n",
      "Iteration: 694. Loss: 0.23671047389507294\n",
      "Iteration: 695. Loss: 0.2040775865316391\n",
      "Iteration: 696. Loss: 0.266956627368927\n",
      "Iteration: 697. Loss: 0.24325191974639893\n",
      "Iteration: 698. Loss: 0.19851675629615784\n",
      "Iteration: 699. Loss: 0.2447282373905182\n",
      "Iteration: 700. Loss: 0.2188948094844818\n",
      "Iteration: 701. Loss: 0.2707447111606598\n",
      "Iteration: 702. Loss: 0.20247024297714233\n",
      "Iteration: 703. Loss: 0.3745144009590149\n",
      "Iteration: 704. Loss: 0.438732385635376\n",
      "Iteration: 705. Loss: 0.28815388679504395\n",
      "Iteration: 706. Loss: 0.26736241579055786\n",
      "Iteration: 707. Loss: 0.2888147830963135\n",
      "Iteration: 708. Loss: 0.312820166349411\n",
      "Iteration: 709. Loss: 0.2949453294277191\n",
      "Iteration: 710. Loss: 0.29189223051071167\n",
      "Iteration: 711. Loss: 0.36899644136428833\n",
      "Iteration: 712. Loss: 0.2893843948841095\n",
      "Iteration: 713. Loss: 0.2793046832084656\n",
      "Iteration: 714. Loss: 0.3055388927459717\n",
      "Iteration: 715. Loss: 0.18139857053756714\n",
      "Iteration: 716. Loss: 0.31438660621643066\n",
      "Iteration: 717. Loss: 0.2974453866481781\n",
      "Iteration: 718. Loss: 0.34255123138427734\n",
      "Iteration: 719. Loss: 0.2686748206615448\n",
      "Iteration: 720. Loss: 0.46473392844200134\n",
      "Iteration: 721. Loss: 0.14148066937923431\n",
      "Iteration: 722. Loss: 0.4510728418827057\n",
      "Iteration: 723. Loss: 0.2854582667350769\n",
      "Iteration: 724. Loss: 0.2899554371833801\n",
      "Iteration: 725. Loss: 0.4482845366001129\n",
      "Iteration: 726. Loss: 0.3057047724723816\n",
      "Iteration: 727. Loss: 0.26911023259162903\n",
      "Iteration: 728. Loss: 0.21342577040195465\n",
      "Iteration: 729. Loss: 0.20232383906841278\n",
      "Iteration: 730. Loss: 0.25787097215652466\n",
      "Iteration: 731. Loss: 0.3543725609779358\n",
      "Iteration: 732. Loss: 0.256656289100647\n",
      "Iteration: 733. Loss: 0.20351853966712952\n",
      "Iteration: 734. Loss: 0.329041987657547\n",
      "Iteration: 735. Loss: 0.34297409653663635\n",
      "Iteration: 736. Loss: 0.2879808247089386\n",
      "Iteration: 737. Loss: 0.15559738874435425\n",
      "Iteration: 738. Loss: 0.5267687439918518\n",
      "Iteration: 739. Loss: 0.19351041316986084\n",
      "Iteration: 740. Loss: 0.3234120309352875\n",
      "Iteration: 741. Loss: 0.3475550413131714\n",
      "Iteration: 742. Loss: 0.13492603600025177\n",
      "Iteration: 743. Loss: 0.3630720376968384\n",
      "Iteration: 744. Loss: 0.3074941337108612\n",
      "Iteration: 745. Loss: 0.10675900429487228\n",
      "Iteration: 746. Loss: 0.2550881505012512\n",
      "Iteration: 747. Loss: 0.25962525606155396\n",
      "Iteration: 748. Loss: 0.16164740920066833\n",
      "Iteration: 749. Loss: 0.18659180402755737\n",
      "Iteration: 750. Loss: 0.2696702480316162\n",
      "Iteration: 751. Loss: 0.3265281319618225\n",
      "Iteration: 752. Loss: 0.2633717358112335\n",
      "Iteration: 753. Loss: 0.4384617209434509\n",
      "Iteration: 754. Loss: 0.2706657648086548\n",
      "Iteration: 755. Loss: 0.26071321964263916\n",
      "Iteration: 756. Loss: 0.27831169962882996\n",
      "Iteration: 757. Loss: 0.31608808040618896\n",
      "Iteration: 758. Loss: 0.26409268379211426\n",
      "Iteration: 759. Loss: 0.3743721842765808\n",
      "Iteration: 760. Loss: 0.2846498489379883\n",
      "Iteration: 761. Loss: 0.14524692296981812\n",
      "Iteration: 762. Loss: 0.21958737075328827\n",
      "Iteration: 763. Loss: 0.20065858960151672\n",
      "Iteration: 764. Loss: 0.27499163150787354\n",
      "Iteration: 765. Loss: 0.2423783838748932\n",
      "Iteration: 766. Loss: 0.31551921367645264\n",
      "Iteration: 767. Loss: 0.2501252293586731\n",
      "Iteration: 768. Loss: 0.31254148483276367\n",
      "Iteration: 769. Loss: 0.20007723569869995\n",
      "Iteration: 770. Loss: 0.2529160976409912\n",
      "Iteration: 771. Loss: 0.1786125749349594\n",
      "Iteration: 772. Loss: 0.21529580652713776\n",
      "Iteration: 773. Loss: 0.14034777879714966\n",
      "Iteration: 774. Loss: 0.1263262778520584\n",
      "Iteration: 775. Loss: 0.21423034369945526\n",
      "Iteration: 776. Loss: 0.18393439054489136\n",
      "Iteration: 777. Loss: 0.28771883249282837\n",
      "Iteration: 778. Loss: 0.18597614765167236\n",
      "Iteration: 779. Loss: 0.28520670533180237\n",
      "Iteration: 780. Loss: 0.2822647988796234\n",
      "Iteration: 781. Loss: 0.10931719839572906\n",
      "Iteration: 782. Loss: 0.41682061553001404\n",
      "Iteration: 783. Loss: 0.2674265205860138\n",
      "Iteration: 784. Loss: 0.26962292194366455\n",
      "Iteration: 785. Loss: 0.3483728766441345\n",
      "Iteration: 786. Loss: 0.2622489333152771\n",
      "Iteration: 787. Loss: 0.24595139920711517\n",
      "Iteration: 788. Loss: 0.3048694133758545\n",
      "Iteration: 789. Loss: 0.14372879266738892\n",
      "Iteration: 790. Loss: 0.48447081446647644\n",
      "Iteration: 791. Loss: 0.27081820368766785\n",
      "Iteration: 792. Loss: 0.18949133157730103\n",
      "Iteration: 793. Loss: 0.19441796839237213\n",
      "Iteration: 794. Loss: 0.17453494668006897\n",
      "Iteration: 795. Loss: 0.30899539589881897\n",
      "Iteration: 796. Loss: 0.27297160029411316\n",
      "Iteration: 797. Loss: 0.2824048101902008\n",
      "Iteration: 798. Loss: 0.4170026481151581\n",
      "Iteration: 799. Loss: 0.2043714076280594\n",
      "Iteration: 800. Loss: 0.26988640427589417\n",
      "Iteration: 801. Loss: 0.22149460017681122\n",
      "Iteration: 802. Loss: 0.3695892095565796\n",
      "Iteration: 803. Loss: 0.2862882912158966\n",
      "Iteration: 804. Loss: 0.2071271687746048\n",
      "Iteration: 805. Loss: 0.21667072176933289\n",
      "Iteration: 806. Loss: 0.23894384503364563\n",
      "Iteration: 807. Loss: 0.1841374933719635\n",
      "Iteration: 808. Loss: 0.2022664099931717\n",
      "Iteration: 809. Loss: 0.25133609771728516\n",
      "Iteration: 810. Loss: 0.373038113117218\n",
      "Iteration: 811. Loss: 0.29939526319503784\n",
      "Iteration: 812. Loss: 0.3425956964492798\n",
      "Iteration: 813. Loss: 0.19692742824554443\n",
      "Iteration: 814. Loss: 0.3082185983657837\n",
      "Iteration: 815. Loss: 0.20625872910022736\n",
      "Iteration: 816. Loss: 0.4035536050796509\n",
      "Iteration: 817. Loss: 0.31797003746032715\n",
      "Iteration: 818. Loss: 0.3509142994880676\n",
      "Iteration: 819. Loss: 0.3053429424762726\n",
      "Iteration: 820. Loss: 0.1830110102891922\n",
      "Iteration: 821. Loss: 0.16602326929569244\n",
      "Iteration: 822. Loss: 0.29871395230293274\n",
      "Iteration: 823. Loss: 0.17084933817386627\n",
      "Iteration: 824. Loss: 0.27868208289146423\n",
      "Iteration: 825. Loss: 0.20713354647159576\n",
      "Iteration: 826. Loss: 0.23361709713935852\n",
      "Iteration: 827. Loss: 0.2156122773885727\n",
      "Iteration: 828. Loss: 0.2662043273448944\n",
      "Iteration: 829. Loss: 0.23184746503829956\n",
      "Iteration: 830. Loss: 0.21963657438755035\n",
      "Iteration: 831. Loss: 0.1851690113544464\n",
      "Iteration: 832. Loss: 0.21940730512142181\n",
      "Iteration: 833. Loss: 0.36579757928848267\n",
      "Iteration: 834. Loss: 0.33614471554756165\n",
      "Iteration: 835. Loss: 0.27995002269744873\n",
      "Iteration: 836. Loss: 0.16452018916606903\n",
      "Iteration: 837. Loss: 0.19704002141952515\n",
      "Iteration: 838. Loss: 0.2145921289920807\n",
      "Iteration: 839. Loss: 0.305562287569046\n",
      "Iteration: 840. Loss: 0.35087406635284424\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 841. Loss: 0.46300697326660156\n",
      "Iteration: 842. Loss: 0.2518138587474823\n",
      "Iteration: 843. Loss: 0.3501167297363281\n",
      "Iteration: 844. Loss: 0.20732203125953674\n",
      "Iteration: 845. Loss: 0.26279416680336\n",
      "Iteration: 846. Loss: 0.28035590052604675\n",
      "Iteration: 847. Loss: 0.2462201565504074\n",
      "Iteration: 848. Loss: 0.25746557116508484\n",
      "Iteration: 849. Loss: 0.1530621349811554\n",
      "Iteration: 850. Loss: 0.18941211700439453\n",
      "Iteration: 851. Loss: 0.2710011601448059\n",
      "Iteration: 852. Loss: 0.18777970969676971\n",
      "Iteration: 853. Loss: 0.3942946493625641\n",
      "Iteration: 854. Loss: 0.18894244730472565\n",
      "Iteration: 855. Loss: 0.22594988346099854\n",
      "Iteration: 856. Loss: 0.35841119289398193\n",
      "Iteration: 857. Loss: 0.1709022969007492\n",
      "Iteration: 858. Loss: 0.19502025842666626\n",
      "Iteration: 859. Loss: 0.26366207003593445\n",
      "Iteration: 860. Loss: 0.29390618205070496\n",
      "Iteration: 861. Loss: 0.3081251084804535\n",
      "Iteration: 862. Loss: 0.23879389464855194\n",
      "Iteration: 863. Loss: 0.33630838990211487\n",
      "Iteration: 864. Loss: 0.29148757457733154\n",
      "Iteration: 865. Loss: 0.3211878538131714\n",
      "Iteration: 866. Loss: 0.28393709659576416\n",
      "Iteration: 867. Loss: 0.20500311255455017\n",
      "Iteration: 868. Loss: 0.23567497730255127\n",
      "Iteration: 869. Loss: 0.26508376002311707\n",
      "Iteration: 870. Loss: 0.415854811668396\n",
      "Iteration: 871. Loss: 0.36090555787086487\n",
      "Iteration: 872. Loss: 0.27058395743370056\n",
      "Iteration: 873. Loss: 0.2735089063644409\n",
      "Iteration: 874. Loss: 0.2899633049964905\n",
      "Iteration: 875. Loss: 0.3139118254184723\n",
      "Iteration: 876. Loss: 0.3602280020713806\n",
      "Iteration: 877. Loss: 0.1963208168745041\n",
      "Iteration: 878. Loss: 0.2829503118991852\n",
      "Iteration: 879. Loss: 0.18004636466503143\n",
      "Iteration: 880. Loss: 0.3239911198616028\n",
      "Iteration: 881. Loss: 0.18903644382953644\n",
      "Iteration: 882. Loss: 0.16976962983608246\n",
      "Iteration: 883. Loss: 0.2935054302215576\n",
      "Iteration: 884. Loss: 0.364908903837204\n",
      "Iteration: 885. Loss: 0.19757698476314545\n",
      "Iteration: 886. Loss: 0.21023239195346832\n",
      "Iteration: 887. Loss: 0.19820871949195862\n",
      "Iteration: 888. Loss: 0.39065468311309814\n",
      "Iteration: 889. Loss: 0.263217568397522\n",
      "Iteration: 890. Loss: 0.3350432515144348\n",
      "Iteration: 891. Loss: 0.3661595582962036\n",
      "Iteration: 892. Loss: 0.2521724998950958\n",
      "Iteration: 893. Loss: 0.2711709439754486\n",
      "Iteration: 894. Loss: 0.23402389883995056\n",
      "Iteration: 895. Loss: 0.23701687157154083\n",
      "Iteration: 896. Loss: 0.3222108483314514\n",
      "Iteration: 897. Loss: 0.3506160080432892\n",
      "Iteration: 898. Loss: 0.17554733157157898\n",
      "Iteration: 899. Loss: 0.24451345205307007\n",
      "Iteration: 900. Loss: 0.18583106994628906\n",
      "Iteration: 901. Loss: 0.22845233976840973\n",
      "Iteration: 902. Loss: 0.27963724732398987\n",
      "Iteration: 903. Loss: 0.14492496848106384\n",
      "Iteration: 904. Loss: 0.25064465403556824\n",
      "Iteration: 905. Loss: 0.2840484380722046\n",
      "Iteration: 906. Loss: 0.24602271616458893\n",
      "Iteration: 907. Loss: 0.13612468540668488\n",
      "Iteration: 908. Loss: 0.3533622622489929\n",
      "Iteration: 909. Loss: 0.21362493932247162\n",
      "Iteration: 910. Loss: 0.2330930531024933\n",
      "Iteration: 911. Loss: 0.24179038405418396\n",
      "Iteration: 912. Loss: 0.12802085280418396\n",
      "Iteration: 913. Loss: 0.23541629314422607\n",
      "Iteration: 914. Loss: 0.1945667415857315\n",
      "Iteration: 915. Loss: 0.21127183735370636\n",
      "Iteration: 916. Loss: 0.35882899165153503\n",
      "Iteration: 917. Loss: 0.20752042531967163\n",
      "Iteration: 918. Loss: 0.2259451448917389\n",
      "Iteration: 919. Loss: 0.20611517131328583\n",
      "Iteration: 920. Loss: 0.3404233157634735\n",
      "Iteration: 921. Loss: 0.1949739009141922\n",
      "Iteration: 922. Loss: 0.2949462831020355\n",
      "Iteration: 923. Loss: 0.16287320852279663\n",
      "Iteration: 924. Loss: 0.38668155670166016\n",
      "Iteration: 925. Loss: 0.2337377667427063\n",
      "Iteration: 926. Loss: 0.17362049221992493\n",
      "Iteration: 927. Loss: 0.1504158079624176\n",
      "Iteration: 928. Loss: 0.17728571593761444\n",
      "Iteration: 929. Loss: 0.15757356584072113\n",
      "Iteration: 930. Loss: 0.15606459975242615\n",
      "Iteration: 931. Loss: 0.17594127357006073\n",
      "Iteration: 932. Loss: 0.13104362785816193\n",
      "Iteration: 933. Loss: 0.24885639548301697\n",
      "Iteration: 934. Loss: 0.18899795413017273\n",
      "Iteration: 935. Loss: 0.16312330961227417\n",
      "Iteration: 936. Loss: 0.1781894564628601\n",
      "Iteration: 937. Loss: 0.1509539633989334\n",
      "Iteration: 938. Loss: 0.21096503734588623\n",
      "Iteration: 939. Loss: 0.4172019958496094\n",
      "Iteration: 940. Loss: 0.22545093297958374\n",
      "Iteration: 941. Loss: 0.20995335280895233\n",
      "Iteration: 942. Loss: 0.19187110662460327\n",
      "Iteration: 943. Loss: 0.1833924502134323\n",
      "Iteration: 944. Loss: 0.19361700117588043\n",
      "Iteration: 945. Loss: 0.2776116132736206\n",
      "Iteration: 946. Loss: 0.3098459839820862\n",
      "Iteration: 947. Loss: 0.2951090931892395\n",
      "Iteration: 948. Loss: 0.1871280074119568\n",
      "Iteration: 949. Loss: 0.20092453062534332\n",
      "Iteration: 950. Loss: 0.22439539432525635\n",
      "Iteration: 951. Loss: 0.23644442856311798\n",
      "Iteration: 952. Loss: 0.2617596387863159\n",
      "Iteration: 953. Loss: 0.23197750747203827\n",
      "Iteration: 954. Loss: 0.27600762248039246\n",
      "Iteration: 955. Loss: 0.25535595417022705\n",
      "Iteration: 956. Loss: 0.3138360381126404\n",
      "Iteration: 957. Loss: 0.26264989376068115\n",
      "Iteration: 958. Loss: 0.24220821261405945\n",
      "Iteration: 959. Loss: 0.19773422181606293\n",
      "Iteration: 960. Loss: 0.1701878309249878\n",
      "Iteration: 961. Loss: 0.22720927000045776\n",
      "Iteration: 962. Loss: 0.2112208753824234\n",
      "Iteration: 963. Loss: 0.1812126487493515\n",
      "Iteration: 964. Loss: 0.15070804953575134\n",
      "Iteration: 965. Loss: 0.27682197093963623\n",
      "Iteration: 966. Loss: 0.16940385103225708\n",
      "Iteration: 967. Loss: 0.3159448504447937\n",
      "Iteration: 968. Loss: 0.2700102925300598\n",
      "Iteration: 969. Loss: 0.2738077640533447\n",
      "Iteration: 970. Loss: 0.18444496393203735\n",
      "Iteration: 971. Loss: 0.33862629532814026\n",
      "Iteration: 972. Loss: 0.23346742987632751\n",
      "Iteration: 973. Loss: 0.17865590751171112\n",
      "Iteration: 974. Loss: 0.31967633962631226\n",
      "Iteration: 975. Loss: 0.353191077709198\n",
      "Iteration: 976. Loss: 0.12378770858049393\n",
      "Iteration: 977. Loss: 0.1361624300479889\n",
      "Iteration: 978. Loss: 0.3548347055912018\n",
      "Iteration: 979. Loss: 0.17243705689907074\n",
      "Iteration: 980. Loss: 0.1798219531774521\n",
      "Iteration: 981. Loss: 0.19765587151050568\n",
      "Iteration: 982. Loss: 0.2595450282096863\n",
      "Iteration: 983. Loss: 0.2887807786464691\n",
      "Iteration: 984. Loss: 0.1406581699848175\n",
      "Iteration: 985. Loss: 0.24125583469867706\n",
      "Iteration: 986. Loss: 0.19622087478637695\n",
      "Iteration: 987. Loss: 0.14482660591602325\n",
      "Iteration: 988. Loss: 0.14882268011569977\n",
      "Iteration: 989. Loss: 0.11677274852991104\n",
      "Iteration: 990. Loss: 0.22607645392417908\n",
      "Iteration: 991. Loss: 0.31969496607780457\n",
      "Iteration: 992. Loss: 0.15273460745811462\n",
      "Iteration: 993. Loss: 0.15126608312129974\n",
      "Iteration: 994. Loss: 0.19407027959823608\n",
      "Iteration: 995. Loss: 0.20713841915130615\n",
      "Iteration: 996. Loss: 0.20031633973121643\n",
      "Iteration: 997. Loss: 0.23170676827430725\n",
      "Iteration: 998. Loss: 0.22921104729175568\n",
      "Iteration: 999. Loss: 0.1949322521686554\n",
      "Iteration: 1000. Loss: 0.14845462143421173\n",
      "Iteration: 1001. Loss: 0.20739570260047913\n",
      "Iteration: 1002. Loss: 0.2678321897983551\n",
      "Iteration: 1003. Loss: 0.16136698424816132\n",
      "Iteration: 1004. Loss: 0.17793336510658264\n",
      "Iteration: 1005. Loss: 0.12642700970172882\n",
      "Iteration: 1006. Loss: 0.11921370774507523\n",
      "Iteration: 1007. Loss: 0.295976847410202\n",
      "Iteration: 1008. Loss: 0.1785324215888977\n",
      "Iteration: 1009. Loss: 0.1715112328529358\n",
      "Iteration: 1010. Loss: 0.4370729923248291\n",
      "Iteration: 1011. Loss: 0.15438394248485565\n",
      "Iteration: 1012. Loss: 0.2087928205728531\n",
      "Iteration: 1013. Loss: 0.23164230585098267\n",
      "Iteration: 1014. Loss: 0.24194549024105072\n",
      "Iteration: 1015. Loss: 0.25587958097457886\n",
      "Iteration: 1016. Loss: 0.20064617693424225\n",
      "Iteration: 1017. Loss: 0.19414934515953064\n",
      "Iteration: 1018. Loss: 0.20965956151485443\n",
      "Iteration: 1019. Loss: 0.15060019493103027\n",
      "Iteration: 1020. Loss: 0.19565023481845856\n",
      "Iteration: 1021. Loss: 0.16539810597896576\n",
      "Iteration: 1022. Loss: 0.16453708708286285\n",
      "Iteration: 1023. Loss: 0.38664233684539795\n",
      "Iteration: 1024. Loss: 0.27998602390289307\n",
      "Iteration: 1025. Loss: 0.1997567117214203\n",
      "Iteration: 1026. Loss: 0.2199794054031372\n",
      "Iteration: 1027. Loss: 0.1477499157190323\n",
      "Iteration: 1028. Loss: 0.13859239220619202\n",
      "Iteration: 1029. Loss: 0.24927093088626862\n",
      "Iteration: 1030. Loss: 0.1797371655702591\n",
      "Iteration: 1031. Loss: 0.20196563005447388\n",
      "Iteration: 1032. Loss: 0.2385266274213791\n",
      "Iteration: 1033. Loss: 0.2861309349536896\n",
      "Iteration: 1034. Loss: 0.11208455264568329\n",
      "Iteration: 1035. Loss: 0.1420748233795166\n",
      "Iteration: 1036. Loss: 0.2075643092393875\n",
      "Iteration: 1037. Loss: 0.15582913160324097\n",
      "Iteration: 1038. Loss: 0.20673799514770508\n",
      "Iteration: 1039. Loss: 0.24892310798168182\n",
      "Iteration: 1040. Loss: 0.23707936704158783\n",
      "Iteration: 1041. Loss: 0.15326321125030518\n",
      "Iteration: 1042. Loss: 0.16128607094287872\n",
      "Iteration: 1043. Loss: 0.20042471587657928\n",
      "Iteration: 1044. Loss: 0.15332479774951935\n",
      "Iteration: 1045. Loss: 0.16078907251358032\n",
      "Iteration: 1046. Loss: 0.18097896873950958\n",
      "Iteration: 1047. Loss: 0.26472344994544983\n",
      "Iteration: 1048. Loss: 0.19845084846019745\n",
      "Iteration: 1049. Loss: 0.22954639792442322\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1050. Loss: 0.46925172209739685\n",
      "Iteration: 1051. Loss: 0.26398342847824097\n",
      "Iteration: 1052. Loss: 0.22837619483470917\n",
      "Iteration: 1053. Loss: 0.21231380105018616\n",
      "Iteration: 1054. Loss: 0.3605397045612335\n",
      "Iteration: 1055. Loss: 0.27589523792266846\n",
      "Iteration: 1056. Loss: 0.17517417669296265\n",
      "Iteration: 1057. Loss: 0.11717342585325241\n",
      "Iteration: 1058. Loss: 0.17210422456264496\n",
      "Iteration: 1059. Loss: 0.33676743507385254\n",
      "Iteration: 1060. Loss: 0.18140295147895813\n",
      "Iteration: 1061. Loss: 0.3245254456996918\n",
      "Iteration: 1062. Loss: 0.2031809240579605\n",
      "Iteration: 1063. Loss: 0.23226088285446167\n",
      "Iteration: 1064. Loss: 0.3168601393699646\n",
      "Iteration: 1065. Loss: 0.4085843563079834\n",
      "Iteration: 1066. Loss: 0.19557586312294006\n",
      "Iteration: 1067. Loss: 0.23111620545387268\n",
      "Iteration: 1068. Loss: 0.1688891053199768\n",
      "Iteration: 1069. Loss: 0.16908077895641327\n",
      "Iteration: 1070. Loss: 0.247115358710289\n",
      "Iteration: 1071. Loss: 0.19905158877372742\n",
      "Iteration: 1072. Loss: 0.1406765729188919\n",
      "Iteration: 1073. Loss: 0.22564274072647095\n",
      "Iteration: 1074. Loss: 0.21152514219284058\n",
      "Iteration: 1075. Loss: 0.35399290919303894\n",
      "Iteration: 1076. Loss: 0.2842511832714081\n",
      "Iteration: 1077. Loss: 0.19600485265254974\n",
      "Iteration: 1078. Loss: 0.10840627551078796\n",
      "Iteration: 1079. Loss: 0.1383415013551712\n",
      "Iteration: 1080. Loss: 0.12245866656303406\n",
      "Iteration: 1081. Loss: 0.2792068421840668\n",
      "Iteration: 1082. Loss: 0.20046505331993103\n",
      "Iteration: 1083. Loss: 0.2287863940000534\n",
      "Iteration: 1084. Loss: 0.17496824264526367\n",
      "Iteration: 1085. Loss: 0.3249419927597046\n",
      "Iteration: 1086. Loss: 0.21693262457847595\n",
      "Iteration: 1087. Loss: 0.21583497524261475\n",
      "Iteration: 1088. Loss: 0.21365806460380554\n",
      "Iteration: 1089. Loss: 0.13367080688476562\n",
      "Iteration: 1090. Loss: 0.12822261452674866\n",
      "Iteration: 1091. Loss: 0.18566593527793884\n",
      "Iteration: 1092. Loss: 0.2129541039466858\n",
      "Iteration: 1093. Loss: 0.1867716908454895\n",
      "Iteration: 1094. Loss: 0.22525165975093842\n",
      "Iteration: 1095. Loss: 0.13151617348194122\n",
      "Iteration: 1096. Loss: 0.2507660388946533\n",
      "Iteration: 1097. Loss: 0.28077784180641174\n",
      "Iteration: 1098. Loss: 0.19027459621429443\n",
      "Iteration: 1099. Loss: 0.1700781285762787\n",
      "Iteration: 1100. Loss: 0.17455507814884186\n",
      "Iteration: 1101. Loss: 0.181768000125885\n",
      "Iteration: 1102. Loss: 0.14073702692985535\n",
      "Iteration: 1103. Loss: 0.17738303542137146\n",
      "Iteration: 1104. Loss: 0.21053007245063782\n",
      "Iteration: 1105. Loss: 0.27972012758255005\n",
      "Iteration: 1106. Loss: 0.2767289876937866\n",
      "Iteration: 1107. Loss: 0.22262170910835266\n",
      "Iteration: 1108. Loss: 0.15425918996334076\n",
      "Iteration: 1109. Loss: 0.18901853263378143\n",
      "Iteration: 1110. Loss: 0.22867351770401\n",
      "Iteration: 1111. Loss: 0.2861812710762024\n",
      "Iteration: 1112. Loss: 0.2010342925786972\n",
      "Iteration: 1113. Loss: 0.16677841544151306\n",
      "Iteration: 1114. Loss: 0.18667861819267273\n",
      "Iteration: 1115. Loss: 0.21121317148208618\n",
      "Iteration: 1116. Loss: 0.11825213581323624\n",
      "Iteration: 1117. Loss: 0.18997804820537567\n",
      "Iteration: 1118. Loss: 0.18845361471176147\n",
      "Iteration: 1119. Loss: 0.11980710178613663\n",
      "Iteration: 1120. Loss: 0.15003034472465515\n",
      "Iteration: 1121. Loss: 0.13498221337795258\n",
      "Iteration: 1122. Loss: 0.24223007261753082\n",
      "Iteration: 1123. Loss: 0.16973793506622314\n",
      "Iteration: 1124. Loss: 0.29681599140167236\n",
      "Iteration: 1125. Loss: 0.14913077652454376\n",
      "Iteration: 1126. Loss: 0.29018858075141907\n",
      "Iteration: 1127. Loss: 0.20506338775157928\n",
      "Iteration: 1128. Loss: 0.3222631812095642\n",
      "Iteration: 1129. Loss: 0.1690463423728943\n",
      "Iteration: 1130. Loss: 0.10952609777450562\n",
      "Iteration: 1131. Loss: 0.24101902544498444\n",
      "Iteration: 1132. Loss: 0.11924321204423904\n",
      "Iteration: 1133. Loss: 0.196407750248909\n",
      "Iteration: 1134. Loss: 0.23062269389629364\n",
      "Iteration: 1135. Loss: 0.19695405662059784\n",
      "Iteration: 1136. Loss: 0.1841491311788559\n",
      "Iteration: 1137. Loss: 0.2675248980522156\n",
      "Iteration: 1138. Loss: 0.13956746459007263\n",
      "Iteration: 1139. Loss: 0.21641655266284943\n",
      "Iteration: 1140. Loss: 0.18166284263134003\n",
      "Iteration: 1141. Loss: 0.13140086829662323\n",
      "Iteration: 1142. Loss: 0.1268676370382309\n",
      "Iteration: 1143. Loss: 0.2468806505203247\n",
      "Iteration: 1144. Loss: 0.22342537343502045\n",
      "Iteration: 1145. Loss: 0.287680059671402\n",
      "Iteration: 1146. Loss: 0.22817836701869965\n",
      "Iteration: 1147. Loss: 0.1514410823583603\n",
      "Iteration: 1148. Loss: 0.18380354344844818\n",
      "Iteration: 1149. Loss: 0.21264810860157013\n",
      "Iteration: 1150. Loss: 0.24893790483474731\n",
      "Iteration: 1151. Loss: 0.201079860329628\n",
      "Iteration: 1152. Loss: 0.1688714176416397\n",
      "Iteration: 1153. Loss: 0.3363451659679413\n",
      "Iteration: 1154. Loss: 0.21537385880947113\n",
      "Iteration: 1155. Loss: 0.28142285346984863\n",
      "Iteration: 1156. Loss: 0.22606058418750763\n",
      "Iteration: 1157. Loss: 0.17629240453243256\n",
      "Iteration: 1158. Loss: 0.21207688748836517\n",
      "Iteration: 1159. Loss: 0.18175123631954193\n",
      "Iteration: 1160. Loss: 0.2231927514076233\n",
      "Iteration: 1161. Loss: 0.14940695464611053\n",
      "Iteration: 1162. Loss: 0.09391994774341583\n",
      "Iteration: 1163. Loss: 0.17626047134399414\n",
      "Iteration: 1164. Loss: 0.1110435500741005\n",
      "Iteration: 1165. Loss: 0.13775639235973358\n",
      "Iteration: 1166. Loss: 0.128914475440979\n",
      "Iteration: 1167. Loss: 0.17553365230560303\n",
      "Iteration: 1168. Loss: 0.21242980659008026\n",
      "Iteration: 1169. Loss: 0.19957412779331207\n",
      "Iteration: 1170. Loss: 0.2473820298910141\n",
      "Iteration: 1171. Loss: 0.21075010299682617\n",
      "Iteration: 1172. Loss: 0.2737281322479248\n",
      "Iteration: 1173. Loss: 0.15341421961784363\n",
      "Iteration: 1174. Loss: 0.09537828713655472\n",
      "Iteration: 1175. Loss: 0.1911081075668335\n",
      "Iteration: 1176. Loss: 0.12015125155448914\n",
      "Iteration: 1177. Loss: 0.11798065900802612\n",
      "Iteration: 1178. Loss: 0.3106028437614441\n",
      "Iteration: 1179. Loss: 0.4537412226200104\n",
      "Iteration: 1180. Loss: 0.21854373812675476\n",
      "Iteration: 1181. Loss: 0.40253448486328125\n",
      "Iteration: 1182. Loss: 0.2364470660686493\n",
      "Iteration: 1183. Loss: 0.2804279923439026\n",
      "Iteration: 1184. Loss: 0.15903668105602264\n",
      "Iteration: 1185. Loss: 0.20334534347057343\n",
      "Iteration: 1186. Loss: 0.12209772318601608\n",
      "Iteration: 1187. Loss: 0.17704574763774872\n",
      "Iteration: 1188. Loss: 0.06540308147668839\n",
      "Iteration: 1189. Loss: 0.23542554676532745\n",
      "Iteration: 1190. Loss: 0.16856154799461365\n",
      "Iteration: 1191. Loss: 0.29837486147880554\n",
      "Iteration: 1192. Loss: 0.17855145037174225\n",
      "Iteration: 1193. Loss: 0.30009695887565613\n",
      "Iteration: 1194. Loss: 0.1797596961259842\n",
      "Iteration: 1195. Loss: 0.21260221302509308\n",
      "Iteration: 1196. Loss: 0.19128981232643127\n",
      "Iteration: 1197. Loss: 0.22524525225162506\n",
      "Iteration: 1198. Loss: 0.2498517781496048\n",
      "Iteration: 1199. Loss: 0.1934444010257721\n",
      "Iteration: 1200. Loss: 0.17905135452747345\n",
      "Iteration: 1201. Loss: 0.3833288848400116\n",
      "Iteration: 1202. Loss: 0.22740256786346436\n",
      "Iteration: 1203. Loss: 0.3433339595794678\n",
      "Iteration: 1204. Loss: 0.3502354323863983\n",
      "Iteration: 1205. Loss: 0.2441372275352478\n",
      "Iteration: 1206. Loss: 0.12485459446907043\n",
      "Iteration: 1207. Loss: 0.1663363128900528\n",
      "Iteration: 1208. Loss: 0.10614261031150818\n",
      "Iteration: 1209. Loss: 0.17061792314052582\n",
      "Iteration: 1210. Loss: 0.12979649007320404\n",
      "Iteration: 1211. Loss: 0.1913181096315384\n",
      "Iteration: 1212. Loss: 0.19843856990337372\n",
      "Iteration: 1213. Loss: 0.24020203948020935\n",
      "Iteration: 1214. Loss: 0.14097800850868225\n",
      "Iteration: 1215. Loss: 0.19270944595336914\n",
      "Iteration: 1216. Loss: 0.18397019803524017\n",
      "Iteration: 1217. Loss: 0.25024184584617615\n",
      "Iteration: 1218. Loss: 0.24398940801620483\n",
      "Iteration: 1219. Loss: 0.15323108434677124\n",
      "Iteration: 1220. Loss: 0.2936517298221588\n",
      "Iteration: 1221. Loss: 0.14946141839027405\n",
      "Iteration: 1222. Loss: 0.14124470949172974\n",
      "Iteration: 1223. Loss: 0.1494472622871399\n",
      "Iteration: 1224. Loss: 0.13930997252464294\n",
      "Iteration: 1225. Loss: 0.08846868574619293\n",
      "Iteration: 1226. Loss: 0.1351802498102188\n",
      "Iteration: 1227. Loss: 0.13788264989852905\n",
      "Iteration: 1228. Loss: 0.17608074843883514\n",
      "Iteration: 1229. Loss: 0.1680973619222641\n",
      "Iteration: 1230. Loss: 0.059015121310949326\n",
      "Iteration: 1231. Loss: 0.12240393459796906\n",
      "Iteration: 1232. Loss: 0.11411202698945999\n",
      "Iteration: 1233. Loss: 0.23230735957622528\n",
      "Iteration: 1234. Loss: 0.1519617736339569\n",
      "Iteration: 1235. Loss: 0.17263658344745636\n",
      "Iteration: 1236. Loss: 0.0957782045006752\n",
      "Iteration: 1237. Loss: 0.18558353185653687\n",
      "Iteration: 1238. Loss: 0.1574869602918625\n",
      "Iteration: 1239. Loss: 0.2148691564798355\n",
      "Iteration: 1240. Loss: 0.3437679708003998\n",
      "Iteration: 1241. Loss: 0.16089986264705658\n",
      "Iteration: 1242. Loss: 0.23758240044116974\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1243. Loss: 0.2300473153591156\n",
      "Iteration: 1244. Loss: 0.19182845950126648\n",
      "Iteration: 1245. Loss: 0.2804933786392212\n",
      "Iteration: 1246. Loss: 0.22534990310668945\n",
      "Iteration: 1247. Loss: 0.1606300324201584\n",
      "Iteration: 1248. Loss: 0.21813608705997467\n",
      "Iteration: 1249. Loss: 0.2261546552181244\n",
      "Iteration: 1250. Loss: 0.23019638657569885\n",
      "Iteration: 1251. Loss: 0.16906596720218658\n",
      "Iteration: 1252. Loss: 0.21594925224781036\n",
      "Iteration: 1253. Loss: 0.326426237821579\n",
      "Iteration: 1254. Loss: 0.2085643708705902\n",
      "Iteration: 1255. Loss: 0.20136278867721558\n",
      "Iteration: 1256. Loss: 0.2671079933643341\n",
      "Iteration: 1257. Loss: 0.08294112235307693\n",
      "Iteration: 1258. Loss: 0.31445106863975525\n",
      "Iteration: 1259. Loss: 0.2343633621931076\n",
      "Iteration: 1260. Loss: 0.22963634133338928\n",
      "Iteration: 1261. Loss: 0.29970982670783997\n",
      "Iteration: 1262. Loss: 0.22053951025009155\n",
      "Iteration: 1263. Loss: 0.13376396894454956\n",
      "Iteration: 1264. Loss: 0.15164993703365326\n",
      "Iteration: 1265. Loss: 0.17599809169769287\n",
      "Iteration: 1266. Loss: 0.1948937624692917\n",
      "Iteration: 1267. Loss: 0.17128410935401917\n",
      "Iteration: 1268. Loss: 0.12765000760555267\n",
      "Iteration: 1269. Loss: 0.13780950009822845\n",
      "Iteration: 1270. Loss: 0.10458534210920334\n",
      "Iteration: 1271. Loss: 0.22734300792217255\n",
      "Iteration: 1272. Loss: 0.23470093309879303\n",
      "Iteration: 1273. Loss: 0.16506387293338776\n",
      "Iteration: 1274. Loss: 0.0814419835805893\n",
      "Iteration: 1275. Loss: 0.12442196160554886\n",
      "Iteration: 1276. Loss: 0.2129202038049698\n",
      "Iteration: 1277. Loss: 0.18885891139507294\n",
      "Iteration: 1278. Loss: 0.13199803233146667\n",
      "Iteration: 1279. Loss: 0.2847239077091217\n",
      "Iteration: 1280. Loss: 0.18492433428764343\n",
      "Iteration: 1281. Loss: 0.102032870054245\n",
      "Iteration: 1282. Loss: 0.10222963243722916\n",
      "Iteration: 1283. Loss: 0.19196808338165283\n",
      "Iteration: 1284. Loss: 0.15065997838974\n",
      "Iteration: 1285. Loss: 0.2401537448167801\n",
      "Iteration: 1286. Loss: 0.08292495459318161\n",
      "Iteration: 1287. Loss: 0.15460850298404694\n",
      "Iteration: 1288. Loss: 0.202309250831604\n",
      "Iteration: 1289. Loss: 0.16310566663742065\n",
      "Iteration: 1290. Loss: 0.13224422931671143\n",
      "Iteration: 1291. Loss: 0.19019345939159393\n",
      "Iteration: 1292. Loss: 0.1540185660123825\n",
      "Iteration: 1293. Loss: 0.19993799924850464\n",
      "Iteration: 1294. Loss: 0.21340902149677277\n",
      "Iteration: 1295. Loss: 0.15593580901622772\n",
      "Iteration: 1296. Loss: 0.2944117486476898\n",
      "Iteration: 1297. Loss: 0.19387879967689514\n",
      "Iteration: 1298. Loss: 0.17305809259414673\n",
      "Iteration: 1299. Loss: 0.13357992470264435\n",
      "Iteration: 1300. Loss: 0.276394248008728\n",
      "Iteration: 1301. Loss: 0.2807513177394867\n",
      "Iteration: 1302. Loss: 0.12558762729167938\n",
      "Iteration: 1303. Loss: 0.13668474555015564\n",
      "Iteration: 1304. Loss: 0.15624430775642395\n",
      "Iteration: 1305. Loss: 0.1250389814376831\n",
      "Iteration: 1306. Loss: 0.18876683712005615\n",
      "Iteration: 1307. Loss: 0.12271137535572052\n",
      "Iteration: 1308. Loss: 0.2976129353046417\n",
      "Iteration: 1309. Loss: 0.17593976855278015\n",
      "Iteration: 1310. Loss: 0.1935880333185196\n",
      "Iteration: 1311. Loss: 0.16613461077213287\n",
      "Iteration: 1312. Loss: 0.17223398387432098\n",
      "Iteration: 1313. Loss: 0.2029152512550354\n",
      "Iteration: 1314. Loss: 0.17993736267089844\n",
      "Iteration: 1315. Loss: 0.09682978689670563\n",
      "Iteration: 1316. Loss: 0.2065632939338684\n",
      "Iteration: 1317. Loss: 0.14682649075984955\n",
      "Iteration: 1318. Loss: 0.1552184373140335\n",
      "Iteration: 1319. Loss: 0.21068361401557922\n",
      "Iteration: 1320. Loss: 0.14606539905071259\n",
      "Iteration: 1321. Loss: 0.1108158677816391\n",
      "Iteration: 1322. Loss: 0.2146052122116089\n",
      "Iteration: 1323. Loss: 0.2633996307849884\n",
      "Iteration: 1324. Loss: 0.18439117074012756\n",
      "Iteration: 1325. Loss: 0.13935290277004242\n",
      "Iteration: 1326. Loss: 0.16199713945388794\n",
      "Iteration: 1327. Loss: 0.3274689018726349\n",
      "Iteration: 1328. Loss: 0.0819641649723053\n",
      "Iteration: 1329. Loss: 0.22984592616558075\n",
      "Iteration: 1330. Loss: 0.4427624046802521\n",
      "Iteration: 1331. Loss: 0.1372639685869217\n",
      "Iteration: 1332. Loss: 0.15512163937091827\n",
      "Iteration: 1333. Loss: 0.0833248421549797\n",
      "Iteration: 1334. Loss: 0.22632896900177002\n",
      "Iteration: 1335. Loss: 0.11201658099889755\n",
      "Iteration: 1336. Loss: 0.26440197229385376\n",
      "Iteration: 1337. Loss: 0.2391592264175415\n",
      "Iteration: 1338. Loss: 0.17245443165302277\n",
      "Iteration: 1339. Loss: 0.11812366545200348\n",
      "Iteration: 1340. Loss: 0.14828868210315704\n",
      "Iteration: 1341. Loss: 0.17485105991363525\n",
      "Iteration: 1342. Loss: 0.1795053631067276\n",
      "Iteration: 1343. Loss: 0.2780475318431854\n",
      "Iteration: 1344. Loss: 0.20136719942092896\n",
      "Iteration: 1345. Loss: 0.25427645444869995\n",
      "Iteration: 1346. Loss: 0.2907409369945526\n",
      "Iteration: 1347. Loss: 0.20182771980762482\n",
      "Iteration: 1348. Loss: 0.26150768995285034\n",
      "Iteration: 1349. Loss: 0.12377164512872696\n",
      "Iteration: 1350. Loss: 0.13323232531547546\n",
      "Iteration: 1351. Loss: 0.2133823186159134\n",
      "Iteration: 1352. Loss: 0.16401827335357666\n",
      "Iteration: 1353. Loss: 0.10114341974258423\n",
      "Iteration: 1354. Loss: 0.21716690063476562\n",
      "Iteration: 1355. Loss: 0.15535949170589447\n",
      "Iteration: 1356. Loss: 0.2750343978404999\n",
      "Iteration: 1357. Loss: 0.1698685586452484\n",
      "Iteration: 1358. Loss: 0.17702600359916687\n",
      "Iteration: 1359. Loss: 0.41661083698272705\n",
      "Iteration: 1360. Loss: 0.1733454167842865\n",
      "Iteration: 1361. Loss: 0.2673051953315735\n",
      "Iteration: 1362. Loss: 0.09850291907787323\n",
      "Iteration: 1363. Loss: 0.0833975225687027\n",
      "Iteration: 1364. Loss: 0.12145840376615524\n",
      "Iteration: 1365. Loss: 0.1653180569410324\n",
      "Iteration: 1366. Loss: 0.21409361064434052\n",
      "Iteration: 1367. Loss: 0.16619902849197388\n",
      "Iteration: 1368. Loss: 0.2195817530155182\n",
      "Iteration: 1369. Loss: 0.2560250759124756\n",
      "Iteration: 1370. Loss: 0.14011508226394653\n",
      "Iteration: 1371. Loss: 0.24005304276943207\n",
      "Iteration: 1372. Loss: 0.06138566881418228\n",
      "Iteration: 1373. Loss: 0.15512043237686157\n",
      "Iteration: 1374. Loss: 0.15166668593883514\n",
      "Iteration: 1375. Loss: 0.2006908804178238\n",
      "Iteration: 1376. Loss: 0.226463183760643\n",
      "Iteration: 1377. Loss: 0.07633836567401886\n",
      "Iteration: 1378. Loss: 0.13790373504161835\n",
      "Iteration: 1379. Loss: 0.21657207608222961\n",
      "Iteration: 1380. Loss: 0.12624773383140564\n",
      "Iteration: 1381. Loss: 0.24022826552391052\n",
      "Iteration: 1382. Loss: 0.08482718467712402\n",
      "Iteration: 1383. Loss: 0.16547589004039764\n",
      "Iteration: 1384. Loss: 0.127580925822258\n",
      "Iteration: 1385. Loss: 0.13195401430130005\n",
      "Iteration: 1386. Loss: 0.1893664300441742\n",
      "Iteration: 1387. Loss: 0.09605047851800919\n",
      "Iteration: 1388. Loss: 0.16002647578716278\n",
      "Iteration: 1389. Loss: 0.21895664930343628\n",
      "Iteration: 1390. Loss: 0.2390402853488922\n",
      "Iteration: 1391. Loss: 0.18575578927993774\n",
      "Iteration: 1392. Loss: 0.07867556810379028\n",
      "Iteration: 1393. Loss: 0.10790473967790604\n",
      "Iteration: 1394. Loss: 0.10040954500436783\n",
      "Iteration: 1395. Loss: 0.235993430018425\n",
      "Iteration: 1396. Loss: 0.09359483420848846\n",
      "Iteration: 1397. Loss: 0.21441036462783813\n",
      "Iteration: 1398. Loss: 0.293460875749588\n",
      "Iteration: 1399. Loss: 0.14769913256168365\n",
      "Iteration: 1400. Loss: 0.16703476011753082\n",
      "Iteration: 1401. Loss: 0.22428016364574432\n",
      "Iteration: 1402. Loss: 0.20898818969726562\n",
      "Iteration: 1403. Loss: 0.14402443170547485\n",
      "Iteration: 1404. Loss: 0.2354895919561386\n",
      "Iteration: 1405. Loss: 0.0884004756808281\n",
      "Iteration: 1406. Loss: 0.10553396493196487\n",
      "Iteration: 1407. Loss: 0.24617119133472443\n",
      "Iteration: 1408. Loss: 0.1646648794412613\n",
      "Iteration: 1409. Loss: 0.15779316425323486\n",
      "Iteration: 1410. Loss: 0.17325887084007263\n",
      "Iteration: 1411. Loss: 0.12128040194511414\n",
      "Iteration: 1412. Loss: 0.20409470796585083\n",
      "Iteration: 1413. Loss: 0.19287723302841187\n",
      "Iteration: 1414. Loss: 0.26789578795433044\n",
      "Iteration: 1415. Loss: 0.1711808741092682\n",
      "Iteration: 1416. Loss: 0.23039750754833221\n",
      "Iteration: 1417. Loss: 0.21920102834701538\n",
      "Iteration: 1418. Loss: 0.15542586147785187\n",
      "Iteration: 1419. Loss: 0.32562825083732605\n",
      "Iteration: 1420. Loss: 0.1637265533208847\n",
      "Iteration: 1421. Loss: 0.3003838062286377\n",
      "Iteration: 1422. Loss: 0.1980813443660736\n",
      "Iteration: 1423. Loss: 0.1980508416891098\n",
      "Iteration: 1424. Loss: 0.21866117417812347\n",
      "Iteration: 1425. Loss: 0.2194126546382904\n",
      "Iteration: 1426. Loss: 0.21607834100723267\n",
      "Iteration: 1427. Loss: 0.1538572609424591\n",
      "Iteration: 1428. Loss: 0.18327952921390533\n",
      "Iteration: 1429. Loss: 0.2681128978729248\n",
      "Iteration: 1430. Loss: 0.18446007370948792\n",
      "Iteration: 1431. Loss: 0.1461082398891449\n",
      "Iteration: 1432. Loss: 0.2616543471813202\n",
      "Iteration: 1433. Loss: 0.22133025527000427\n",
      "Iteration: 1434. Loss: 0.1640893816947937\n",
      "Iteration: 1435. Loss: 0.1932293176651001\n",
      "Iteration: 1436. Loss: 0.14668047428131104\n",
      "Iteration: 1437. Loss: 0.18307816982269287\n",
      "Iteration: 1438. Loss: 0.21283015608787537\n",
      "Iteration: 1439. Loss: 0.2237304300069809\n",
      "Iteration: 1440. Loss: 0.14953117072582245\n",
      "Iteration: 1441. Loss: 0.08095471560955048\n",
      "Iteration: 1442. Loss: 0.15220637619495392\n",
      "Iteration: 1443. Loss: 0.22235964238643646\n",
      "Iteration: 1444. Loss: 0.24689102172851562\n",
      "Iteration: 1445. Loss: 0.21812845766544342\n",
      "Iteration: 1446. Loss: 0.2250826209783554\n",
      "Iteration: 1447. Loss: 0.23774337768554688\n",
      "Iteration: 1448. Loss: 0.27838972210884094\n",
      "Iteration: 1449. Loss: 0.2951626479625702\n",
      "Iteration: 1450. Loss: 0.1926560401916504\n",
      "Iteration: 1451. Loss: 0.158749058842659\n",
      "Iteration: 1452. Loss: 0.14027805626392365\n",
      "Iteration: 1453. Loss: 0.1521819680929184\n",
      "Iteration: 1454. Loss: 0.10377733409404755\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1455. Loss: 0.20864222943782806\n",
      "Iteration: 1456. Loss: 0.1407012641429901\n",
      "Iteration: 1457. Loss: 0.14702217280864716\n",
      "Iteration: 1458. Loss: 0.12444231659173965\n",
      "Iteration: 1459. Loss: 0.16282939910888672\n",
      "Iteration: 1460. Loss: 0.14273768663406372\n",
      "Iteration: 1461. Loss: 0.18562918901443481\n",
      "Iteration: 1462. Loss: 0.12535293400287628\n",
      "Iteration: 1463. Loss: 0.09136098623275757\n",
      "Iteration: 1464. Loss: 0.237280011177063\n",
      "Iteration: 1465. Loss: 0.1835484355688095\n",
      "Iteration: 1466. Loss: 0.13793104887008667\n",
      "Iteration: 1467. Loss: 0.16291353106498718\n",
      "Iteration: 1468. Loss: 0.2144044041633606\n",
      "Iteration: 1469. Loss: 0.22938823699951172\n",
      "Iteration: 1470. Loss: 0.09495945274829865\n",
      "Iteration: 1471. Loss: 0.16716113686561584\n",
      "Iteration: 1472. Loss: 0.21421998739242554\n",
      "Iteration: 1473. Loss: 0.2570391893386841\n",
      "Iteration: 1474. Loss: 0.2102416604757309\n",
      "Iteration: 1475. Loss: 0.21700622141361237\n",
      "Iteration: 1476. Loss: 0.25264301896095276\n",
      "Iteration: 1477. Loss: 0.21531341969966888\n",
      "Iteration: 1478. Loss: 0.18958351016044617\n",
      "Iteration: 1479. Loss: 0.11887850612401962\n",
      "Iteration: 1480. Loss: 0.1495942622423172\n",
      "Iteration: 1481. Loss: 0.1148676797747612\n",
      "Iteration: 1482. Loss: 0.1328403651714325\n",
      "Iteration: 1483. Loss: 0.3029695153236389\n",
      "Iteration: 1484. Loss: 0.1606481373310089\n",
      "Iteration: 1485. Loss: 0.2128903567790985\n",
      "Iteration: 1486. Loss: 0.15604233741760254\n",
      "Iteration: 1487. Loss: 0.09670646488666534\n",
      "Iteration: 1488. Loss: 0.20734809339046478\n",
      "Iteration: 1489. Loss: 0.10078734159469604\n",
      "Iteration: 1490. Loss: 0.19909809529781342\n",
      "Iteration: 1491. Loss: 0.18504828214645386\n",
      "Iteration: 1492. Loss: 0.20626379549503326\n",
      "Iteration: 1493. Loss: 0.07056939601898193\n",
      "Iteration: 1494. Loss: 0.13571226596832275\n",
      "Iteration: 1495. Loss: 0.14785590767860413\n",
      "Iteration: 1496. Loss: 0.17259272933006287\n",
      "Iteration: 1497. Loss: 0.13584163784980774\n",
      "Iteration: 1498. Loss: 0.19126223027706146\n",
      "Iteration: 1499. Loss: 0.2843256890773773\n",
      "Iteration: 1500. Loss: 0.22556264698505402\n",
      "Iteration: 1501. Loss: 0.26681315898895264\n",
      "Iteration: 1502. Loss: 0.20541727542877197\n",
      "Iteration: 1503. Loss: 0.12203427404165268\n",
      "Iteration: 1504. Loss: 0.27447783946990967\n",
      "Iteration: 1505. Loss: 0.3128509223461151\n",
      "Iteration: 1506. Loss: 0.05885476619005203\n",
      "Iteration: 1507. Loss: 0.08468098938465118\n",
      "Iteration: 1508. Loss: 0.1957852989435196\n",
      "Iteration: 1509. Loss: 0.2697060704231262\n",
      "Iteration: 1510. Loss: 0.12010487914085388\n",
      "Iteration: 1511. Loss: 0.17568746209144592\n",
      "Iteration: 1512. Loss: 0.2441893368959427\n",
      "Iteration: 1513. Loss: 0.15723147988319397\n",
      "Iteration: 1514. Loss: 0.14355072379112244\n",
      "Iteration: 1515. Loss: 0.14916831254959106\n",
      "Iteration: 1516. Loss: 0.15170162916183472\n",
      "Iteration: 1517. Loss: 0.24248626828193665\n",
      "Iteration: 1518. Loss: 0.16868379712104797\n",
      "Iteration: 1519. Loss: 0.1917387843132019\n",
      "Iteration: 1520. Loss: 0.21445642411708832\n",
      "Iteration: 1521. Loss: 0.09530620276927948\n",
      "Iteration: 1522. Loss: 0.2406928390264511\n",
      "Iteration: 1523. Loss: 0.19464164972305298\n",
      "Iteration: 1524. Loss: 0.22313930094242096\n",
      "Iteration: 1525. Loss: 0.14380578696727753\n",
      "Iteration: 1526. Loss: 0.18930359184741974\n",
      "Iteration: 1527. Loss: 0.13258656859397888\n",
      "Iteration: 1528. Loss: 0.11612288653850555\n",
      "Iteration: 1529. Loss: 0.048348888754844666\n",
      "Iteration: 1530. Loss: 0.11005754768848419\n",
      "Iteration: 1531. Loss: 0.18187031149864197\n",
      "Iteration: 1532. Loss: 0.07558248192071915\n",
      "Iteration: 1533. Loss: 0.3148406147956848\n",
      "Iteration: 1534. Loss: 0.14093124866485596\n",
      "Iteration: 1535. Loss: 0.07613852620124817\n",
      "Iteration: 1536. Loss: 0.2564834654331207\n",
      "Iteration: 1537. Loss: 0.12411431223154068\n",
      "Iteration: 1538. Loss: 0.13251027464866638\n",
      "Iteration: 1539. Loss: 0.053414326161146164\n",
      "Iteration: 1540. Loss: 0.12005234509706497\n",
      "Iteration: 1541. Loss: 0.22901001572608948\n",
      "Iteration: 1542. Loss: 0.1635928750038147\n",
      "Iteration: 1543. Loss: 0.05833864584565163\n",
      "Iteration: 1544. Loss: 0.24763792753219604\n",
      "Iteration: 1545. Loss: 0.21470636129379272\n",
      "Iteration: 1546. Loss: 0.16317811608314514\n",
      "Iteration: 1547. Loss: 0.07840725779533386\n",
      "Iteration: 1548. Loss: 0.07616251707077026\n",
      "Iteration: 1549. Loss: 0.1976526975631714\n",
      "Iteration: 1550. Loss: 0.12914103269577026\n",
      "Iteration: 1551. Loss: 0.16706176102161407\n",
      "Iteration: 1552. Loss: 0.14993511140346527\n",
      "Iteration: 1553. Loss: 0.3182201683521271\n",
      "Iteration: 1554. Loss: 0.059859536588191986\n",
      "Iteration: 1555. Loss: 0.23256038129329681\n",
      "Iteration: 1556. Loss: 0.1842748075723648\n",
      "Iteration: 1557. Loss: 0.14712105691432953\n",
      "Iteration: 1558. Loss: 0.2131982445716858\n",
      "Iteration: 1559. Loss: 0.126848965883255\n",
      "Iteration: 1560. Loss: 0.09818629920482635\n",
      "Iteration: 1561. Loss: 0.1712959110736847\n",
      "Iteration: 1562. Loss: 0.20090371370315552\n",
      "Iteration: 1563. Loss: 0.12663114070892334\n",
      "Iteration: 1564. Loss: 0.09490427374839783\n",
      "Iteration: 1565. Loss: 0.13157136738300323\n",
      "Iteration: 1566. Loss: 0.1117694154381752\n",
      "Iteration: 1567. Loss: 0.07108612358570099\n",
      "Iteration: 1568. Loss: 0.1921425312757492\n",
      "Iteration: 1569. Loss: 0.1639581322669983\n",
      "Iteration: 1570. Loss: 0.3031773567199707\n",
      "Iteration: 1571. Loss: 0.18073824048042297\n",
      "Iteration: 1572. Loss: 0.12352058291435242\n",
      "Iteration: 1573. Loss: 0.19245372712612152\n",
      "Iteration: 1574. Loss: 0.17514772713184357\n",
      "Iteration: 1575. Loss: 0.2891756296157837\n",
      "Iteration: 1576. Loss: 0.14080342650413513\n",
      "Iteration: 1577. Loss: 0.23162998259067535\n",
      "Iteration: 1578. Loss: 0.2306053191423416\n",
      "Iteration: 1579. Loss: 0.15385423600673676\n",
      "Iteration: 1580. Loss: 0.14588114619255066\n",
      "Iteration: 1581. Loss: 0.19629667699337006\n",
      "Iteration: 1582. Loss: 0.1709357053041458\n",
      "Iteration: 1583. Loss: 0.04252404347062111\n",
      "Iteration: 1584. Loss: 0.27007153630256653\n",
      "Iteration: 1585. Loss: 0.19129543006420135\n",
      "Iteration: 1586. Loss: 0.1669531613588333\n",
      "Iteration: 1587. Loss: 0.16595301032066345\n",
      "Iteration: 1588. Loss: 0.17329400777816772\n",
      "Iteration: 1589. Loss: 0.11829273402690887\n",
      "Iteration: 1590. Loss: 0.16236430406570435\n",
      "Iteration: 1591. Loss: 0.16129688918590546\n",
      "Iteration: 1592. Loss: 0.09862910211086273\n",
      "Iteration: 1593. Loss: 0.12089479714632034\n",
      "Iteration: 1594. Loss: 0.20989318192005157\n",
      "Iteration: 1595. Loss: 0.19268183410167694\n",
      "Iteration: 1596. Loss: 0.13589893281459808\n",
      "Iteration: 1597. Loss: 0.15472161769866943\n",
      "Iteration: 1598. Loss: 0.1992008537054062\n",
      "Iteration: 1599. Loss: 0.27417030930519104\n",
      "Iteration: 1600. Loss: 0.16725566983222961\n",
      "Iteration: 1601. Loss: 0.10049375891685486\n",
      "Iteration: 1602. Loss: 0.11757256835699081\n",
      "Iteration: 1603. Loss: 0.2757319211959839\n",
      "Iteration: 1604. Loss: 0.13449159264564514\n",
      "Iteration: 1605. Loss: 0.12421612441539764\n",
      "Iteration: 1606. Loss: 0.2432190328836441\n",
      "Iteration: 1607. Loss: 0.09106417745351791\n",
      "Iteration: 1608. Loss: 0.2585655152797699\n",
      "Iteration: 1609. Loss: 0.30343568325042725\n",
      "Iteration: 1610. Loss: 0.14844278991222382\n",
      "Iteration: 1611. Loss: 0.20818956196308136\n",
      "Iteration: 1612. Loss: 0.13538923859596252\n",
      "Iteration: 1613. Loss: 0.1665995568037033\n",
      "Iteration: 1614. Loss: 0.09186925739049911\n",
      "Iteration: 1615. Loss: 0.2304389327764511\n",
      "Iteration: 1616. Loss: 0.31233131885528564\n",
      "Iteration: 1617. Loss: 0.1398165076971054\n",
      "Iteration: 1618. Loss: 0.13743235170841217\n",
      "Iteration: 1619. Loss: 0.16037891805171967\n",
      "Iteration: 1620. Loss: 0.2766166925430298\n",
      "Iteration: 1621. Loss: 0.250651091337204\n",
      "Iteration: 1622. Loss: 0.0848279669880867\n",
      "Iteration: 1623. Loss: 0.2574786841869354\n",
      "Iteration: 1624. Loss: 0.1294873207807541\n",
      "Iteration: 1625. Loss: 0.26168736815452576\n",
      "Iteration: 1626. Loss: 0.1698894500732422\n",
      "Iteration: 1627. Loss: 0.12089581787586212\n",
      "Iteration: 1628. Loss: 0.21360015869140625\n",
      "Iteration: 1629. Loss: 0.19650086760520935\n",
      "Iteration: 1630. Loss: 0.10945975035429001\n",
      "Iteration: 1631. Loss: 0.2086423933506012\n",
      "Iteration: 1632. Loss: 0.10162761807441711\n",
      "Iteration: 1633. Loss: 0.10672108829021454\n",
      "Iteration: 1634. Loss: 0.2238541543483734\n",
      "Iteration: 1635. Loss: 0.20692087709903717\n",
      "Iteration: 1636. Loss: 0.18388818204402924\n",
      "Iteration: 1637. Loss: 0.08724746853113174\n",
      "Iteration: 1638. Loss: 0.17936450242996216\n",
      "Iteration: 1639. Loss: 0.1608162671327591\n",
      "Iteration: 1640. Loss: 0.1456584632396698\n",
      "Iteration: 1641. Loss: 0.12318222969770432\n",
      "Iteration: 1642. Loss: 0.20883285999298096\n",
      "Iteration: 1643. Loss: 0.09885410219430923\n",
      "Iteration: 1644. Loss: 0.29193875193595886\n",
      "Iteration: 1645. Loss: 0.10166355967521667\n",
      "Iteration: 1646. Loss: 0.11160482466220856\n",
      "Iteration: 1647. Loss: 0.08706015348434448\n",
      "Iteration: 1648. Loss: 0.1566171497106552\n",
      "Iteration: 1649. Loss: 0.1437751054763794\n",
      "Iteration: 1650. Loss: 0.09973453730344772\n",
      "Iteration: 1651. Loss: 0.10082998871803284\n",
      "Iteration: 1652. Loss: 0.10914145410060883\n",
      "Iteration: 1653. Loss: 0.04847283288836479\n",
      "Iteration: 1654. Loss: 0.18472766876220703\n",
      "Iteration: 1655. Loss: 0.08100449293851852\n",
      "Iteration: 1656. Loss: 0.18299084901809692\n",
      "Iteration: 1657. Loss: 0.09528925269842148\n",
      "Iteration: 1658. Loss: 0.11548996716737747\n",
      "Iteration: 1659. Loss: 0.07630280405282974\n",
      "Iteration: 1660. Loss: 0.13115371763706207\n",
      "Iteration: 1661. Loss: 0.16268503665924072\n",
      "Iteration: 1662. Loss: 0.2548663914203644\n",
      "Iteration: 1663. Loss: 0.1239921972155571\n",
      "Iteration: 1664. Loss: 0.13445129990577698\n",
      "Iteration: 1665. Loss: 0.18879461288452148\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1666. Loss: 0.07227207720279694\n",
      "Iteration: 1667. Loss: 0.18023069202899933\n",
      "Iteration: 1668. Loss: 0.16704846918582916\n",
      "Iteration: 1669. Loss: 0.22658509016036987\n",
      "Iteration: 1670. Loss: 0.18165569007396698\n",
      "Iteration: 1671. Loss: 0.13240481913089752\n",
      "Iteration: 1672. Loss: 0.14248567819595337\n",
      "Iteration: 1673. Loss: 0.12971214950084686\n",
      "Iteration: 1674. Loss: 0.15662698447704315\n",
      "Iteration: 1675. Loss: 0.16601881384849548\n",
      "Iteration: 1676. Loss: 0.2821389138698578\n",
      "Iteration: 1677. Loss: 0.1418754607439041\n",
      "Iteration: 1678. Loss: 0.15808002650737762\n",
      "Iteration: 1679. Loss: 0.10914373397827148\n",
      "Iteration: 1680. Loss: 0.18767137825489044\n",
      "Iteration: 1681. Loss: 0.04923393204808235\n",
      "Iteration: 1682. Loss: 0.12128537893295288\n",
      "Iteration: 1683. Loss: 0.1040138527750969\n",
      "Iteration: 1684. Loss: 0.1733105629682541\n",
      "Iteration: 1685. Loss: 0.21557939052581787\n",
      "Iteration: 1686. Loss: 0.3073490560054779\n",
      "Iteration: 1687. Loss: 0.15158458054065704\n",
      "Iteration: 1688. Loss: 0.14661437273025513\n",
      "Iteration: 1689. Loss: 0.14922653138637543\n",
      "Iteration: 1690. Loss: 0.10133982449769974\n",
      "Iteration: 1691. Loss: 0.1413433998823166\n",
      "Iteration: 1692. Loss: 0.13261842727661133\n",
      "Iteration: 1693. Loss: 0.1946374475955963\n",
      "Iteration: 1694. Loss: 0.16312247514724731\n",
      "Iteration: 1695. Loss: 0.1584632247686386\n",
      "Iteration: 1696. Loss: 0.16720598936080933\n",
      "Iteration: 1697. Loss: 0.1283816546201706\n",
      "Iteration: 1698. Loss: 0.1912519633769989\n",
      "Iteration: 1699. Loss: 0.12704643607139587\n",
      "Iteration: 1700. Loss: 0.2484644651412964\n",
      "Iteration: 1701. Loss: 0.17014841735363007\n",
      "Iteration: 1702. Loss: 0.14591793715953827\n",
      "Iteration: 1703. Loss: 0.17828983068466187\n",
      "Iteration: 1704. Loss: 0.08804506063461304\n",
      "Iteration: 1705. Loss: 0.15157781541347504\n",
      "Iteration: 1706. Loss: 0.3037087023258209\n",
      "Iteration: 1707. Loss: 0.29052865505218506\n",
      "Iteration: 1708. Loss: 0.1593192219734192\n",
      "Iteration: 1709. Loss: 0.18469519913196564\n",
      "Iteration: 1710. Loss: 0.15993337333202362\n",
      "Iteration: 1711. Loss: 0.15651626884937286\n",
      "Iteration: 1712. Loss: 0.08927097916603088\n",
      "Iteration: 1713. Loss: 0.16103984415531158\n",
      "Iteration: 1714. Loss: 0.14166486263275146\n",
      "Iteration: 1715. Loss: 0.3011559545993805\n",
      "Iteration: 1716. Loss: 0.12605705857276917\n",
      "Iteration: 1717. Loss: 0.1338915377855301\n",
      "Iteration: 1718. Loss: 0.23897770047187805\n",
      "Iteration: 1719. Loss: 0.22540932893753052\n",
      "Iteration: 1720. Loss: 0.4179462492465973\n",
      "Iteration: 1721. Loss: 0.1523871123790741\n",
      "Iteration: 1722. Loss: 0.21622563898563385\n",
      "Iteration: 1723. Loss: 0.22162258625030518\n",
      "Iteration: 1724. Loss: 0.17336870729923248\n",
      "Iteration: 1725. Loss: 0.19862313568592072\n",
      "Iteration: 1726. Loss: 0.20204775035381317\n",
      "Iteration: 1727. Loss: 0.12114698439836502\n",
      "Iteration: 1728. Loss: 0.08996997773647308\n",
      "Iteration: 1729. Loss: 0.23105646669864655\n",
      "Iteration: 1730. Loss: 0.17408645153045654\n",
      "Iteration: 1731. Loss: 0.27506202459335327\n",
      "Iteration: 1732. Loss: 0.12275909632444382\n",
      "Iteration: 1733. Loss: 0.12458641082048416\n",
      "Iteration: 1734. Loss: 0.16561612486839294\n",
      "Iteration: 1735. Loss: 0.14216755330562592\n",
      "Iteration: 1736. Loss: 0.21503959596157074\n",
      "Iteration: 1737. Loss: 0.12053926289081573\n",
      "Iteration: 1738. Loss: 0.16485120356082916\n",
      "Iteration: 1739. Loss: 0.12696915864944458\n",
      "Iteration: 1740. Loss: 0.379567414522171\n",
      "Iteration: 1741. Loss: 0.2080681025981903\n",
      "Iteration: 1742. Loss: 0.10237355530261993\n",
      "Iteration: 1743. Loss: 0.13565702736377716\n",
      "Iteration: 1744. Loss: 0.08897504955530167\n",
      "Iteration: 1745. Loss: 0.1645890772342682\n",
      "Iteration: 1746. Loss: 0.215744748711586\n",
      "Iteration: 1747. Loss: 0.08551894873380661\n",
      "Iteration: 1748. Loss: 0.19019269943237305\n",
      "Iteration: 1749. Loss: 0.1525188684463501\n",
      "Iteration: 1750. Loss: 0.13046325743198395\n",
      "Iteration: 1751. Loss: 0.16501769423484802\n",
      "Iteration: 1752. Loss: 0.3059751093387604\n",
      "Iteration: 1753. Loss: 0.05762935057282448\n",
      "Iteration: 1754. Loss: 0.052627019584178925\n",
      "Iteration: 1755. Loss: 0.17079605162143707\n",
      "Iteration: 1756. Loss: 0.11204000562429428\n",
      "Iteration: 1757. Loss: 0.2624885141849518\n",
      "Iteration: 1758. Loss: 0.271249383687973\n",
      "Iteration: 1759. Loss: 0.17124080657958984\n",
      "Iteration: 1760. Loss: 0.14723104238510132\n",
      "Iteration: 1761. Loss: 0.1358921378850937\n",
      "Iteration: 1762. Loss: 0.14789481461048126\n",
      "Iteration: 1763. Loss: 0.31954309344291687\n",
      "Iteration: 1764. Loss: 0.10271114110946655\n",
      "Iteration: 1765. Loss: 0.11487949639558792\n",
      "Iteration: 1766. Loss: 0.07571018487215042\n",
      "Iteration: 1767. Loss: 0.20064571499824524\n",
      "Iteration: 1768. Loss: 0.16539546847343445\n",
      "Iteration: 1769. Loss: 0.14447647333145142\n",
      "Iteration: 1770. Loss: 0.1119949221611023\n",
      "Iteration: 1771. Loss: 0.24961984157562256\n",
      "Iteration: 1772. Loss: 0.1817038357257843\n",
      "Iteration: 1773. Loss: 0.22781439125537872\n",
      "Iteration: 1774. Loss: 0.15731173753738403\n",
      "Iteration: 1775. Loss: 0.1308584362268448\n",
      "Iteration: 1776. Loss: 0.14565223455429077\n",
      "Iteration: 1777. Loss: 0.12482360005378723\n",
      "Iteration: 1778. Loss: 0.13153645396232605\n",
      "Iteration: 1779. Loss: 0.3491871654987335\n",
      "Iteration: 1780. Loss: 0.27006852626800537\n",
      "Iteration: 1781. Loss: 0.19200578331947327\n",
      "Iteration: 1782. Loss: 0.12095532566308975\n",
      "Iteration: 1783. Loss: 0.07672140747308731\n",
      "Iteration: 1784. Loss: 0.25200915336608887\n",
      "Iteration: 1785. Loss: 0.21665899455547333\n",
      "Iteration: 1786. Loss: 0.21717901527881622\n",
      "Iteration: 1787. Loss: 0.14803434908390045\n",
      "Iteration: 1788. Loss: 0.14089800417423248\n",
      "Iteration: 1789. Loss: 0.0869859904050827\n",
      "Iteration: 1790. Loss: 0.09558837115764618\n",
      "Iteration: 1791. Loss: 0.10960891097784042\n",
      "Iteration: 1792. Loss: 0.12696753442287445\n",
      "Iteration: 1793. Loss: 0.2244356870651245\n",
      "Iteration: 1794. Loss: 0.12411525100469589\n",
      "Iteration: 1795. Loss: 0.11893343180418015\n",
      "Iteration: 1796. Loss: 0.07991839945316315\n",
      "Iteration: 1797. Loss: 0.15234585106372833\n",
      "Iteration: 1798. Loss: 0.16342245042324066\n",
      "Iteration: 1799. Loss: 0.19602897763252258\n",
      "Iteration: 1800. Loss: 0.18066903948783875\n",
      "Iteration: 1801. Loss: 0.14722876250743866\n",
      "Iteration: 1802. Loss: 0.11925569176673889\n",
      "Iteration: 1803. Loss: 0.1865244060754776\n",
      "Iteration: 1804. Loss: 0.09169589728116989\n",
      "Iteration: 1805. Loss: 0.26137664914131165\n",
      "Iteration: 1806. Loss: 0.08491317927837372\n",
      "Iteration: 1807. Loss: 0.12289389967918396\n",
      "Iteration: 1808. Loss: 0.17829877138137817\n",
      "Iteration: 1809. Loss: 0.1628427356481552\n",
      "Iteration: 1810. Loss: 0.15720076858997345\n",
      "Iteration: 1811. Loss: 0.07582166790962219\n",
      "Iteration: 1812. Loss: 0.12040484696626663\n",
      "Iteration: 1813. Loss: 0.11817146092653275\n",
      "Iteration: 1814. Loss: 0.20290987193584442\n",
      "Iteration: 1815. Loss: 0.16762666404247284\n",
      "Iteration: 1816. Loss: 0.1585654765367508\n",
      "Iteration: 1817. Loss: 0.16781938076019287\n",
      "Iteration: 1818. Loss: 0.18975518643856049\n",
      "Iteration: 1819. Loss: 0.14926663041114807\n",
      "Iteration: 1820. Loss: 0.24591384828090668\n",
      "Iteration: 1821. Loss: 0.12236137688159943\n",
      "Iteration: 1822. Loss: 0.10413255542516708\n",
      "Iteration: 1823. Loss: 0.11942465603351593\n",
      "Iteration: 1824. Loss: 0.08914563804864883\n",
      "Iteration: 1825. Loss: 0.1465301364660263\n",
      "Iteration: 1826. Loss: 0.12144935876131058\n",
      "Iteration: 1827. Loss: 0.1400749236345291\n",
      "Iteration: 1828. Loss: 0.1346186399459839\n",
      "Iteration: 1829. Loss: 0.20260965824127197\n",
      "Iteration: 1830. Loss: 0.35192975401878357\n",
      "Iteration: 1831. Loss: 0.22946976125240326\n",
      "Iteration: 1832. Loss: 0.1189989447593689\n",
      "Iteration: 1833. Loss: 0.11690657585859299\n",
      "Iteration: 1834. Loss: 0.13521775603294373\n",
      "Iteration: 1835. Loss: 0.09668388962745667\n",
      "Iteration: 1836. Loss: 0.06951342523097992\n",
      "Iteration: 1837. Loss: 0.09235174208879471\n",
      "Iteration: 1838. Loss: 0.09725365042686462\n",
      "Iteration: 1839. Loss: 0.23694978654384613\n",
      "Iteration: 1840. Loss: 0.13401608169078827\n",
      "Iteration: 1841. Loss: 0.137546107172966\n",
      "Iteration: 1842. Loss: 0.31016167998313904\n",
      "Iteration: 1843. Loss: 0.1256110668182373\n",
      "Iteration: 1844. Loss: 0.10471156984567642\n",
      "Iteration: 1845. Loss: 0.07818810641765594\n",
      "Iteration: 1846. Loss: 0.15762940049171448\n",
      "Iteration: 1847. Loss: 0.16074712574481964\n",
      "Iteration: 1848. Loss: 0.11383479088544846\n",
      "Iteration: 1849. Loss: 0.15649238228797913\n",
      "Iteration: 1850. Loss: 0.10394012182950974\n",
      "Iteration: 1851. Loss: 0.2087165117263794\n",
      "Iteration: 1852. Loss: 0.23041392862796783\n",
      "Iteration: 1853. Loss: 0.18866842985153198\n",
      "Iteration: 1854. Loss: 0.24613922834396362\n",
      "Iteration: 1855. Loss: 0.15288031101226807\n",
      "Iteration: 1856. Loss: 0.1884475201368332\n",
      "Iteration: 1857. Loss: 0.22649498283863068\n",
      "Iteration: 1858. Loss: 0.2634446918964386\n",
      "Iteration: 1859. Loss: 0.10673997551202774\n",
      "Iteration: 1860. Loss: 0.11817049235105515\n",
      "Iteration: 1861. Loss: 0.21826377511024475\n",
      "Iteration: 1862. Loss: 0.2393253743648529\n",
      "Iteration: 1863. Loss: 0.1744648516178131\n",
      "Iteration: 1864. Loss: 0.09664081782102585\n",
      "Iteration: 1865. Loss: 0.2163006067276001\n",
      "Iteration: 1866. Loss: 0.1463170200586319\n",
      "Iteration: 1867. Loss: 0.16148194670677185\n",
      "Iteration: 1868. Loss: 0.15029990673065186\n",
      "Iteration: 1869. Loss: 0.20506645739078522\n",
      "Iteration: 1870. Loss: 0.1981729120016098\n",
      "Iteration: 1871. Loss: 0.0822715014219284\n",
      "Iteration: 1872. Loss: 0.06944293528795242\n",
      "Iteration: 1873. Loss: 0.15022334456443787\n",
      "Iteration: 1874. Loss: 0.24171297252178192\n",
      "Iteration: 1875. Loss: 0.170538991689682\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1876. Loss: 0.13683147728443146\n",
      "Iteration: 1877. Loss: 0.2753555476665497\n",
      "Iteration: 1878. Loss: 0.13018523156642914\n",
      "Iteration: 1879. Loss: 0.19221292436122894\n",
      "Iteration: 1880. Loss: 0.1189299002289772\n",
      "Iteration: 1881. Loss: 0.14850859344005585\n",
      "Iteration: 1882. Loss: 0.09713983535766602\n",
      "Iteration: 1883. Loss: 0.10660839825868607\n",
      "Iteration: 1884. Loss: 0.20686225593090057\n",
      "Iteration: 1885. Loss: 0.07593448460102081\n",
      "Iteration: 1886. Loss: 0.1023217961192131\n",
      "Iteration: 1887. Loss: 0.06173054873943329\n",
      "Iteration: 1888. Loss: 0.06545636802911758\n",
      "Iteration: 1889. Loss: 0.15457206964492798\n",
      "Iteration: 1890. Loss: 0.11386984586715698\n",
      "Iteration: 1891. Loss: 0.21201211214065552\n",
      "Iteration: 1892. Loss: 0.17151764035224915\n",
      "Iteration: 1893. Loss: 0.14718057215213776\n",
      "Iteration: 1894. Loss: 0.17441509664058685\n",
      "Iteration: 1895. Loss: 0.1346878856420517\n",
      "Iteration: 1896. Loss: 0.07990417629480362\n",
      "Iteration: 1897. Loss: 0.1362656056880951\n",
      "Iteration: 1898. Loss: 0.133161723613739\n",
      "Iteration: 1899. Loss: 0.1391175240278244\n",
      "Iteration: 1900. Loss: 0.13167305290699005\n",
      "Iteration: 1901. Loss: 0.12309040129184723\n",
      "Iteration: 1902. Loss: 0.1564895361661911\n",
      "Iteration: 1903. Loss: 0.16764849424362183\n",
      "Iteration: 1904. Loss: 0.10057438164949417\n",
      "Iteration: 1905. Loss: 0.13034406304359436\n",
      "Iteration: 1906. Loss: 0.15315711498260498\n",
      "Iteration: 1907. Loss: 0.08652853965759277\n",
      "Iteration: 1908. Loss: 0.2927166521549225\n",
      "Iteration: 1909. Loss: 0.09412907809019089\n",
      "Iteration: 1910. Loss: 0.07117447257041931\n",
      "Iteration: 1911. Loss: 0.16317792236804962\n",
      "Iteration: 1912. Loss: 0.11105553805828094\n",
      "Iteration: 1913. Loss: 0.2459293007850647\n",
      "Iteration: 1914. Loss: 0.14204524457454681\n",
      "Iteration: 1915. Loss: 0.12459345906972885\n",
      "Iteration: 1916. Loss: 0.10661593079566956\n",
      "Iteration: 1917. Loss: 0.14430542290210724\n",
      "Iteration: 1918. Loss: 0.10641901195049286\n",
      "Iteration: 1919. Loss: 0.1025259718298912\n",
      "Iteration: 1920. Loss: 0.08740159869194031\n",
      "Iteration: 1921. Loss: 0.09264350682497025\n",
      "Iteration: 1922. Loss: 0.19764818251132965\n",
      "Iteration: 1923. Loss: 0.1537391096353531\n",
      "Iteration: 1924. Loss: 0.06254909187555313\n",
      "Iteration: 1925. Loss: 0.14030319452285767\n",
      "Iteration: 1926. Loss: 0.14417660236358643\n",
      "Iteration: 1927. Loss: 0.09783411771059036\n",
      "Iteration: 1928. Loss: 0.27090418338775635\n",
      "Iteration: 1929. Loss: 0.23731279373168945\n",
      "Iteration: 1930. Loss: 0.14531618356704712\n",
      "Iteration: 1931. Loss: 0.10118106007575989\n",
      "Iteration: 1932. Loss: 0.15347491204738617\n",
      "Iteration: 1933. Loss: 0.12211013585329056\n",
      "Iteration: 1934. Loss: 0.08668529242277145\n",
      "Iteration: 1935. Loss: 0.14219319820404053\n",
      "Iteration: 1936. Loss: 0.14643076062202454\n",
      "Iteration: 1937. Loss: 0.20601636171340942\n",
      "Iteration: 1938. Loss: 0.09453513473272324\n",
      "Iteration: 1939. Loss: 0.06695788353681564\n",
      "Iteration: 1940. Loss: 0.07467400282621384\n",
      "Iteration: 1941. Loss: 0.1031537726521492\n",
      "Iteration: 1942. Loss: 0.15035635232925415\n",
      "Iteration: 1943. Loss: 0.22526222467422485\n",
      "Iteration: 1944. Loss: 0.1361011415719986\n",
      "Iteration: 1945. Loss: 0.06832750141620636\n",
      "Iteration: 1946. Loss: 0.1638539433479309\n",
      "Iteration: 1947. Loss: 0.1375383734703064\n",
      "Iteration: 1948. Loss: 0.12164749950170517\n",
      "Iteration: 1949. Loss: 0.1656971424818039\n",
      "Iteration: 1950. Loss: 0.15938962996006012\n",
      "Iteration: 1951. Loss: 0.0899977758526802\n",
      "Iteration: 1952. Loss: 0.08214299380779266\n",
      "Iteration: 1953. Loss: 0.0632893294095993\n",
      "Iteration: 1954. Loss: 0.1725701093673706\n",
      "Iteration: 1955. Loss: 0.19231626391410828\n",
      "Iteration: 1956. Loss: 0.1536387801170349\n",
      "Iteration: 1957. Loss: 0.15256378054618835\n",
      "Iteration: 1958. Loss: 0.34009894728660583\n",
      "Iteration: 1959. Loss: 0.1703122854232788\n",
      "Iteration: 1960. Loss: 0.0972953587770462\n",
      "Iteration: 1961. Loss: 0.30354657769203186\n",
      "Iteration: 1962. Loss: 0.09338238835334778\n",
      "Iteration: 1963. Loss: 0.11641313880681992\n",
      "Iteration: 1964. Loss: 0.1214958131313324\n",
      "Iteration: 1965. Loss: 0.179229736328125\n",
      "Iteration: 1966. Loss: 0.14893576502799988\n",
      "Iteration: 1967. Loss: 0.21058574318885803\n",
      "Iteration: 1968. Loss: 0.08771947771310806\n",
      "Iteration: 1969. Loss: 0.051709480583667755\n",
      "Iteration: 1970. Loss: 0.09545132517814636\n",
      "Iteration: 1971. Loss: 0.08243992924690247\n",
      "Iteration: 1972. Loss: 0.15885891020298004\n",
      "Iteration: 1973. Loss: 0.2706252336502075\n",
      "Iteration: 1974. Loss: 0.11991909146308899\n",
      "Iteration: 1975. Loss: 0.1989399790763855\n",
      "Iteration: 1976. Loss: 0.17713701725006104\n",
      "Iteration: 1977. Loss: 0.09282152354717255\n",
      "Iteration: 1978. Loss: 0.20653751492500305\n",
      "Iteration: 1979. Loss: 0.1700357049703598\n",
      "Iteration: 1980. Loss: 0.11635811626911163\n",
      "Iteration: 1981. Loss: 0.11012406647205353\n",
      "Iteration: 1982. Loss: 0.06537929177284241\n",
      "Iteration: 1983. Loss: 0.09599191695451736\n",
      "Iteration: 1984. Loss: 0.0703636035323143\n",
      "Iteration: 1985. Loss: 0.11367514729499817\n",
      "Iteration: 1986. Loss: 0.23182134330272675\n",
      "Iteration: 1987. Loss: 0.11524487286806107\n",
      "Iteration: 1988. Loss: 0.1569054275751114\n",
      "Iteration: 1989. Loss: 0.31771329045295715\n",
      "Iteration: 1990. Loss: 0.14328232407569885\n",
      "Iteration: 1991. Loss: 0.09456950426101685\n",
      "Iteration: 1992. Loss: 0.14523594081401825\n",
      "Iteration: 1993. Loss: 0.07627436518669128\n",
      "Iteration: 1994. Loss: 0.08146824687719345\n",
      "Iteration: 1995. Loss: 0.11524734646081924\n",
      "Iteration: 1996. Loss: 0.10320138931274414\n",
      "Iteration: 1997. Loss: 0.12721435725688934\n",
      "Iteration: 1998. Loss: 0.13983678817749023\n",
      "Iteration: 1999. Loss: 0.0925220474600792\n",
      "Iteration: 2000. Loss: 0.15019239485263824\n",
      "Iteration: 2001. Loss: 0.1462535560131073\n",
      "Iteration: 2002. Loss: 0.05947044491767883\n",
      "Iteration: 2003. Loss: 0.11203351616859436\n",
      "Iteration: 2004. Loss: 0.09854830056428909\n",
      "Iteration: 2005. Loss: 0.10959652066230774\n",
      "Iteration: 2006. Loss: 0.09805816411972046\n",
      "Iteration: 2007. Loss: 0.051954470574855804\n",
      "Iteration: 2008. Loss: 0.1719478815793991\n",
      "Iteration: 2009. Loss: 0.15258511900901794\n",
      "Iteration: 2010. Loss: 0.12158281356096268\n",
      "Iteration: 2011. Loss: 0.24008667469024658\n",
      "Iteration: 2012. Loss: 0.0839606374502182\n",
      "Iteration: 2013. Loss: 0.18996523320674896\n",
      "Iteration: 2014. Loss: 0.09488319605588913\n",
      "Iteration: 2015. Loss: 0.19677096605300903\n",
      "Iteration: 2016. Loss: 0.0991896316409111\n",
      "Iteration: 2017. Loss: 0.1547272652387619\n",
      "Iteration: 2018. Loss: 0.26121968030929565\n",
      "Iteration: 2019. Loss: 0.11962978541851044\n",
      "Iteration: 2020. Loss: 0.13191448152065277\n",
      "Iteration: 2021. Loss: 0.20403677225112915\n",
      "Iteration: 2022. Loss: 0.09017975628376007\n",
      "Iteration: 2023. Loss: 0.10762432217597961\n",
      "Iteration: 2024. Loss: 0.24191899597644806\n",
      "Iteration: 2025. Loss: 0.22670724987983704\n",
      "Iteration: 2026. Loss: 0.21712884306907654\n",
      "Iteration: 2027. Loss: 0.05490739271044731\n",
      "Iteration: 2028. Loss: 0.14230559766292572\n",
      "Iteration: 2029. Loss: 0.20735237002372742\n",
      "Iteration: 2030. Loss: 0.15827636420726776\n",
      "Iteration: 2031. Loss: 0.14985303580760956\n",
      "Iteration: 2032. Loss: 0.10786557197570801\n",
      "Iteration: 2033. Loss: 0.08901163190603256\n",
      "Iteration: 2034. Loss: 0.2038833051919937\n",
      "Iteration: 2035. Loss: 0.11969193816184998\n",
      "Iteration: 2036. Loss: 0.09370563179254532\n",
      "Iteration: 2037. Loss: 0.12403640896081924\n",
      "Iteration: 2038. Loss: 0.29724472761154175\n",
      "Iteration: 2039. Loss: 0.16522282361984253\n",
      "Iteration: 2040. Loss: 0.1708875447511673\n",
      "Iteration: 2041. Loss: 0.10828950256109238\n",
      "Iteration: 2042. Loss: 0.19873830676078796\n",
      "Iteration: 2043. Loss: 0.09284862875938416\n",
      "Iteration: 2044. Loss: 0.20986595749855042\n",
      "Iteration: 2045. Loss: 0.19345851242542267\n",
      "Iteration: 2046. Loss: 0.13171100616455078\n",
      "Iteration: 2047. Loss: 0.2418251931667328\n",
      "Iteration: 2048. Loss: 0.09727037698030472\n",
      "Iteration: 2049. Loss: 0.10897152870893478\n",
      "Iteration: 2050. Loss: 0.12281820923089981\n",
      "Iteration: 2051. Loss: 0.07570642232894897\n",
      "Iteration: 2052. Loss: 0.07252637296915054\n",
      "Iteration: 2053. Loss: 0.06540264189243317\n",
      "Iteration: 2054. Loss: 0.2000219225883484\n",
      "Iteration: 2055. Loss: 0.18885019421577454\n",
      "Iteration: 2056. Loss: 0.07949453592300415\n",
      "Iteration: 2057. Loss: 0.1426934450864792\n",
      "Iteration: 2058. Loss: 0.09467941522598267\n",
      "Iteration: 2059. Loss: 0.05937226861715317\n",
      "Iteration: 2060. Loss: 0.14491774141788483\n",
      "Iteration: 2061. Loss: 0.22705402970314026\n",
      "Iteration: 2062. Loss: 0.05246414989233017\n",
      "Iteration: 2063. Loss: 0.14175888895988464\n",
      "Iteration: 2064. Loss: 0.09804116934537888\n",
      "Iteration: 2065. Loss: 0.2186952829360962\n",
      "Iteration: 2066. Loss: 0.11137226223945618\n",
      "Iteration: 2067. Loss: 0.11800043284893036\n",
      "Iteration: 2068. Loss: 0.18201684951782227\n",
      "Iteration: 2069. Loss: 0.20418065786361694\n",
      "Iteration: 2070. Loss: 0.12143338471651077\n",
      "Iteration: 2071. Loss: 0.08506863564252853\n",
      "Iteration: 2072. Loss: 0.15424159169197083\n",
      "Iteration: 2073. Loss: 0.1824205070734024\n",
      "Iteration: 2074. Loss: 0.1723235547542572\n",
      "Iteration: 2075. Loss: 0.09030130505561829\n",
      "Iteration: 2076. Loss: 0.0944298803806305\n",
      "Iteration: 2077. Loss: 0.22705814242362976\n",
      "Iteration: 2078. Loss: 0.19666336476802826\n",
      "Iteration: 2079. Loss: 0.06189429387450218\n",
      "Iteration: 2080. Loss: 0.10187914222478867\n",
      "Iteration: 2081. Loss: 0.15588253736495972\n",
      "Iteration: 2082. Loss: 0.10661700367927551\n",
      "Iteration: 2083. Loss: 0.1268450915813446\n",
      "Iteration: 2084. Loss: 0.10692624747753143\n",
      "Iteration: 2085. Loss: 0.21181640028953552\n",
      "Iteration: 2086. Loss: 0.07325097173452377\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2087. Loss: 0.1403389722108841\n",
      "Iteration: 2088. Loss: 0.13842716813087463\n",
      "Iteration: 2089. Loss: 0.3718460500240326\n",
      "Iteration: 2090. Loss: 0.12884239852428436\n",
      "Iteration: 2091. Loss: 0.3041526973247528\n",
      "Iteration: 2092. Loss: 0.18910104036331177\n",
      "Iteration: 2093. Loss: 0.12602640688419342\n",
      "Iteration: 2094. Loss: 0.08410481363534927\n",
      "Iteration: 2095. Loss: 0.1924135833978653\n",
      "Iteration: 2096. Loss: 0.1448967009782791\n",
      "Iteration: 2097. Loss: 0.2656461298465729\n",
      "Iteration: 2098. Loss: 0.09655580669641495\n",
      "Iteration: 2099. Loss: 0.12586528062820435\n",
      "Iteration: 2100. Loss: 0.051081761717796326\n",
      "Iteration: 2101. Loss: 0.10117034614086151\n",
      "Iteration: 2102. Loss: 0.07454347610473633\n",
      "Iteration: 2103. Loss: 0.20581063628196716\n",
      "Iteration: 2104. Loss: 0.1624574512243271\n",
      "Iteration: 2105. Loss: 0.17946098744869232\n",
      "Iteration: 2106. Loss: 0.20175248384475708\n",
      "Iteration: 2107. Loss: 0.14977926015853882\n",
      "Iteration: 2108. Loss: 0.09532246738672256\n",
      "Iteration: 2109. Loss: 0.2872868776321411\n",
      "Iteration: 2110. Loss: 0.12441728264093399\n",
      "Iteration: 2111. Loss: 0.11711034923791885\n",
      "Iteration: 2112. Loss: 0.22194692492485046\n",
      "Iteration: 2113. Loss: 0.042891088873147964\n",
      "Iteration: 2114. Loss: 0.12708519399166107\n",
      "Iteration: 2115. Loss: 0.15389278531074524\n",
      "Iteration: 2116. Loss: 0.20584537088871002\n",
      "Iteration: 2117. Loss: 0.1673530638217926\n",
      "Iteration: 2118. Loss: 0.17309604585170746\n",
      "Iteration: 2119. Loss: 0.10728500038385391\n",
      "Iteration: 2120. Loss: 0.09111227095127106\n",
      "Iteration: 2121. Loss: 0.0832124799489975\n",
      "Iteration: 2122. Loss: 0.17070239782333374\n",
      "Iteration: 2123. Loss: 0.1751435101032257\n",
      "Iteration: 2124. Loss: 0.09352245926856995\n",
      "Iteration: 2125. Loss: 0.1485586315393448\n",
      "Iteration: 2126. Loss: 0.07740979641675949\n",
      "Iteration: 2127. Loss: 0.128749281167984\n",
      "Iteration: 2128. Loss: 0.31447237730026245\n",
      "Iteration: 2129. Loss: 0.10415659099817276\n",
      "Iteration: 2130. Loss: 0.13585394620895386\n",
      "Iteration: 2131. Loss: 0.10830923169851303\n",
      "Iteration: 2132. Loss: 0.20937904715538025\n",
      "Iteration: 2133. Loss: 0.27536073327064514\n",
      "Iteration: 2134. Loss: 0.062159933149814606\n",
      "Iteration: 2135. Loss: 0.034652646631002426\n",
      "Iteration: 2136. Loss: 0.13681012392044067\n",
      "Iteration: 2137. Loss: 0.06850812584161758\n",
      "Iteration: 2138. Loss: 0.12487098574638367\n",
      "Iteration: 2139. Loss: 0.20218725502490997\n",
      "Iteration: 2140. Loss: 0.14653074741363525\n",
      "Iteration: 2141. Loss: 0.14698787033557892\n",
      "Iteration: 2142. Loss: 0.11794226616621017\n",
      "Iteration: 2143. Loss: 0.15957820415496826\n",
      "Iteration: 2144. Loss: 0.2271658331155777\n",
      "Iteration: 2145. Loss: 0.11651293933391571\n",
      "Iteration: 2146. Loss: 0.11173782497644424\n",
      "Iteration: 2147. Loss: 0.09567900747060776\n",
      "Iteration: 2148. Loss: 0.1277637779712677\n",
      "Iteration: 2149. Loss: 0.18235397338867188\n",
      "Iteration: 2150. Loss: 0.06408076733350754\n",
      "Iteration: 2151. Loss: 0.18810907006263733\n",
      "Iteration: 2152. Loss: 0.08575829863548279\n",
      "Iteration: 2153. Loss: 0.03943885862827301\n",
      "Iteration: 2154. Loss: 0.15161392092704773\n",
      "Iteration: 2155. Loss: 0.13166478276252747\n",
      "Iteration: 2156. Loss: 0.09627155214548111\n",
      "Iteration: 2157. Loss: 0.10723669826984406\n",
      "Iteration: 2158. Loss: 0.09869768470525742\n",
      "Iteration: 2159. Loss: 0.14081935584545135\n",
      "Iteration: 2160. Loss: 0.11388220638036728\n",
      "Iteration: 2161. Loss: 0.04773412272334099\n",
      "Iteration: 2162. Loss: 0.03665466606616974\n",
      "Iteration: 2163. Loss: 0.038188476115465164\n",
      "Iteration: 2164. Loss: 0.0703614205121994\n",
      "Iteration: 2165. Loss: 0.07102206349372864\n",
      "Iteration: 2166. Loss: 0.10963562875986099\n",
      "Iteration: 2167. Loss: 0.05537739768624306\n",
      "Iteration: 2168. Loss: 0.1006409227848053\n",
      "Iteration: 2169. Loss: 0.09140890836715698\n",
      "Iteration: 2170. Loss: 0.06646492332220078\n",
      "Iteration: 2171. Loss: 0.17418323457241058\n",
      "Iteration: 2172. Loss: 0.24072560667991638\n",
      "Iteration: 2173. Loss: 0.03880021721124649\n",
      "Iteration: 2174. Loss: 0.14132638275623322\n",
      "Iteration: 2175. Loss: 0.1326858550310135\n",
      "Iteration: 2176. Loss: 0.1360589861869812\n",
      "Iteration: 2177. Loss: 0.12523336708545685\n",
      "Iteration: 2178. Loss: 0.15004797279834747\n",
      "Iteration: 2179. Loss: 0.07514114677906036\n",
      "Iteration: 2180. Loss: 0.19957515597343445\n",
      "Iteration: 2181. Loss: 0.034368228167295456\n",
      "Iteration: 2182. Loss: 0.11720774322748184\n",
      "Iteration: 2183. Loss: 0.20870618522167206\n",
      "Iteration: 2184. Loss: 0.13028140366077423\n",
      "Iteration: 2185. Loss: 0.08675643056631088\n",
      "Iteration: 2186. Loss: 0.17088302969932556\n",
      "Iteration: 2187. Loss: 0.17454305291175842\n",
      "Iteration: 2188. Loss: 0.19811084866523743\n",
      "Iteration: 2189. Loss: 0.12395423650741577\n",
      "Iteration: 2190. Loss: 0.05021042004227638\n",
      "Iteration: 2191. Loss: 0.20829570293426514\n",
      "Iteration: 2192. Loss: 0.19015462696552277\n",
      "Iteration: 2193. Loss: 0.06928234547376633\n",
      "Iteration: 2194. Loss: 0.11343977600336075\n",
      "Iteration: 2195. Loss: 0.1239645779132843\n",
      "Iteration: 2196. Loss: 0.04680957645177841\n",
      "Iteration: 2197. Loss: 0.17154698073863983\n",
      "Iteration: 2198. Loss: 0.16453920304775238\n",
      "Iteration: 2199. Loss: 0.11571390181779861\n",
      "Iteration: 2200. Loss: 0.10782121866941452\n",
      "Iteration: 2201. Loss: 0.18653802573680878\n",
      "Iteration: 2202. Loss: 0.07453912496566772\n",
      "Iteration: 2203. Loss: 0.1654987782239914\n",
      "Iteration: 2204. Loss: 0.06455458700656891\n",
      "Iteration: 2205. Loss: 0.11081771552562714\n",
      "Iteration: 2206. Loss: 0.10841452330350876\n",
      "Iteration: 2207. Loss: 0.15774251520633698\n",
      "Iteration: 2208. Loss: 0.12163086980581284\n",
      "Iteration: 2209. Loss: 0.19368793070316315\n",
      "Iteration: 2210. Loss: 0.12144545465707779\n",
      "Iteration: 2211. Loss: 0.08874823898077011\n",
      "Iteration: 2212. Loss: 0.11298587918281555\n",
      "Iteration: 2213. Loss: 0.09734483063220978\n",
      "Iteration: 2214. Loss: 0.14489886164665222\n",
      "Iteration: 2215. Loss: 0.21078138053417206\n",
      "Iteration: 2216. Loss: 0.2022201120853424\n",
      "Iteration: 2217. Loss: 0.20479220151901245\n",
      "Iteration: 2218. Loss: 0.136651873588562\n",
      "Iteration: 2219. Loss: 0.08120916038751602\n",
      "Iteration: 2220. Loss: 0.06324242800474167\n",
      "Iteration: 2221. Loss: 0.0859156996011734\n",
      "Iteration: 2222. Loss: 0.159972682595253\n",
      "Iteration: 2223. Loss: 0.04292544350028038\n",
      "Iteration: 2224. Loss: 0.15768571197986603\n",
      "Iteration: 2225. Loss: 0.13039055466651917\n",
      "Iteration: 2226. Loss: 0.1833837628364563\n",
      "Iteration: 2227. Loss: 0.06469950824975967\n",
      "Iteration: 2228. Loss: 0.09846574813127518\n",
      "Iteration: 2229. Loss: 0.25927576422691345\n",
      "Iteration: 2230. Loss: 0.1389041393995285\n",
      "Iteration: 2231. Loss: 0.1309218406677246\n",
      "Iteration: 2232. Loss: 0.18823972344398499\n",
      "Iteration: 2233. Loss: 0.15682396292686462\n",
      "Iteration: 2234. Loss: 0.15006768703460693\n",
      "Iteration: 2235. Loss: 0.05890829861164093\n",
      "Iteration: 2236. Loss: 0.2647057771682739\n",
      "Iteration: 2237. Loss: 0.17663729190826416\n",
      "Iteration: 2238. Loss: 0.08042901009321213\n",
      "Iteration: 2239. Loss: 0.14544342458248138\n",
      "Iteration: 2240. Loss: 0.2136697769165039\n",
      "Iteration: 2241. Loss: 0.060645151883363724\n",
      "Iteration: 2242. Loss: 0.17232409119606018\n",
      "Iteration: 2243. Loss: 0.0750473141670227\n",
      "Iteration: 2244. Loss: 0.15752406418323517\n",
      "Iteration: 2245. Loss: 0.0638224333524704\n",
      "Iteration: 2246. Loss: 0.11292848736047745\n",
      "Iteration: 2247. Loss: 0.20282794535160065\n",
      "Iteration: 2248. Loss: 0.14669403433799744\n",
      "Iteration: 2249. Loss: 0.061382222920656204\n",
      "Iteration: 2250. Loss: 0.07422051578760147\n",
      "Iteration: 2251. Loss: 0.17115932703018188\n",
      "Iteration: 2252. Loss: 0.06026630476117134\n",
      "Iteration: 2253. Loss: 0.134398952126503\n",
      "Iteration: 2254. Loss: 0.08191867172718048\n",
      "Iteration: 2255. Loss: 0.14597854018211365\n",
      "Iteration: 2256. Loss: 0.18103626370429993\n",
      "Iteration: 2257. Loss: 0.1868738979101181\n",
      "Iteration: 2258. Loss: 0.07987969368696213\n",
      "Iteration: 2259. Loss: 0.1086779311299324\n",
      "Iteration: 2260. Loss: 0.19754891097545624\n",
      "Iteration: 2261. Loss: 0.2559502422809601\n",
      "Iteration: 2262. Loss: 0.26460325717926025\n",
      "Iteration: 2263. Loss: 0.07891682535409927\n",
      "Iteration: 2264. Loss: 0.19361582398414612\n",
      "Iteration: 2265. Loss: 0.1549081802368164\n",
      "Iteration: 2266. Loss: 0.19871965050697327\n",
      "Iteration: 2267. Loss: 0.10295748710632324\n",
      "Iteration: 2268. Loss: 0.09602387249469757\n",
      "Iteration: 2269. Loss: 0.1424294412136078\n",
      "Iteration: 2270. Loss: 0.14027374982833862\n",
      "Iteration: 2271. Loss: 0.08212488889694214\n",
      "Iteration: 2272. Loss: 0.07165417820215225\n",
      "Iteration: 2273. Loss: 0.1777157336473465\n",
      "Iteration: 2274. Loss: 0.07287102937698364\n",
      "Iteration: 2275. Loss: 0.07786735892295837\n",
      "Iteration: 2276. Loss: 0.0568506196141243\n",
      "Iteration: 2277. Loss: 0.132224440574646\n",
      "Iteration: 2278. Loss: 0.10677991062402725\n",
      "Iteration: 2279. Loss: 0.18992790579795837\n",
      "Iteration: 2280. Loss: 0.17809104919433594\n",
      "Iteration: 2281. Loss: 0.11326280236244202\n",
      "Iteration: 2282. Loss: 0.1419060379266739\n",
      "Iteration: 2283. Loss: 0.0969925969839096\n",
      "Iteration: 2284. Loss: 0.17089010775089264\n",
      "Iteration: 2285. Loss: 0.19529323279857635\n",
      "Iteration: 2286. Loss: 0.158578023314476\n",
      "Iteration: 2287. Loss: 0.2410242110490799\n",
      "Iteration: 2288. Loss: 0.15503965318202972\n",
      "Iteration: 2289. Loss: 0.32612136006355286\n",
      "Iteration: 2290. Loss: 0.21291212737560272\n",
      "Iteration: 2291. Loss: 0.1725664883852005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2292. Loss: 0.06648117303848267\n",
      "Iteration: 2293. Loss: 0.2550830543041229\n",
      "Iteration: 2294. Loss: 0.1166798323392868\n",
      "Iteration: 2295. Loss: 0.08113935589790344\n",
      "Iteration: 2296. Loss: 0.1477367877960205\n",
      "Iteration: 2297. Loss: 0.1811293661594391\n",
      "Iteration: 2298. Loss: 0.09806524217128754\n",
      "Iteration: 2299. Loss: 0.09435933828353882\n",
      "Iteration: 2300. Loss: 0.10829871147871017\n",
      "Iteration: 2301. Loss: 0.07827803492546082\n",
      "Iteration: 2302. Loss: 0.11387621611356735\n",
      "Iteration: 2303. Loss: 0.06160937622189522\n",
      "Iteration: 2304. Loss: 0.11121319979429245\n",
      "Iteration: 2305. Loss: 0.2334643304347992\n",
      "Iteration: 2306. Loss: 0.14312410354614258\n",
      "Iteration: 2307. Loss: 0.24361161887645721\n",
      "Iteration: 2308. Loss: 0.12084422260522842\n",
      "Iteration: 2309. Loss: 0.07328357547521591\n",
      "Iteration: 2310. Loss: 0.17038924992084503\n",
      "Iteration: 2311. Loss: 0.0622536763548851\n",
      "Iteration: 2312. Loss: 0.19865824282169342\n",
      "Iteration: 2313. Loss: 0.12760259211063385\n",
      "Iteration: 2314. Loss: 0.06896121799945831\n",
      "Iteration: 2315. Loss: 0.16271600127220154\n",
      "Iteration: 2316. Loss: 0.15586774051189423\n",
      "Iteration: 2317. Loss: 0.06886766105890274\n",
      "Iteration: 2318. Loss: 0.1263217031955719\n",
      "Iteration: 2319. Loss: 0.14857779443264008\n",
      "Iteration: 2320. Loss: 0.09359633177518845\n",
      "Iteration: 2321. Loss: 0.07989947497844696\n",
      "Iteration: 2322. Loss: 0.30727967619895935\n",
      "Iteration: 2323. Loss: 0.07069450616836548\n",
      "Iteration: 2324. Loss: 0.11375205963850021\n",
      "Iteration: 2325. Loss: 0.05429420620203018\n",
      "Iteration: 2326. Loss: 0.168527752161026\n",
      "Iteration: 2327. Loss: 0.07092396169900894\n",
      "Iteration: 2328. Loss: 0.11282148212194443\n",
      "Iteration: 2329. Loss: 0.10498813539743423\n",
      "Iteration: 2330. Loss: 0.06873147189617157\n",
      "Iteration: 2331. Loss: 0.19831283390522003\n",
      "Iteration: 2332. Loss: 0.05602339282631874\n",
      "Iteration: 2333. Loss: 0.12636679410934448\n",
      "Iteration: 2334. Loss: 0.1685367226600647\n",
      "Iteration: 2335. Loss: 0.17469753324985504\n",
      "Iteration: 2336. Loss: 0.1660710573196411\n",
      "Iteration: 2337. Loss: 0.10679751634597778\n",
      "Iteration: 2338. Loss: 0.1233484074473381\n",
      "Iteration: 2339. Loss: 0.20913366973400116\n",
      "Iteration: 2340. Loss: 0.19949808716773987\n",
      "Iteration: 2341. Loss: 0.14391480386257172\n",
      "Iteration: 2342. Loss: 0.07013203203678131\n",
      "Iteration: 2343. Loss: 0.1618627905845642\n",
      "Iteration: 2344. Loss: 0.0776892602443695\n",
      "Iteration: 2345. Loss: 0.13472910225391388\n",
      "Iteration: 2346. Loss: 0.10762610286474228\n",
      "Iteration: 2347. Loss: 0.23008403182029724\n",
      "Iteration: 2348. Loss: 0.135939821600914\n",
      "Iteration: 2349. Loss: 0.1311446726322174\n",
      "Iteration: 2350. Loss: 0.09362486004829407\n",
      "Iteration: 2351. Loss: 0.10208377242088318\n",
      "Iteration: 2352. Loss: 0.09050269424915314\n",
      "Iteration: 2353. Loss: 0.13897857069969177\n",
      "Iteration: 2354. Loss: 0.1298885941505432\n",
      "Iteration: 2355. Loss: 0.1756136566400528\n",
      "Iteration: 2356. Loss: 0.12243359535932541\n",
      "Iteration: 2357. Loss: 0.06860113888978958\n",
      "Iteration: 2358. Loss: 0.09364569932222366\n",
      "Iteration: 2359. Loss: 0.06779690086841583\n",
      "Iteration: 2360. Loss: 0.10181376338005066\n",
      "Iteration: 2361. Loss: 0.033934105187654495\n",
      "Iteration: 2362. Loss: 0.24980536103248596\n",
      "Iteration: 2363. Loss: 0.1302908957004547\n",
      "Iteration: 2364. Loss: 0.14790941774845123\n",
      "Iteration: 2365. Loss: 0.09880559891462326\n",
      "Iteration: 2366. Loss: 0.09231005609035492\n",
      "Iteration: 2367. Loss: 0.1070689857006073\n",
      "Iteration: 2368. Loss: 0.1635051965713501\n",
      "Iteration: 2369. Loss: 0.19169120490550995\n",
      "Iteration: 2370. Loss: 0.12273385375738144\n",
      "Iteration: 2371. Loss: 0.05728217586874962\n",
      "Iteration: 2372. Loss: 0.1019103080034256\n",
      "Iteration: 2373. Loss: 0.11838515102863312\n",
      "Iteration: 2374. Loss: 0.10780369490385056\n",
      "Iteration: 2375. Loss: 0.23139920830726624\n",
      "Iteration: 2376. Loss: 0.2809566557407379\n",
      "Iteration: 2377. Loss: 0.17721402645111084\n",
      "Iteration: 2378. Loss: 0.14003318548202515\n",
      "Iteration: 2379. Loss: 0.0806136205792427\n",
      "Iteration: 2380. Loss: 0.0830579400062561\n",
      "Iteration: 2381. Loss: 0.15040640532970428\n",
      "Iteration: 2382. Loss: 0.1363801658153534\n",
      "Iteration: 2383. Loss: 0.07180739939212799\n",
      "Iteration: 2384. Loss: 0.14088819921016693\n",
      "Iteration: 2385. Loss: 0.06094400957226753\n",
      "Iteration: 2386. Loss: 0.1444137543439865\n",
      "Iteration: 2387. Loss: 0.1289004236459732\n",
      "Iteration: 2388. Loss: 0.11552517116069794\n",
      "Iteration: 2389. Loss: 0.06019370257854462\n",
      "Iteration: 2390. Loss: 0.10392188280820847\n",
      "Iteration: 2391. Loss: 0.05553875118494034\n",
      "Iteration: 2392. Loss: 0.14939367771148682\n",
      "Iteration: 2393. Loss: 0.22366300225257874\n",
      "Iteration: 2394. Loss: 0.10590201616287231\n",
      "Iteration: 2395. Loss: 0.18555286526679993\n",
      "Iteration: 2396. Loss: 0.10306801646947861\n",
      "Iteration: 2397. Loss: 0.2871801555156708\n",
      "Iteration: 2398. Loss: 0.10522547364234924\n",
      "Iteration: 2399. Loss: 0.07959125936031342\n",
      "Iteration: 2400. Loss: 0.09343864768743515\n",
      "Iteration: 2401. Loss: 0.1633170247077942\n",
      "Iteration: 2402. Loss: 0.13541124761104584\n",
      "Iteration: 2403. Loss: 0.06565281748771667\n",
      "Iteration: 2404. Loss: 0.21166618168354034\n",
      "Iteration: 2405. Loss: 0.12273251265287399\n",
      "Iteration: 2406. Loss: 0.21313558518886566\n",
      "Iteration: 2407. Loss: 0.04379960149526596\n",
      "Iteration: 2408. Loss: 0.08989980071783066\n",
      "Iteration: 2409. Loss: 0.07216518372297287\n",
      "Iteration: 2410. Loss: 0.13883863389492035\n",
      "Iteration: 2411. Loss: 0.048282332718372345\n",
      "Iteration: 2412. Loss: 0.19267353415489197\n",
      "Iteration: 2413. Loss: 0.13927194476127625\n",
      "Iteration: 2414. Loss: 0.15454144775867462\n",
      "Iteration: 2415. Loss: 0.1586000770330429\n",
      "Iteration: 2416. Loss: 0.10482257604598999\n",
      "Iteration: 2417. Loss: 0.1130303367972374\n",
      "Iteration: 2418. Loss: 0.10978159308433533\n",
      "Iteration: 2419. Loss: 0.1676786094903946\n",
      "Iteration: 2420. Loss: 0.16740664839744568\n",
      "Iteration: 2421. Loss: 0.14568613469600677\n",
      "Iteration: 2422. Loss: 0.06660603731870651\n",
      "Iteration: 2423. Loss: 0.12428851425647736\n",
      "Iteration: 2424. Loss: 0.1775328516960144\n",
      "Iteration: 2425. Loss: 0.1216270923614502\n",
      "Iteration: 2426. Loss: 0.06868668645620346\n",
      "Iteration: 2427. Loss: 0.08293973654508591\n",
      "Iteration: 2428. Loss: 0.08303969353437424\n",
      "Iteration: 2429. Loss: 0.13492822647094727\n",
      "Iteration: 2430. Loss: 0.14272905886173248\n",
      "Iteration: 2431. Loss: 0.07037290185689926\n",
      "Iteration: 2432. Loss: 0.13664013147354126\n",
      "Iteration: 2433. Loss: 0.07428722828626633\n",
      "Iteration: 2434. Loss: 0.10558643937110901\n",
      "Iteration: 2435. Loss: 0.20470845699310303\n",
      "Iteration: 2436. Loss: 0.10991419851779938\n",
      "Iteration: 2437. Loss: 0.12335049360990524\n",
      "Iteration: 2438. Loss: 0.08200328797101974\n",
      "Iteration: 2439. Loss: 0.1255464106798172\n",
      "Iteration: 2440. Loss: 0.05372186005115509\n",
      "Iteration: 2441. Loss: 0.191761776804924\n",
      "Iteration: 2442. Loss: 0.11398298293352127\n",
      "Iteration: 2443. Loss: 0.07666046917438507\n",
      "Iteration: 2444. Loss: 0.07303059101104736\n",
      "Iteration: 2445. Loss: 0.07466130703687668\n",
      "Iteration: 2446. Loss: 0.1473442167043686\n",
      "Iteration: 2447. Loss: 0.06806979328393936\n",
      "Iteration: 2448. Loss: 0.12094568461179733\n",
      "Iteration: 2449. Loss: 0.20487812161445618\n",
      "Iteration: 2450. Loss: 0.09728264063596725\n",
      "Iteration: 2451. Loss: 0.20696912705898285\n",
      "Iteration: 2452. Loss: 0.0945063903927803\n",
      "Iteration: 2453. Loss: 0.08725858479738235\n",
      "Iteration: 2454. Loss: 0.10062366724014282\n",
      "Iteration: 2455. Loss: 0.09635398536920547\n",
      "Iteration: 2456. Loss: 0.10526338219642639\n",
      "Iteration: 2457. Loss: 0.08320960402488708\n",
      "Iteration: 2458. Loss: 0.11767104268074036\n",
      "Iteration: 2459. Loss: 0.08774451166391373\n",
      "Iteration: 2460. Loss: 0.04998435080051422\n",
      "Iteration: 2461. Loss: 0.12393821775913239\n",
      "Iteration: 2462. Loss: 0.07355804741382599\n",
      "Iteration: 2463. Loss: 0.18486633896827698\n",
      "Iteration: 2464. Loss: 0.04773150384426117\n",
      "Iteration: 2465. Loss: 0.14416572451591492\n",
      "Iteration: 2466. Loss: 0.22029806673526764\n",
      "Iteration: 2467. Loss: 0.08938572555780411\n",
      "Iteration: 2468. Loss: 0.15043944120407104\n",
      "Iteration: 2469. Loss: 0.2368146926164627\n",
      "Iteration: 2470. Loss: 0.07306002825498581\n",
      "Iteration: 2471. Loss: 0.06544679403305054\n",
      "Iteration: 2472. Loss: 0.11412648111581802\n",
      "Iteration: 2473. Loss: 0.0871296226978302\n",
      "Iteration: 2474. Loss: 0.12464700639247894\n",
      "Iteration: 2475. Loss: 0.10816260427236557\n",
      "Iteration: 2476. Loss: 0.068961001932621\n",
      "Iteration: 2477. Loss: 0.19141267240047455\n",
      "Iteration: 2478. Loss: 0.0854666605591774\n",
      "Iteration: 2479. Loss: 0.09192529320716858\n",
      "Iteration: 2480. Loss: 0.1576526015996933\n",
      "Iteration: 2481. Loss: 0.11680174618959427\n",
      "Iteration: 2482. Loss: 0.16581745445728302\n",
      "Iteration: 2483. Loss: 0.08626057952642441\n",
      "Iteration: 2484. Loss: 0.09311920404434204\n",
      "Iteration: 2485. Loss: 0.3223972022533417\n",
      "Iteration: 2486. Loss: 0.16281045973300934\n",
      "Iteration: 2487. Loss: 0.07386825978755951\n",
      "Iteration: 2488. Loss: 0.12105274945497513\n",
      "Iteration: 2489. Loss: 0.0941615104675293\n",
      "Iteration: 2490. Loss: 0.07582957297563553\n",
      "Iteration: 2491. Loss: 0.09634049236774445\n",
      "Iteration: 2492. Loss: 0.07761089503765106\n",
      "Iteration: 2493. Loss: 0.10539619624614716\n",
      "Iteration: 2494. Loss: 0.14091196656227112\n",
      "Iteration: 2495. Loss: 0.13641440868377686\n",
      "Iteration: 2496. Loss: 0.10204438120126724\n",
      "Iteration: 2497. Loss: 0.13706234097480774\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2498. Loss: 0.17799502611160278\n",
      "Iteration: 2499. Loss: 0.07634835690259933\n",
      "Iteration: 2500. Loss: 0.06915847957134247\n",
      "Iteration: 2501. Loss: 0.15350954234600067\n",
      "Iteration: 2502. Loss: 0.04725457355380058\n",
      "Iteration: 2503. Loss: 0.09147051721811295\n",
      "Iteration: 2504. Loss: 0.12137772887945175\n",
      "Iteration: 2505. Loss: 0.10118475556373596\n",
      "Iteration: 2506. Loss: 0.13528773188591003\n",
      "Iteration: 2507. Loss: 0.16166667640209198\n",
      "Iteration: 2508. Loss: 0.06342534720897675\n",
      "Iteration: 2509. Loss: 0.09067617356777191\n",
      "Iteration: 2510. Loss: 0.13762634992599487\n",
      "Iteration: 2511. Loss: 0.1289798617362976\n",
      "Iteration: 2512. Loss: 0.13840621709823608\n",
      "Iteration: 2513. Loss: 0.17919813096523285\n",
      "Iteration: 2514. Loss: 0.14740075170993805\n",
      "Iteration: 2515. Loss: 0.05432223156094551\n",
      "Iteration: 2516. Loss: 0.13398195803165436\n",
      "Iteration: 2517. Loss: 0.17174652218818665\n",
      "Iteration: 2518. Loss: 0.13377691805362701\n",
      "Iteration: 2519. Loss: 0.07290752977132797\n",
      "Iteration: 2520. Loss: 0.12873274087905884\n",
      "Iteration: 2521. Loss: 0.11555755883455276\n",
      "Iteration: 2522. Loss: 0.2549128234386444\n",
      "Iteration: 2523. Loss: 0.17175567150115967\n",
      "Iteration: 2524. Loss: 0.04433846101164818\n",
      "Iteration: 2525. Loss: 0.06612836569547653\n",
      "Iteration: 2526. Loss: 0.15065321326255798\n",
      "Iteration: 2527. Loss: 0.20164085924625397\n",
      "Iteration: 2528. Loss: 0.10689479112625122\n",
      "Iteration: 2529. Loss: 0.19165073335170746\n",
      "Iteration: 2530. Loss: 0.15248903632164001\n",
      "Iteration: 2531. Loss: 0.09812283515930176\n",
      "Iteration: 2532. Loss: 0.13336071372032166\n",
      "Iteration: 2533. Loss: 0.061246588826179504\n",
      "Iteration: 2534. Loss: 0.2570711672306061\n",
      "Iteration: 2535. Loss: 0.08463770896196365\n",
      "Iteration: 2536. Loss: 0.117669016122818\n",
      "Iteration: 2537. Loss: 0.11881110072135925\n",
      "Iteration: 2538. Loss: 0.10391456633806229\n",
      "Iteration: 2539. Loss: 0.2574664056301117\n",
      "Iteration: 2540. Loss: 0.09127113223075867\n",
      "Iteration: 2541. Loss: 0.14407864212989807\n",
      "Iteration: 2542. Loss: 0.18405033648014069\n",
      "Iteration: 2543. Loss: 0.10636728256940842\n",
      "Iteration: 2544. Loss: 0.14764919877052307\n",
      "Iteration: 2545. Loss: 0.04859788343310356\n",
      "Iteration: 2546. Loss: 0.1823119968175888\n",
      "Iteration: 2547. Loss: 0.10531497746706009\n",
      "Iteration: 2548. Loss: 0.08903519809246063\n",
      "Iteration: 2549. Loss: 0.058207444846630096\n",
      "Iteration: 2550. Loss: 0.06977438926696777\n",
      "Iteration: 2551. Loss: 0.05357511341571808\n",
      "Iteration: 2552. Loss: 0.13160039484500885\n",
      "Iteration: 2553. Loss: 0.16626952588558197\n",
      "Iteration: 2554. Loss: 0.14906352758407593\n",
      "Iteration: 2555. Loss: 0.16972368955612183\n",
      "Iteration: 2556. Loss: 0.09874973446130753\n",
      "Iteration: 2557. Loss: 0.11122413724660873\n",
      "Iteration: 2558. Loss: 0.0701378732919693\n",
      "Iteration: 2559. Loss: 0.05388280004262924\n",
      "Iteration: 2560. Loss: 0.06115172430872917\n",
      "Iteration: 2561. Loss: 0.13892388343811035\n",
      "Iteration: 2562. Loss: 0.06778555363416672\n",
      "Iteration: 2563. Loss: 0.15862490236759186\n",
      "Iteration: 2564. Loss: 0.17700186371803284\n",
      "Iteration: 2565. Loss: 0.13432477414608002\n",
      "Iteration: 2566. Loss: 0.11231600493192673\n",
      "Iteration: 2567. Loss: 0.12709708511829376\n",
      "Iteration: 2568. Loss: 0.09678231924772263\n",
      "Iteration: 2569. Loss: 0.044435806572437286\n",
      "Iteration: 2570. Loss: 0.06362201273441315\n",
      "Iteration: 2571. Loss: 0.13810713589191437\n",
      "Iteration: 2572. Loss: 0.11688198149204254\n",
      "Iteration: 2573. Loss: 0.09872780740261078\n",
      "Iteration: 2574. Loss: 0.18046912550926208\n",
      "Iteration: 2575. Loss: 0.08885464072227478\n",
      "Iteration: 2576. Loss: 0.17889513075351715\n",
      "Iteration: 2577. Loss: 0.09042671322822571\n",
      "Iteration: 2578. Loss: 0.20120693743228912\n",
      "Iteration: 2579. Loss: 0.08759652823209763\n",
      "Iteration: 2580. Loss: 0.05956074222922325\n",
      "Iteration: 2581. Loss: 0.1538093388080597\n",
      "Iteration: 2582. Loss: 0.09143354743719101\n",
      "Iteration: 2583. Loss: 0.04580814763903618\n",
      "Iteration: 2584. Loss: 0.06992330402135849\n",
      "Iteration: 2585. Loss: 0.13636979460716248\n",
      "Iteration: 2586. Loss: 0.0588536374270916\n",
      "Iteration: 2587. Loss: 0.07195696979761124\n",
      "Iteration: 2588. Loss: 0.03543933480978012\n",
      "Iteration: 2589. Loss: 0.06751808524131775\n",
      "Iteration: 2590. Loss: 0.06503627449274063\n",
      "Iteration: 2591. Loss: 0.11417800933122635\n",
      "Iteration: 2592. Loss: 0.13139168918132782\n",
      "Iteration: 2593. Loss: 0.1677190661430359\n",
      "Iteration: 2594. Loss: 0.05340385437011719\n",
      "Iteration: 2595. Loss: 0.14060096442699432\n",
      "Iteration: 2596. Loss: 0.097078837454319\n",
      "Iteration: 2597. Loss: 0.13302764296531677\n",
      "Iteration: 2598. Loss: 0.07742484658956528\n",
      "Iteration: 2599. Loss: 0.19641315937042236\n",
      "Iteration: 2600. Loss: 0.1179734393954277\n",
      "Iteration: 2601. Loss: 0.06265383213758469\n",
      "Iteration: 2602. Loss: 0.12088198959827423\n",
      "Iteration: 2603. Loss: 0.1294843703508377\n",
      "Iteration: 2604. Loss: 0.1977245956659317\n",
      "Iteration: 2605. Loss: 0.1856735348701477\n",
      "Iteration: 2606. Loss: 0.11715572327375412\n",
      "Iteration: 2607. Loss: 0.09073682874441147\n",
      "Iteration: 2608. Loss: 0.15651299059391022\n",
      "Iteration: 2609. Loss: 0.09282413125038147\n",
      "Iteration: 2610. Loss: 0.09482541680335999\n",
      "Iteration: 2611. Loss: 0.09616797417402267\n",
      "Iteration: 2612. Loss: 0.12108146399259567\n",
      "Iteration: 2613. Loss: 0.11625213921070099\n",
      "Iteration: 2614. Loss: 0.08356161415576935\n",
      "Iteration: 2615. Loss: 0.11204534769058228\n",
      "Iteration: 2616. Loss: 0.09070594608783722\n",
      "Iteration: 2617. Loss: 0.11776182055473328\n",
      "Iteration: 2618. Loss: 0.09222108870744705\n",
      "Iteration: 2619. Loss: 0.106277234852314\n",
      "Iteration: 2620. Loss: 0.04462277889251709\n",
      "Iteration: 2621. Loss: 0.05598306655883789\n",
      "Iteration: 2622. Loss: 0.03525086119771004\n",
      "Iteration: 2623. Loss: 0.06769757717847824\n",
      "Iteration: 2624. Loss: 0.08815319091081619\n",
      "Iteration: 2625. Loss: 0.14294148981571198\n",
      "Iteration: 2626. Loss: 0.24154627323150635\n",
      "Iteration: 2627. Loss: 0.18873488903045654\n",
      "Iteration: 2628. Loss: 0.20349037647247314\n",
      "Iteration: 2629. Loss: 0.07749609649181366\n",
      "Iteration: 2630. Loss: 0.1278354525566101\n",
      "Iteration: 2631. Loss: 0.11746085435152054\n",
      "Iteration: 2632. Loss: 0.048962391912937164\n",
      "Iteration: 2633. Loss: 0.16449882090091705\n",
      "Iteration: 2634. Loss: 0.0592682845890522\n",
      "Iteration: 2635. Loss: 0.09104937314987183\n",
      "Iteration: 2636. Loss: 0.12076075375080109\n",
      "Iteration: 2637. Loss: 0.05688855051994324\n",
      "Iteration: 2638. Loss: 0.08127463608980179\n",
      "Iteration: 2639. Loss: 0.04009542614221573\n",
      "Iteration: 2640. Loss: 0.16245681047439575\n",
      "Iteration: 2641. Loss: 0.08949366211891174\n",
      "Iteration: 2642. Loss: 0.06937143206596375\n",
      "Iteration: 2643. Loss: 0.11147423833608627\n",
      "Iteration: 2644. Loss: 0.022796237841248512\n",
      "Iteration: 2645. Loss: 0.08928563445806503\n",
      "Iteration: 2646. Loss: 0.09141451120376587\n",
      "Iteration: 2647. Loss: 0.12376939505338669\n",
      "Iteration: 2648. Loss: 0.0883106216788292\n",
      "Iteration: 2649. Loss: 0.024601731449365616\n",
      "Iteration: 2650. Loss: 0.13809415698051453\n",
      "Iteration: 2651. Loss: 0.0861675813794136\n",
      "Iteration: 2652. Loss: 0.07229167222976685\n",
      "Iteration: 2653. Loss: 0.22611701488494873\n",
      "Iteration: 2654. Loss: 0.07899602502584457\n",
      "Iteration: 2655. Loss: 0.05314951390028\n",
      "Iteration: 2656. Loss: 0.14567337930202484\n",
      "Iteration: 2657. Loss: 0.15043854713439941\n",
      "Iteration: 2658. Loss: 0.17651565372943878\n",
      "Iteration: 2659. Loss: 0.10263484716415405\n",
      "Iteration: 2660. Loss: 0.1181783452630043\n",
      "Iteration: 2661. Loss: 0.0469747819006443\n",
      "Iteration: 2662. Loss: 0.1008961871266365\n",
      "Iteration: 2663. Loss: 0.0756855309009552\n",
      "Iteration: 2664. Loss: 0.1824055314064026\n",
      "Iteration: 2665. Loss: 0.0845189318060875\n",
      "Iteration: 2666. Loss: 0.07248824834823608\n",
      "Iteration: 2667. Loss: 0.036700308322906494\n",
      "Iteration: 2668. Loss: 0.04934339225292206\n",
      "Iteration: 2669. Loss: 0.20102344453334808\n",
      "Iteration: 2670. Loss: 0.1461980789899826\n",
      "Iteration: 2671. Loss: 0.1069963201880455\n",
      "Iteration: 2672. Loss: 0.14735403656959534\n",
      "Iteration: 2673. Loss: 0.08782506734132767\n",
      "Iteration: 2674. Loss: 0.1493140459060669\n",
      "Iteration: 2675. Loss: 0.2151573747396469\n",
      "Iteration: 2676. Loss: 0.08200192451477051\n",
      "Iteration: 2677. Loss: 0.1755458116531372\n",
      "Iteration: 2678. Loss: 0.05216173827648163\n",
      "Iteration: 2679. Loss: 0.20909547805786133\n",
      "Iteration: 2680. Loss: 0.06535963714122772\n",
      "Iteration: 2681. Loss: 0.0927751362323761\n",
      "Iteration: 2682. Loss: 0.02919536828994751\n",
      "Iteration: 2683. Loss: 0.08031325042247772\n",
      "Iteration: 2684. Loss: 0.13241098821163177\n",
      "Iteration: 2685. Loss: 0.08509493619203568\n",
      "Iteration: 2686. Loss: 0.20474234223365784\n",
      "Iteration: 2687. Loss: 0.12704534828662872\n",
      "Iteration: 2688. Loss: 0.04486323893070221\n",
      "Iteration: 2689. Loss: 0.10987016558647156\n",
      "Iteration: 2690. Loss: 0.11053851991891861\n",
      "Iteration: 2691. Loss: 0.13628791272640228\n",
      "Iteration: 2692. Loss: 0.05887840315699577\n",
      "Iteration: 2693. Loss: 0.02860797941684723\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2694. Loss: 0.10947572439908981\n",
      "Iteration: 2695. Loss: 0.12452099472284317\n",
      "Iteration: 2696. Loss: 0.11676749587059021\n",
      "Iteration: 2697. Loss: 0.06193318963050842\n",
      "Iteration: 2698. Loss: 0.09527101367712021\n",
      "Iteration: 2699. Loss: 0.22313323616981506\n",
      "Iteration: 2700. Loss: 0.10393037647008896\n",
      "Iteration: 2701. Loss: 0.13034778833389282\n",
      "Iteration: 2702. Loss: 0.11037073284387589\n",
      "Iteration: 2703. Loss: 0.09780462086200714\n",
      "Iteration: 2704. Loss: 0.033624038100242615\n",
      "Iteration: 2705. Loss: 0.06568307429552078\n",
      "Iteration: 2706. Loss: 0.06719312071800232\n",
      "Iteration: 2707. Loss: 0.03523131087422371\n",
      "Iteration: 2708. Loss: 0.08525115251541138\n",
      "Iteration: 2709. Loss: 0.08045417815446854\n",
      "Iteration: 2710. Loss: 0.040569305419921875\n",
      "Iteration: 2711. Loss: 0.06596865504980087\n",
      "Iteration: 2712. Loss: 0.06525203585624695\n",
      "Iteration: 2713. Loss: 0.2270570546388626\n",
      "Iteration: 2714. Loss: 0.12099488079547882\n",
      "Iteration: 2715. Loss: 0.05416035279631615\n",
      "Iteration: 2716. Loss: 0.13030306994915009\n",
      "Iteration: 2717. Loss: 0.17615340650081635\n",
      "Iteration: 2718. Loss: 0.04071338474750519\n",
      "Iteration: 2719. Loss: 0.04966847971081734\n",
      "Iteration: 2720. Loss: 0.17305392026901245\n",
      "Iteration: 2721. Loss: 0.07004544138908386\n",
      "Iteration: 2722. Loss: 0.11274746805429459\n",
      "Iteration: 2723. Loss: 0.12540698051452637\n",
      "Iteration: 2724. Loss: 0.10569121688604355\n",
      "Iteration: 2725. Loss: 0.08655717223882675\n",
      "Iteration: 2726. Loss: 0.14180713891983032\n",
      "Iteration: 2727. Loss: 0.1719706952571869\n",
      "Iteration: 2728. Loss: 0.056044820696115494\n",
      "Iteration: 2729. Loss: 0.11753901839256287\n",
      "Iteration: 2730. Loss: 0.1193920448422432\n",
      "Iteration: 2731. Loss: 0.11096946895122528\n",
      "Iteration: 2732. Loss: 0.06123150885105133\n",
      "Iteration: 2733. Loss: 0.1034628301858902\n",
      "Iteration: 2734. Loss: 0.06948287785053253\n",
      "Iteration: 2735. Loss: 0.1365508735179901\n",
      "Iteration: 2736. Loss: 0.03775608167052269\n",
      "Iteration: 2737. Loss: 0.03655923530459404\n",
      "Iteration: 2738. Loss: 0.1323844939470291\n",
      "Iteration: 2739. Loss: 0.26371821761131287\n",
      "Iteration: 2740. Loss: 0.05991915240883827\n",
      "Iteration: 2741. Loss: 0.061667412519454956\n",
      "Iteration: 2742. Loss: 0.05259222537279129\n",
      "Iteration: 2743. Loss: 0.08515165001153946\n",
      "Iteration: 2744. Loss: 0.07457169890403748\n",
      "Iteration: 2745. Loss: 0.18332800269126892\n",
      "Iteration: 2746. Loss: 0.09987123310565948\n",
      "Iteration: 2747. Loss: 0.07271665334701538\n",
      "Iteration: 2748. Loss: 0.15019431710243225\n",
      "Iteration: 2749. Loss: 0.051004473119974136\n",
      "Iteration: 2750. Loss: 0.1294156312942505\n",
      "Iteration: 2751. Loss: 0.04439501836895943\n",
      "Iteration: 2752. Loss: 0.2826376259326935\n",
      "Iteration: 2753. Loss: 0.10257071256637573\n",
      "Iteration: 2754. Loss: 0.08486496657133102\n",
      "Iteration: 2755. Loss: 0.15785683691501617\n",
      "Iteration: 2756. Loss: 0.06951195001602173\n",
      "Iteration: 2757. Loss: 0.09184864163398743\n",
      "Iteration: 2758. Loss: 0.14171141386032104\n",
      "Iteration: 2759. Loss: 0.08158107101917267\n",
      "Iteration: 2760. Loss: 0.07860739529132843\n",
      "Iteration: 2761. Loss: 0.10121731460094452\n",
      "Iteration: 2762. Loss: 0.11956927925348282\n",
      "Iteration: 2763. Loss: 0.1336001455783844\n",
      "Iteration: 2764. Loss: 0.21818414330482483\n",
      "Iteration: 2765. Loss: 0.15568213164806366\n",
      "Iteration: 2766. Loss: 0.1044236272573471\n",
      "Iteration: 2767. Loss: 0.07239261269569397\n",
      "Iteration: 2768. Loss: 0.10438110679388046\n",
      "Iteration: 2769. Loss: 0.08897029608488083\n",
      "Iteration: 2770. Loss: 0.1437283158302307\n",
      "Iteration: 2771. Loss: 0.04482069984078407\n",
      "Iteration: 2772. Loss: 0.12301278859376907\n",
      "Iteration: 2773. Loss: 0.08365436643362045\n",
      "Iteration: 2774. Loss: 0.11605533957481384\n",
      "Iteration: 2775. Loss: 0.11863835155963898\n",
      "Iteration: 2776. Loss: 0.08106671273708344\n",
      "Iteration: 2777. Loss: 0.026452360674738884\n",
      "Iteration: 2778. Loss: 0.2236301749944687\n",
      "Iteration: 2779. Loss: 0.05943823233246803\n",
      "Iteration: 2780. Loss: 0.14577022194862366\n",
      "Iteration: 2781. Loss: 0.13461901247501373\n",
      "Iteration: 2782. Loss: 0.061851486563682556\n",
      "Iteration: 2783. Loss: 0.19760753214359283\n",
      "Iteration: 2784. Loss: 0.09075672924518585\n",
      "Iteration: 2785. Loss: 0.04120218753814697\n",
      "Iteration: 2786. Loss: 0.06682352721691132\n",
      "Iteration: 2787. Loss: 0.05482096970081329\n",
      "Iteration: 2788. Loss: 0.13465677201747894\n",
      "Iteration: 2789. Loss: 0.14031395316123962\n",
      "Iteration: 2790. Loss: 0.10469308495521545\n",
      "Iteration: 2791. Loss: 0.24864666163921356\n",
      "Iteration: 2792. Loss: 0.07585978507995605\n",
      "Iteration: 2793. Loss: 0.24184177815914154\n",
      "Iteration: 2794. Loss: 0.10025925934314728\n",
      "Iteration: 2795. Loss: 0.10025746375322342\n",
      "Iteration: 2796. Loss: 0.09746865183115005\n",
      "Iteration: 2797. Loss: 0.077735036611557\n",
      "Iteration: 2798. Loss: 0.17005543410778046\n",
      "Iteration: 2799. Loss: 0.03609151765704155\n",
      "Iteration: 2800. Loss: 0.09583154320716858\n",
      "Iteration: 2801. Loss: 0.15382999181747437\n",
      "Iteration: 2802. Loss: 0.0910971611738205\n",
      "Iteration: 2803. Loss: 0.09798574447631836\n",
      "Iteration: 2804. Loss: 0.11823079735040665\n",
      "Iteration: 2805. Loss: 0.044790323823690414\n",
      "Iteration: 2806. Loss: 0.028909016400575638\n",
      "Iteration: 2807. Loss: 0.052233461290597916\n",
      "Iteration: 2808. Loss: 0.16263507306575775\n",
      "Iteration: 2809. Loss: 0.09100328385829926\n",
      "Iteration: 2810. Loss: 0.1142377182841301\n",
      "Iteration: 2811. Loss: 0.10755299776792526\n",
      "Iteration: 2812. Loss: 0.12949241697788239\n",
      "Iteration: 2813. Loss: 0.12617313861846924\n",
      "Iteration: 2814. Loss: 0.1748785376548767\n",
      "Iteration: 2815. Loss: 0.11704324930906296\n",
      "Iteration: 2816. Loss: 0.16262735426425934\n",
      "Iteration: 2817. Loss: 0.15139339864253998\n",
      "Iteration: 2818. Loss: 0.08943291008472443\n",
      "Iteration: 2819. Loss: 0.0864657387137413\n",
      "Iteration: 2820. Loss: 0.047897934913635254\n",
      "Iteration: 2821. Loss: 0.1654261350631714\n",
      "Iteration: 2822. Loss: 0.038308728486299515\n",
      "Iteration: 2823. Loss: 0.22421638667583466\n",
      "Iteration: 2824. Loss: 0.07898315787315369\n",
      "Iteration: 2825. Loss: 0.13109399378299713\n",
      "Iteration: 2826. Loss: 0.20804758369922638\n",
      "Iteration: 2827. Loss: 0.07432160526514053\n",
      "Iteration: 2828. Loss: 0.057143207639455795\n",
      "Iteration: 2829. Loss: 0.04719855636358261\n",
      "Iteration: 2830. Loss: 0.05271259322762489\n",
      "Iteration: 2831. Loss: 0.09584502130746841\n",
      "Iteration: 2832. Loss: 0.1503404676914215\n",
      "Iteration: 2833. Loss: 0.08603692054748535\n",
      "Iteration: 2834. Loss: 0.15472668409347534\n",
      "Iteration: 2835. Loss: 0.06975116580724716\n",
      "Iteration: 2836. Loss: 0.1223394125699997\n",
      "Iteration: 2837. Loss: 0.0735308825969696\n",
      "Iteration: 2838. Loss: 0.11328315734863281\n",
      "Iteration: 2839. Loss: 0.1911156326532364\n",
      "Iteration: 2840. Loss: 0.07229616492986679\n",
      "Iteration: 2841. Loss: 0.13475283980369568\n",
      "Iteration: 2842. Loss: 0.08407916128635406\n",
      "Iteration: 2843. Loss: 0.041770853102207184\n",
      "Iteration: 2844. Loss: 0.08420158177614212\n",
      "Iteration: 2845. Loss: 0.09035712480545044\n",
      "Iteration: 2846. Loss: 0.05112598463892937\n",
      "Iteration: 2847. Loss: 0.10535794496536255\n",
      "Iteration: 2848. Loss: 0.23716042935848236\n",
      "Iteration: 2849. Loss: 0.09945043921470642\n",
      "Iteration: 2850. Loss: 0.04010249301791191\n",
      "Iteration: 2851. Loss: 0.09750131517648697\n",
      "Iteration: 2852. Loss: 0.10101398825645447\n",
      "Iteration: 2853. Loss: 0.04693566635251045\n",
      "Iteration: 2854. Loss: 0.12842078506946564\n",
      "Iteration: 2855. Loss: 0.09803362190723419\n",
      "Iteration: 2856. Loss: 0.15439920127391815\n",
      "Iteration: 2857. Loss: 0.12431946396827698\n",
      "Iteration: 2858. Loss: 0.04406549036502838\n",
      "Iteration: 2859. Loss: 0.10272938758134842\n",
      "Iteration: 2860. Loss: 0.11138986796140671\n",
      "Iteration: 2861. Loss: 0.1985045075416565\n",
      "Iteration: 2862. Loss: 0.20983687043190002\n",
      "Iteration: 2863. Loss: 0.08264966309070587\n",
      "Iteration: 2864. Loss: 0.15234142541885376\n",
      "Iteration: 2865. Loss: 0.13326101005077362\n",
      "Iteration: 2866. Loss: 0.1996106505393982\n",
      "Iteration: 2867. Loss: 0.08196521550416946\n",
      "Iteration: 2868. Loss: 0.14544984698295593\n",
      "Iteration: 2869. Loss: 0.08018385618925095\n",
      "Iteration: 2870. Loss: 0.09259822964668274\n",
      "Iteration: 2871. Loss: 0.13944418728351593\n",
      "Iteration: 2872. Loss: 0.1821737140417099\n",
      "Iteration: 2873. Loss: 0.15273568034172058\n",
      "Iteration: 2874. Loss: 0.0977129340171814\n",
      "Iteration: 2875. Loss: 0.17197851836681366\n",
      "Iteration: 2876. Loss: 0.12731964886188507\n",
      "Iteration: 2877. Loss: 0.11214785277843475\n",
      "Iteration: 2878. Loss: 0.09299509227275848\n",
      "Iteration: 2879. Loss: 0.0995594784617424\n",
      "Iteration: 2880. Loss: 0.046948011964559555\n",
      "Iteration: 2881. Loss: 0.1449507474899292\n",
      "Iteration: 2882. Loss: 0.16406315565109253\n",
      "Iteration: 2883. Loss: 0.132759690284729\n",
      "Iteration: 2884. Loss: 0.0768536701798439\n",
      "Iteration: 2885. Loss: 0.1477319896221161\n",
      "Iteration: 2886. Loss: 0.08889559656381607\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2887. Loss: 0.043224919587373734\n",
      "Iteration: 2888. Loss: 0.10188210755586624\n",
      "Iteration: 2889. Loss: 0.1715763509273529\n",
      "Iteration: 2890. Loss: 0.060319457203149796\n",
      "Iteration: 2891. Loss: 0.07966947555541992\n",
      "Iteration: 2892. Loss: 0.09946151077747345\n",
      "Iteration: 2893. Loss: 0.08015517145395279\n",
      "Iteration: 2894. Loss: 0.11153356730937958\n",
      "Iteration: 2895. Loss: 0.15279501676559448\n",
      "Iteration: 2896. Loss: 0.14878256618976593\n",
      "Iteration: 2897. Loss: 0.06893664598464966\n",
      "Iteration: 2898. Loss: 0.11635011434555054\n",
      "Iteration: 2899. Loss: 0.12776978313922882\n",
      "Iteration: 2900. Loss: 0.032608289271593094\n",
      "Iteration: 2901. Loss: 0.09097747504711151\n",
      "Iteration: 2902. Loss: 0.09696827828884125\n",
      "Iteration: 2903. Loss: 0.17596352100372314\n",
      "Iteration: 2904. Loss: 0.1967346966266632\n",
      "Iteration: 2905. Loss: 0.0831632986664772\n",
      "Iteration: 2906. Loss: 0.2039925456047058\n",
      "Iteration: 2907. Loss: 0.07018424570560455\n",
      "Iteration: 2908. Loss: 0.09525808691978455\n",
      "Iteration: 2909. Loss: 0.17678146064281464\n",
      "Iteration: 2910. Loss: 0.15845541656017303\n",
      "Iteration: 2911. Loss: 0.1571216732263565\n",
      "Iteration: 2912. Loss: 0.08977965265512466\n",
      "Iteration: 2913. Loss: 0.05121280625462532\n",
      "Iteration: 2914. Loss: 0.10188912600278854\n",
      "Iteration: 2915. Loss: 0.31769683957099915\n",
      "Iteration: 2916. Loss: 0.0941256582736969\n",
      "Iteration: 2917. Loss: 0.08669930696487427\n",
      "Iteration: 2918. Loss: 0.05123208463191986\n",
      "Iteration: 2919. Loss: 0.15878993272781372\n",
      "Iteration: 2920. Loss: 0.07398398965597153\n",
      "Iteration: 2921. Loss: 0.15367554128170013\n",
      "Iteration: 2922. Loss: 0.12364193797111511\n",
      "Iteration: 2923. Loss: 0.09324381500482559\n",
      "Iteration: 2924. Loss: 0.15112251043319702\n",
      "Iteration: 2925. Loss: 0.07411514222621918\n",
      "Iteration: 2926. Loss: 0.16541962325572968\n",
      "Iteration: 2927. Loss: 0.1369975060224533\n",
      "Iteration: 2928. Loss: 0.08595128357410431\n",
      "Iteration: 2929. Loss: 0.07390926033258438\n",
      "Iteration: 2930. Loss: 0.0527324341237545\n",
      "Iteration: 2931. Loss: 0.13161373138427734\n",
      "Iteration: 2932. Loss: 0.09904862195253372\n",
      "Iteration: 2933. Loss: 0.19472035765647888\n",
      "Iteration: 2934. Loss: 0.07045406103134155\n",
      "Iteration: 2935. Loss: 0.17697538435459137\n",
      "Iteration: 2936. Loss: 0.11485372483730316\n",
      "Iteration: 2937. Loss: 0.08150319010019302\n",
      "Iteration: 2938. Loss: 0.16476404666900635\n",
      "Iteration: 2939. Loss: 0.05574214085936546\n",
      "Iteration: 2940. Loss: 0.0924917608499527\n",
      "Iteration: 2941. Loss: 0.1774357557296753\n",
      "Iteration: 2942. Loss: 0.14531193673610687\n",
      "Iteration: 2943. Loss: 0.11353486776351929\n",
      "Iteration: 2944. Loss: 0.12029437720775604\n",
      "Iteration: 2945. Loss: 0.12113507091999054\n",
      "Iteration: 2946. Loss: 0.13205109536647797\n",
      "Iteration: 2947. Loss: 0.14212149381637573\n",
      "Iteration: 2948. Loss: 0.09225780516862869\n",
      "Iteration: 2949. Loss: 0.09690073877573013\n",
      "Iteration: 2950. Loss: 0.057671017944812775\n",
      "Iteration: 2951. Loss: 0.15350985527038574\n",
      "Iteration: 2952. Loss: 0.2847733199596405\n",
      "Iteration: 2953. Loss: 0.09410858899354935\n",
      "Iteration: 2954. Loss: 0.045158009976148605\n",
      "Iteration: 2955. Loss: 0.03790728375315666\n",
      "Iteration: 2956. Loss: 0.06243513152003288\n",
      "Iteration: 2957. Loss: 0.18143674731254578\n",
      "Iteration: 2958. Loss: 0.09323054552078247\n",
      "Iteration: 2959. Loss: 0.034938860684633255\n",
      "Iteration: 2960. Loss: 0.18406493961811066\n",
      "Iteration: 2961. Loss: 0.21048177778720856\n",
      "Iteration: 2962. Loss: 0.112488754093647\n",
      "Iteration: 2963. Loss: 0.06125732883810997\n",
      "Iteration: 2964. Loss: 0.15492211282253265\n",
      "Iteration: 2965. Loss: 0.1121930181980133\n",
      "Iteration: 2966. Loss: 0.07379105687141418\n",
      "Iteration: 2967. Loss: 0.07384710013866425\n",
      "Iteration: 2968. Loss: 0.04233420267701149\n",
      "Iteration: 2969. Loss: 0.07687927037477493\n",
      "Iteration: 2970. Loss: 0.07253052294254303\n",
      "Iteration: 2971. Loss: 0.07502331584692001\n",
      "Iteration: 2972. Loss: 0.10766089707612991\n",
      "Iteration: 2973. Loss: 0.057559095323085785\n",
      "Iteration: 2974. Loss: 0.03921835124492645\n",
      "Iteration: 2975. Loss: 0.08661764115095139\n",
      "Iteration: 2976. Loss: 0.20338304340839386\n",
      "Iteration: 2977. Loss: 0.06458792835474014\n",
      "Iteration: 2978. Loss: 0.11426788568496704\n",
      "Iteration: 2979. Loss: 0.04438694566488266\n",
      "Iteration: 2980. Loss: 0.17207813262939453\n",
      "Iteration: 2981. Loss: 0.05017928034067154\n",
      "Iteration: 2982. Loss: 0.1449044644832611\n",
      "Iteration: 2983. Loss: 0.14062491059303284\n",
      "Iteration: 2984. Loss: 0.028820786625146866\n",
      "Iteration: 2985. Loss: 0.13358958065509796\n",
      "Iteration: 2986. Loss: 0.09802000969648361\n",
      "Iteration: 2987. Loss: 0.05284377560019493\n",
      "Iteration: 2988. Loss: 0.17121782898902893\n",
      "Iteration: 2989. Loss: 0.164777472615242\n",
      "Iteration: 2990. Loss: 0.05579164996743202\n",
      "Iteration: 2991. Loss: 0.11339760571718216\n",
      "Iteration: 2992. Loss: 0.14675068855285645\n",
      "Iteration: 2993. Loss: 0.22815218567848206\n",
      "Iteration: 2994. Loss: 0.1718442738056183\n",
      "Iteration: 2995. Loss: 0.1393689513206482\n",
      "Iteration: 2996. Loss: 0.1344275325536728\n",
      "Iteration: 2997. Loss: 0.07919017225503922\n",
      "Iteration: 2998. Loss: 0.12461187690496445\n",
      "Iteration: 2999. Loss: 0.07543063908815384\n",
      "Iteration: 3000. Loss: 0.05800364911556244\n"
     ]
    }
   ],
   "source": [
    "# Training \n",
    "# convert input into variable \n",
    "iter=0 \n",
    "for epochs in range(num_epochs):\n",
    "    for i,(images,labels) in enumerate (train_loader): \n",
    "        images = Variable(images.view(-1,28*28))\n",
    "        labels = Variable(labels)\n",
    "        \n",
    "    # clear the gradients\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    # foward pass to obtain the outputs\n",
    "        outputs = model(images)\n",
    "    \n",
    "    #loss \n",
    "        loss = criterion(outputs,labels)\n",
    "    \n",
    "    #backpropagation \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    \n",
    "        iter+=1\n",
    "\n",
    "        print('Iteration: {}. Loss: {}'.format(iter, loss.data[0]))\n",
    "\n",
    "        \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%Iteration: 3000. Loss: 0.05800364911556244. Accuracy: 96.65\n"
     ]
    }
   ],
   "source": [
    "if iter %500 == 0:\n",
    "            # Calculate Accuracy         \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through test dataset\n",
    "            for images, labels in test_loader:\n",
    "               \n",
    "                images = Variable(images.view(-1, 28*28))\n",
    "                \n",
    "                # Forward pass only to get logits/output\n",
    "                outputs = model(images)\n",
    "                \n",
    "                # Get predictions from the maximum value\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                \n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "                \n",
    "                correct += (predicted.cpu() == labels.cpu()).sum()\n",
    "            \n",
    "            accuracy = 100 * correct / total\n",
    "        \n",
    "            # Print Loss\n",
    "            print('%Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.data[0], accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
