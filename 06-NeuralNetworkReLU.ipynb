{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dataset = dsets.MNIST(root='./data', \n",
    "                            train=True, \n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='./data', \n",
    "                           train=False, \n",
    "                           transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "n_iters = 3000\n",
    "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a model class \n",
    "# input->linear function->non linear function(sigmoid)->linear function->Softmax->CrossEntropy\n",
    "class FeedFowardNeuralNetwork(nn.Module): \n",
    "    def __init__(self,input_dim,hidden_dim,output_dim):\n",
    "        super(FeedFowardNeuralNetwork,self).__init__()\n",
    "        self.fc1=nn.Linear(input_dim,hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2=nn.Linear(hidden_dim,output_dim)\n",
    "        \n",
    "    def forward(self,x): \n",
    "        out=self.fc1(x)\n",
    "        out=self.relu(out)\n",
    "        out=self.fc2(out)\n",
    "        return out \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the model \n",
    "input_dim=28*28 \n",
    "hidden_dim=100\n",
    "output_dim=10 \n",
    "model=FeedFowardNeuralNetwork(input_dim,hidden_dim,output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# instantiate the loss \n",
    "criterion=nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# instatiate the optimize \n",
    "learning_rate=0.1\n",
    "optimizer=torch.optim.SGD(model.parameters(),lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "torch.Size([100, 784])\n",
      "torch.Size([100])\n",
      "torch.Size([10, 100])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "print(len(list(model.parameters())))\n",
    "print(list(model.parameters())[0].size())\n",
    "print(list(model.parameters())[1].size())\n",
    "print(list(model.parameters())[2].size())\n",
    "print(list(model.parameters())[3].size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1. Loss: 2.304568290710449\n",
      "Iteration: 2. Loss: 2.3007898330688477\n",
      "Iteration: 3. Loss: 2.2684624195098877\n",
      "Iteration: 4. Loss: 2.2446839809417725\n",
      "Iteration: 5. Loss: 2.231877088546753\n",
      "Iteration: 6. Loss: 2.2281017303466797\n",
      "Iteration: 7. Loss: 2.208669662475586\n",
      "Iteration: 8. Loss: 2.182025909423828\n",
      "Iteration: 9. Loss: 2.18385648727417\n",
      "Iteration: 10. Loss: 2.1456480026245117\n",
      "Iteration: 11. Loss: 2.1451170444488525\n",
      "Iteration: 12. Loss: 2.0888419151306152\n",
      "Iteration: 13. Loss: 2.1040329933166504\n",
      "Iteration: 14. Loss: 2.053187608718872\n",
      "Iteration: 15. Loss: 2.0388927459716797\n",
      "Iteration: 16. Loss: 2.0337793827056885\n",
      "Iteration: 17. Loss: 1.968064546585083\n",
      "Iteration: 18. Loss: 1.9469878673553467\n",
      "Iteration: 19. Loss: 1.9215055704116821\n",
      "Iteration: 20. Loss: 1.8813903331756592\n",
      "Iteration: 21. Loss: 1.8320906162261963\n",
      "Iteration: 22. Loss: 1.8658249378204346\n",
      "Iteration: 23. Loss: 1.8529778718948364\n",
      "Iteration: 24. Loss: 1.8202382326126099\n",
      "Iteration: 25. Loss: 1.8139169216156006\n",
      "Iteration: 26. Loss: 1.6901841163635254\n",
      "Iteration: 27. Loss: 1.6791915893554688\n",
      "Iteration: 28. Loss: 1.708611011505127\n",
      "Iteration: 29. Loss: 1.6506049633026123\n",
      "Iteration: 30. Loss: 1.5508800745010376\n",
      "Iteration: 31. Loss: 1.5762808322906494\n",
      "Iteration: 32. Loss: 1.5840731859207153\n",
      "Iteration: 33. Loss: 1.5257935523986816\n",
      "Iteration: 34. Loss: 1.4367491006851196\n",
      "Iteration: 35. Loss: 1.4045602083206177\n",
      "Iteration: 36. Loss: 1.4482595920562744\n",
      "Iteration: 37. Loss: 1.3418655395507812\n",
      "Iteration: 38. Loss: 1.3616214990615845\n",
      "Iteration: 39. Loss: 1.2952500581741333\n",
      "Iteration: 40. Loss: 1.383091688156128\n",
      "Iteration: 41. Loss: 1.2791731357574463\n",
      "Iteration: 42. Loss: 1.2816237211227417\n",
      "Iteration: 43. Loss: 1.2406505346298218\n",
      "Iteration: 44. Loss: 1.2815539836883545\n",
      "Iteration: 45. Loss: 1.050449013710022\n",
      "Iteration: 46. Loss: 1.176846981048584\n",
      "Iteration: 47. Loss: 1.1502383947372437\n",
      "Iteration: 48. Loss: 1.0371798276901245\n",
      "Iteration: 49. Loss: 1.0287094116210938\n",
      "Iteration: 50. Loss: 1.0232831239700317\n",
      "Iteration: 51. Loss: 1.0039591789245605\n",
      "Iteration: 52. Loss: 1.0424187183380127\n",
      "Iteration: 53. Loss: 1.0248616933822632\n",
      "Iteration: 54. Loss: 0.9532756209373474\n",
      "Iteration: 55. Loss: 0.9823763370513916\n",
      "Iteration: 56. Loss: 0.9554367661476135\n",
      "Iteration: 57. Loss: 0.9145030379295349\n",
      "Iteration: 58. Loss: 0.9424381256103516\n",
      "Iteration: 59. Loss: 0.8907145857810974\n",
      "Iteration: 60. Loss: 0.9309420585632324\n",
      "Iteration: 61. Loss: 0.8013736009597778\n",
      "Iteration: 62. Loss: 0.9253724813461304\n",
      "Iteration: 63. Loss: 0.9311766028404236\n",
      "Iteration: 64. Loss: 0.9631443023681641\n",
      "Iteration: 65. Loss: 0.8229156732559204\n",
      "Iteration: 66. Loss: 0.7693631052970886\n",
      "Iteration: 67. Loss: 0.8243635296821594\n",
      "Iteration: 68. Loss: 0.8051719069480896\n",
      "Iteration: 69. Loss: 0.8652423024177551\n",
      "Iteration: 70. Loss: 0.8161402344703674\n",
      "Iteration: 71. Loss: 0.801343560218811\n",
      "Iteration: 72. Loss: 0.789649248123169\n",
      "Iteration: 73. Loss: 0.8034355044364929\n",
      "Iteration: 74. Loss: 0.733641505241394\n",
      "Iteration: 75. Loss: 0.8285291194915771\n",
      "Iteration: 76. Loss: 0.5325504541397095\n",
      "Iteration: 77. Loss: 0.856645405292511\n",
      "Iteration: 78. Loss: 0.621768593788147\n",
      "Iteration: 79. Loss: 0.833676815032959\n",
      "Iteration: 80. Loss: 0.7395603656768799\n",
      "Iteration: 81. Loss: 0.6546611785888672\n",
      "Iteration: 82. Loss: 0.7250938415527344\n",
      "Iteration: 83. Loss: 0.7052841782569885\n",
      "Iteration: 84. Loss: 0.6800696849822998\n",
      "Iteration: 85. Loss: 0.5977417230606079\n",
      "Iteration: 86. Loss: 0.808948814868927\n",
      "Iteration: 87. Loss: 0.5964186787605286\n",
      "Iteration: 88. Loss: 0.6907124519348145\n",
      "Iteration: 89. Loss: 0.5810793042182922\n",
      "Iteration: 90. Loss: 0.7261931896209717\n",
      "Iteration: 91. Loss: 0.6696967482566833\n",
      "Iteration: 92. Loss: 0.5098191499710083\n",
      "Iteration: 93. Loss: 0.6996249556541443\n",
      "Iteration: 94. Loss: 0.601396918296814\n",
      "Iteration: 95. Loss: 0.49176567792892456\n",
      "Iteration: 96. Loss: 0.6535298824310303\n",
      "Iteration: 97. Loss: 0.6473227143287659\n",
      "Iteration: 98. Loss: 0.554573118686676\n",
      "Iteration: 99. Loss: 0.6267622709274292\n",
      "Iteration: 100. Loss: 0.6468297839164734\n",
      "Iteration: 101. Loss: 0.5775778889656067\n",
      "Iteration: 102. Loss: 0.6910128593444824\n",
      "Iteration: 103. Loss: 0.5187243819236755\n",
      "Iteration: 104. Loss: 0.6725798845291138\n",
      "Iteration: 105. Loss: 0.5462228059768677\n",
      "Iteration: 106. Loss: 0.5990792512893677\n",
      "Iteration: 107. Loss: 0.620283305644989\n",
      "Iteration: 108. Loss: 0.6860809326171875\n",
      "Iteration: 109. Loss: 0.5510018467903137\n",
      "Iteration: 110. Loss: 0.5203173160552979\n",
      "Iteration: 111. Loss: 0.6070249080657959\n",
      "Iteration: 112. Loss: 0.49033796787261963\n",
      "Iteration: 113. Loss: 0.6495698690414429\n",
      "Iteration: 114. Loss: 0.7085080742835999\n",
      "Iteration: 115. Loss: 0.589382529258728\n",
      "Iteration: 116. Loss: 0.4541628956794739\n",
      "Iteration: 117. Loss: 0.44000157713890076\n",
      "Iteration: 118. Loss: 0.6918255686759949\n",
      "Iteration: 119. Loss: 0.6374027729034424\n",
      "Iteration: 120. Loss: 0.5565662980079651\n",
      "Iteration: 121. Loss: 0.5061339735984802\n",
      "Iteration: 122. Loss: 0.6315610408782959\n",
      "Iteration: 123. Loss: 0.5594685077667236\n",
      "Iteration: 124. Loss: 0.5628573894500732\n",
      "Iteration: 125. Loss: 0.474989116191864\n",
      "Iteration: 126. Loss: 0.5759497284889221\n",
      "Iteration: 127. Loss: 0.4650251865386963\n",
      "Iteration: 128. Loss: 0.6601677536964417\n",
      "Iteration: 129. Loss: 0.5128902196884155\n",
      "Iteration: 130. Loss: 0.6032636761665344\n",
      "Iteration: 131. Loss: 0.5712996125221252\n",
      "Iteration: 132. Loss: 0.5594160556793213\n",
      "Iteration: 133. Loss: 0.588324785232544\n",
      "Iteration: 134. Loss: 0.5346660614013672\n",
      "Iteration: 135. Loss: 0.5051066875457764\n",
      "Iteration: 136. Loss: 0.6247584223747253\n",
      "Iteration: 137. Loss: 0.515874445438385\n",
      "Iteration: 138. Loss: 0.47212910652160645\n",
      "Iteration: 139. Loss: 0.40012991428375244\n",
      "Iteration: 140. Loss: 0.6125876903533936\n",
      "Iteration: 141. Loss: 0.4869877099990845\n",
      "Iteration: 142. Loss: 0.5260634422302246\n",
      "Iteration: 143. Loss: 0.5483091473579407\n",
      "Iteration: 144. Loss: 0.5565778613090515\n",
      "Iteration: 145. Loss: 0.5428512096405029\n",
      "Iteration: 146. Loss: 0.5060811042785645\n",
      "Iteration: 147. Loss: 0.3870864510536194\n",
      "Iteration: 148. Loss: 0.4999768137931824\n",
      "Iteration: 149. Loss: 0.3910006284713745\n",
      "Iteration: 150. Loss: 0.44626137614250183\n",
      "Iteration: 151. Loss: 0.41941937804222107\n",
      "Iteration: 152. Loss: 0.4755501449108124\n",
      "Iteration: 153. Loss: 0.40210047364234924\n",
      "Iteration: 154. Loss: 0.5149776339530945\n",
      "Iteration: 155. Loss: 0.39407870173454285\n",
      "Iteration: 156. Loss: 0.4404715597629547\n",
      "Iteration: 157. Loss: 0.39017876982688904\n",
      "Iteration: 158. Loss: 0.44362151622772217\n",
      "Iteration: 159. Loss: 0.5241276621818542\n",
      "Iteration: 160. Loss: 0.5096930861473083\n",
      "Iteration: 161. Loss: 0.45388278365135193\n",
      "Iteration: 162. Loss: 0.5477628111839294\n",
      "Iteration: 163. Loss: 0.6019706130027771\n",
      "Iteration: 164. Loss: 0.45717719197273254\n",
      "Iteration: 165. Loss: 0.4734021723270416\n",
      "Iteration: 166. Loss: 0.47179722785949707\n",
      "Iteration: 167. Loss: 0.5066481232643127\n",
      "Iteration: 168. Loss: 0.4777339994907379\n",
      "Iteration: 169. Loss: 0.4966549277305603\n",
      "Iteration: 170. Loss: 0.506060779094696\n",
      "Iteration: 171. Loss: 0.5082790851593018\n",
      "Iteration: 172. Loss: 0.37070250511169434\n",
      "Iteration: 173. Loss: 0.45869842171669006\n",
      "Iteration: 174. Loss: 0.4670143127441406\n",
      "Iteration: 175. Loss: 0.5444867610931396\n",
      "Iteration: 176. Loss: 0.45041441917419434\n",
      "Iteration: 177. Loss: 0.36213216185569763\n",
      "Iteration: 178. Loss: 0.4012376070022583\n",
      "Iteration: 179. Loss: 0.47213849425315857\n",
      "Iteration: 180. Loss: 0.4728253185749054\n",
      "Iteration: 181. Loss: 0.4863266348838806\n",
      "Iteration: 182. Loss: 0.4881412088871002\n",
      "Iteration: 183. Loss: 0.41843143105506897\n",
      "Iteration: 184. Loss: 0.34180739521980286\n",
      "Iteration: 185. Loss: 0.4695625305175781\n",
      "Iteration: 186. Loss: 0.36574047803878784\n",
      "Iteration: 187. Loss: 0.3712339401245117\n",
      "Iteration: 188. Loss: 0.4144827127456665\n",
      "Iteration: 189. Loss: 0.5195779204368591\n",
      "Iteration: 190. Loss: 0.6043589115142822\n",
      "Iteration: 191. Loss: 0.4982225000858307\n",
      "Iteration: 192. Loss: 0.5641341805458069\n",
      "Iteration: 193. Loss: 0.3349500000476837\n",
      "Iteration: 194. Loss: 0.5186284780502319\n",
      "Iteration: 195. Loss: 0.4060685336589813\n",
      "Iteration: 196. Loss: 0.3674628734588623\n",
      "Iteration: 197. Loss: 0.4809996783733368\n",
      "Iteration: 198. Loss: 0.4720875024795532\n",
      "Iteration: 199. Loss: 0.3758107125759125\n",
      "Iteration: 200. Loss: 0.44678428769111633\n",
      "Iteration: 201. Loss: 0.37091416120529175\n",
      "Iteration: 202. Loss: 0.5021828413009644\n",
      "Iteration: 203. Loss: 0.38887298107147217\n",
      "Iteration: 204. Loss: 0.43791213631629944\n",
      "Iteration: 205. Loss: 0.40533873438835144\n",
      "Iteration: 206. Loss: 0.24783380329608917\n",
      "Iteration: 207. Loss: 0.48869553208351135\n",
      "Iteration: 208. Loss: 0.3514624536037445\n",
      "Iteration: 209. Loss: 0.5114436745643616\n",
      "Iteration: 210. Loss: 0.3554309010505676\n",
      "Iteration: 211. Loss: 0.46582573652267456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 212. Loss: 0.47948750853538513\n",
      "Iteration: 213. Loss: 0.335465669631958\n",
      "Iteration: 214. Loss: 0.3196067810058594\n",
      "Iteration: 215. Loss: 0.3712795674800873\n",
      "Iteration: 216. Loss: 0.46826252341270447\n",
      "Iteration: 217. Loss: 0.5317886471748352\n",
      "Iteration: 218. Loss: 0.4105917811393738\n",
      "Iteration: 219. Loss: 0.4404526948928833\n",
      "Iteration: 220. Loss: 0.35654938220977783\n",
      "Iteration: 221. Loss: 0.3082595467567444\n",
      "Iteration: 222. Loss: 0.6001537442207336\n",
      "Iteration: 223. Loss: 0.38756850361824036\n",
      "Iteration: 224. Loss: 0.44036856293678284\n",
      "Iteration: 225. Loss: 0.30504733324050903\n",
      "Iteration: 226. Loss: 0.5231377482414246\n",
      "Iteration: 227. Loss: 0.3859824240207672\n",
      "Iteration: 228. Loss: 0.3989296853542328\n",
      "Iteration: 229. Loss: 0.3993067145347595\n",
      "Iteration: 230. Loss: 0.31584668159484863\n",
      "Iteration: 231. Loss: 0.4281332492828369\n",
      "Iteration: 232. Loss: 0.26937952637672424\n",
      "Iteration: 233. Loss: 0.5000604391098022\n",
      "Iteration: 234. Loss: 0.39248421788215637\n",
      "Iteration: 235. Loss: 0.505332887172699\n",
      "Iteration: 236. Loss: 0.3310581147670746\n",
      "Iteration: 237. Loss: 0.369726300239563\n",
      "Iteration: 238. Loss: 0.37407898902893066\n",
      "Iteration: 239. Loss: 0.45779553055763245\n",
      "Iteration: 240. Loss: 0.419612318277359\n",
      "Iteration: 241. Loss: 0.3311535120010376\n",
      "Iteration: 242. Loss: 0.4834681451320648\n",
      "Iteration: 243. Loss: 0.39980852603912354\n",
      "Iteration: 244. Loss: 0.4177580773830414\n",
      "Iteration: 245. Loss: 0.501570999622345\n",
      "Iteration: 246. Loss: 0.37164023518562317\n",
      "Iteration: 247. Loss: 0.4436878263950348\n",
      "Iteration: 248. Loss: 0.30645668506622314\n",
      "Iteration: 249. Loss: 0.4115280210971832\n",
      "Iteration: 250. Loss: 0.4762677848339081\n",
      "Iteration: 251. Loss: 0.45053061842918396\n",
      "Iteration: 252. Loss: 0.5281001925468445\n",
      "Iteration: 253. Loss: 0.42560359835624695\n",
      "Iteration: 254. Loss: 0.32521793246269226\n",
      "Iteration: 255. Loss: 0.4419953525066376\n",
      "Iteration: 256. Loss: 0.3894396722316742\n",
      "Iteration: 257. Loss: 0.24805186688899994\n",
      "Iteration: 258. Loss: 0.34661683440208435\n",
      "Iteration: 259. Loss: 0.43309304118156433\n",
      "Iteration: 260. Loss: 0.40751075744628906\n",
      "Iteration: 261. Loss: 0.47980108857154846\n",
      "Iteration: 262. Loss: 0.4229895770549774\n",
      "Iteration: 263. Loss: 0.31121814250946045\n",
      "Iteration: 264. Loss: 0.3753148019313812\n",
      "Iteration: 265. Loss: 0.44168803095817566\n",
      "Iteration: 266. Loss: 0.38954484462738037\n",
      "Iteration: 267. Loss: 0.49393078684806824\n",
      "Iteration: 268. Loss: 0.42047595977783203\n",
      "Iteration: 269. Loss: 0.3402147591114044\n",
      "Iteration: 270. Loss: 0.29815906286239624\n",
      "Iteration: 271. Loss: 0.37130364775657654\n",
      "Iteration: 272. Loss: 0.4905526638031006\n",
      "Iteration: 273. Loss: 0.3910672664642334\n",
      "Iteration: 274. Loss: 0.2868277132511139\n",
      "Iteration: 275. Loss: 0.4041440486907959\n",
      "Iteration: 276. Loss: 0.37818074226379395\n",
      "Iteration: 277. Loss: 0.35925784707069397\n",
      "Iteration: 278. Loss: 0.4829556941986084\n",
      "Iteration: 279. Loss: 0.4659377336502075\n",
      "Iteration: 280. Loss: 0.41942092776298523\n",
      "Iteration: 281. Loss: 0.21490643918514252\n",
      "Iteration: 282. Loss: 0.4288616478443146\n",
      "Iteration: 283. Loss: 0.4282294809818268\n",
      "Iteration: 284. Loss: 0.43425408005714417\n",
      "Iteration: 285. Loss: 0.21873138844966888\n",
      "Iteration: 286. Loss: 0.5554097294807434\n",
      "Iteration: 287. Loss: 0.27813661098480225\n",
      "Iteration: 288. Loss: 0.40480101108551025\n",
      "Iteration: 289. Loss: 0.28594738245010376\n",
      "Iteration: 290. Loss: 0.3939433991909027\n",
      "Iteration: 291. Loss: 0.33074623346328735\n",
      "Iteration: 292. Loss: 0.2569143772125244\n",
      "Iteration: 293. Loss: 0.2612062394618988\n",
      "Iteration: 294. Loss: 0.4714004099369049\n",
      "Iteration: 295. Loss: 0.331695020198822\n",
      "Iteration: 296. Loss: 0.3933781087398529\n",
      "Iteration: 297. Loss: 0.5285958647727966\n",
      "Iteration: 298. Loss: 0.4077394902706146\n",
      "Iteration: 299. Loss: 0.2971004247665405\n",
      "Iteration: 300. Loss: 0.21118217706680298\n",
      "Iteration: 301. Loss: 0.4258570969104767\n",
      "Iteration: 302. Loss: 0.2740544080734253\n",
      "Iteration: 303. Loss: 0.34937557578086853\n",
      "Iteration: 304. Loss: 0.27377745509147644\n",
      "Iteration: 305. Loss: 0.3936699330806732\n",
      "Iteration: 306. Loss: 0.3925967812538147\n",
      "Iteration: 307. Loss: 0.30590832233428955\n",
      "Iteration: 308. Loss: 0.3107653558254242\n",
      "Iteration: 309. Loss: 0.25507718324661255\n",
      "Iteration: 310. Loss: 0.3790954649448395\n",
      "Iteration: 311. Loss: 0.2839009761810303\n",
      "Iteration: 312. Loss: 0.35882800817489624\n",
      "Iteration: 313. Loss: 0.26593026518821716\n",
      "Iteration: 314. Loss: 0.3401445150375366\n",
      "Iteration: 315. Loss: 0.3734520375728607\n",
      "Iteration: 316. Loss: 0.49511510133743286\n",
      "Iteration: 317. Loss: 0.46080484986305237\n",
      "Iteration: 318. Loss: 0.29545915126800537\n",
      "Iteration: 319. Loss: 0.36197441816329956\n",
      "Iteration: 320. Loss: 0.48748522996902466\n",
      "Iteration: 321. Loss: 0.3289458155632019\n",
      "Iteration: 322. Loss: 0.3601871132850647\n",
      "Iteration: 323. Loss: 0.33021900057792664\n",
      "Iteration: 324. Loss: 0.36704257130622864\n",
      "Iteration: 325. Loss: 0.5445159077644348\n",
      "Iteration: 326. Loss: 0.36613669991493225\n",
      "Iteration: 327. Loss: 0.3833772540092468\n",
      "Iteration: 328. Loss: 0.3930507004261017\n",
      "Iteration: 329. Loss: 0.22408704459667206\n",
      "Iteration: 330. Loss: 0.3387647271156311\n",
      "Iteration: 331. Loss: 0.2290680706501007\n",
      "Iteration: 332. Loss: 0.27612438797950745\n",
      "Iteration: 333. Loss: 0.3800088167190552\n",
      "Iteration: 334. Loss: 0.2886502742767334\n",
      "Iteration: 335. Loss: 0.41647040843963623\n",
      "Iteration: 336. Loss: 0.3996242880821228\n",
      "Iteration: 337. Loss: 0.2911161482334137\n",
      "Iteration: 338. Loss: 0.3163122236728668\n",
      "Iteration: 339. Loss: 0.23487798869609833\n",
      "Iteration: 340. Loss: 0.35950523614883423\n",
      "Iteration: 341. Loss: 0.4547918736934662\n",
      "Iteration: 342. Loss: 0.2949817478656769\n",
      "Iteration: 343. Loss: 0.44435006380081177\n",
      "Iteration: 344. Loss: 0.38405123353004456\n",
      "Iteration: 345. Loss: 0.47710296511650085\n",
      "Iteration: 346. Loss: 0.2983261048793793\n",
      "Iteration: 347. Loss: 0.3943876326084137\n",
      "Iteration: 348. Loss: 0.4908381700515747\n",
      "Iteration: 349. Loss: 0.47443312406539917\n",
      "Iteration: 350. Loss: 0.36524590849876404\n",
      "Iteration: 351. Loss: 0.3922256827354431\n",
      "Iteration: 352. Loss: 0.3055896461009979\n",
      "Iteration: 353. Loss: 0.2686212956905365\n",
      "Iteration: 354. Loss: 0.3528556823730469\n",
      "Iteration: 355. Loss: 0.24551281332969666\n",
      "Iteration: 356. Loss: 0.27532997727394104\n",
      "Iteration: 357. Loss: 0.6408596634864807\n",
      "Iteration: 358. Loss: 0.4983009994029999\n",
      "Iteration: 359. Loss: 0.5844197273254395\n",
      "Iteration: 360. Loss: 0.3002054989337921\n",
      "Iteration: 361. Loss: 0.38648590445518494\n",
      "Iteration: 362. Loss: 0.3189317286014557\n",
      "Iteration: 363. Loss: 0.2738839387893677\n",
      "Iteration: 364. Loss: 0.39537855982780457\n",
      "Iteration: 365. Loss: 0.3638869822025299\n",
      "Iteration: 366. Loss: 0.422067791223526\n",
      "Iteration: 367. Loss: 0.3143582046031952\n",
      "Iteration: 368. Loss: 0.3881543278694153\n",
      "Iteration: 369. Loss: 0.3963707387447357\n",
      "Iteration: 370. Loss: 0.270852655172348\n",
      "Iteration: 371. Loss: 0.3414469063282013\n",
      "Iteration: 372. Loss: 0.43504369258880615\n",
      "Iteration: 373. Loss: 0.3085796535015106\n",
      "Iteration: 374. Loss: 0.4335334300994873\n",
      "Iteration: 375. Loss: 0.34064042568206787\n",
      "Iteration: 376. Loss: 0.3728768825531006\n",
      "Iteration: 377. Loss: 0.2766578197479248\n",
      "Iteration: 378. Loss: 0.42512279748916626\n",
      "Iteration: 379. Loss: 0.279227614402771\n",
      "Iteration: 380. Loss: 0.3098119795322418\n",
      "Iteration: 381. Loss: 0.3010305166244507\n",
      "Iteration: 382. Loss: 0.4500877857208252\n",
      "Iteration: 383. Loss: 0.36465099453926086\n",
      "Iteration: 384. Loss: 0.28497159481048584\n",
      "Iteration: 385. Loss: 0.27519693970680237\n",
      "Iteration: 386. Loss: 0.44570431113243103\n",
      "Iteration: 387. Loss: 0.3273506462574005\n",
      "Iteration: 388. Loss: 0.3941139280796051\n",
      "Iteration: 389. Loss: 0.24320641160011292\n",
      "Iteration: 390. Loss: 0.28990697860717773\n",
      "Iteration: 391. Loss: 0.26189330220222473\n",
      "Iteration: 392. Loss: 0.2984010577201843\n",
      "Iteration: 393. Loss: 0.30254408717155457\n",
      "Iteration: 394. Loss: 0.2706414759159088\n",
      "Iteration: 395. Loss: 0.5624101758003235\n",
      "Iteration: 396. Loss: 0.4300062954425812\n",
      "Iteration: 397. Loss: 0.34783804416656494\n",
      "Iteration: 398. Loss: 0.24385225772857666\n",
      "Iteration: 399. Loss: 0.46027880907058716\n",
      "Iteration: 400. Loss: 0.3016342222690582\n",
      "Iteration: 401. Loss: 0.29297715425491333\n",
      "Iteration: 402. Loss: 0.3111414313316345\n",
      "Iteration: 403. Loss: 0.4452822804450989\n",
      "Iteration: 404. Loss: 0.40428391098976135\n",
      "Iteration: 405. Loss: 0.26035046577453613\n",
      "Iteration: 406. Loss: 0.3295412063598633\n",
      "Iteration: 407. Loss: 0.3541375696659088\n",
      "Iteration: 408. Loss: 0.3490782678127289\n",
      "Iteration: 409. Loss: 0.3305981159210205\n",
      "Iteration: 410. Loss: 0.3199704587459564\n",
      "Iteration: 411. Loss: 0.5727245807647705\n",
      "Iteration: 412. Loss: 0.2852895259857178\n",
      "Iteration: 413. Loss: 0.2885738015174866\n",
      "Iteration: 414. Loss: 0.5510249137878418\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 415. Loss: 0.24716119468212128\n",
      "Iteration: 416. Loss: 0.2716721296310425\n",
      "Iteration: 417. Loss: 0.346912145614624\n",
      "Iteration: 418. Loss: 0.21301555633544922\n",
      "Iteration: 419. Loss: 0.5428553223609924\n",
      "Iteration: 420. Loss: 0.42069634795188904\n",
      "Iteration: 421. Loss: 0.41925084590911865\n",
      "Iteration: 422. Loss: 0.3702988028526306\n",
      "Iteration: 423. Loss: 0.23930507898330688\n",
      "Iteration: 424. Loss: 0.4433562159538269\n",
      "Iteration: 425. Loss: 0.31293830275535583\n",
      "Iteration: 426. Loss: 0.3285143971443176\n",
      "Iteration: 427. Loss: 0.2440948486328125\n",
      "Iteration: 428. Loss: 0.41964712738990784\n",
      "Iteration: 429. Loss: 0.3217262625694275\n",
      "Iteration: 430. Loss: 0.2937425374984741\n",
      "Iteration: 431. Loss: 0.3183949887752533\n",
      "Iteration: 432. Loss: 0.35360658168792725\n",
      "Iteration: 433. Loss: 0.24391412734985352\n",
      "Iteration: 434. Loss: 0.4350854456424713\n",
      "Iteration: 435. Loss: 0.2707177996635437\n",
      "Iteration: 436. Loss: 0.42165032029151917\n",
      "Iteration: 437. Loss: 0.2570733428001404\n",
      "Iteration: 438. Loss: 0.20202063024044037\n",
      "Iteration: 439. Loss: 0.3591696619987488\n",
      "Iteration: 440. Loss: 0.3727797567844391\n",
      "Iteration: 441. Loss: 0.4223446249961853\n",
      "Iteration: 442. Loss: 0.16826066374778748\n",
      "Iteration: 443. Loss: 0.3102926015853882\n",
      "Iteration: 444. Loss: 0.3962842524051666\n",
      "Iteration: 445. Loss: 0.41760462522506714\n",
      "Iteration: 446. Loss: 0.4005294442176819\n",
      "Iteration: 447. Loss: 0.24321676790714264\n",
      "Iteration: 448. Loss: 0.2358887642621994\n",
      "Iteration: 449. Loss: 0.31707677245140076\n",
      "Iteration: 450. Loss: 0.2861909568309784\n",
      "Iteration: 451. Loss: 0.31686997413635254\n",
      "Iteration: 452. Loss: 0.2783074676990509\n",
      "Iteration: 453. Loss: 0.32287901639938354\n",
      "Iteration: 454. Loss: 0.28828921914100647\n",
      "Iteration: 455. Loss: 0.3933151662349701\n",
      "Iteration: 456. Loss: 0.6150731444358826\n",
      "Iteration: 457. Loss: 0.3000052571296692\n",
      "Iteration: 458. Loss: 0.23834609985351562\n",
      "Iteration: 459. Loss: 0.4464668929576874\n",
      "Iteration: 460. Loss: 0.24712376296520233\n",
      "Iteration: 461. Loss: 0.3263189196586609\n",
      "Iteration: 462. Loss: 0.31408095359802246\n",
      "Iteration: 463. Loss: 0.2300066202878952\n",
      "Iteration: 464. Loss: 0.23325447738170624\n",
      "Iteration: 465. Loss: 0.2680566906929016\n",
      "Iteration: 466. Loss: 0.44487830996513367\n",
      "Iteration: 467. Loss: 0.19180187582969666\n",
      "Iteration: 468. Loss: 0.3443363904953003\n",
      "Iteration: 469. Loss: 0.5032544732093811\n",
      "Iteration: 470. Loss: 0.2924096882343292\n",
      "Iteration: 471. Loss: 0.2913986146450043\n",
      "Iteration: 472. Loss: 0.45170044898986816\n",
      "Iteration: 473. Loss: 0.3673596680164337\n",
      "Iteration: 474. Loss: 0.45368558168411255\n",
      "Iteration: 475. Loss: 0.36120036244392395\n",
      "Iteration: 476. Loss: 0.3391039967536926\n",
      "Iteration: 477. Loss: 0.379236102104187\n",
      "Iteration: 478. Loss: 0.2643798291683197\n",
      "Iteration: 479. Loss: 0.23509527742862701\n",
      "Iteration: 480. Loss: 0.29033106565475464\n",
      "Iteration: 481. Loss: 0.17476437985897064\n",
      "Iteration: 482. Loss: 0.3062223494052887\n",
      "Iteration: 483. Loss: 0.33344703912734985\n",
      "Iteration: 484. Loss: 0.2519068121910095\n",
      "Iteration: 485. Loss: 0.34360581636428833\n",
      "Iteration: 486. Loss: 0.3614078760147095\n",
      "Iteration: 487. Loss: 0.31959113478660583\n",
      "Iteration: 488. Loss: 0.35108739137649536\n",
      "Iteration: 489. Loss: 0.373814195394516\n",
      "Iteration: 490. Loss: 0.4494869112968445\n",
      "Iteration: 491. Loss: 0.3362906277179718\n",
      "Iteration: 492. Loss: 0.24669305980205536\n",
      "Iteration: 493. Loss: 0.5209823846817017\n",
      "Iteration: 494. Loss: 0.27443307638168335\n",
      "Iteration: 495. Loss: 0.31548526883125305\n",
      "Iteration: 496. Loss: 0.2562945783138275\n",
      "Iteration: 497. Loss: 0.34868720173835754\n",
      "Iteration: 498. Loss: 0.28237059712409973\n",
      "Iteration: 499. Loss: 0.20927369594573975\n",
      "Iteration: 500. Loss: 0.3228286802768707\n",
      "Iteration: 501. Loss: 0.2051738202571869\n",
      "Iteration: 502. Loss: 0.3456769585609436\n",
      "Iteration: 503. Loss: 0.34513476490974426\n",
      "Iteration: 504. Loss: 0.4492965042591095\n",
      "Iteration: 505. Loss: 0.19641970098018646\n",
      "Iteration: 506. Loss: 0.22954988479614258\n",
      "Iteration: 507. Loss: 0.19764171540737152\n",
      "Iteration: 508. Loss: 0.2377590537071228\n",
      "Iteration: 509. Loss: 0.29266881942749023\n",
      "Iteration: 510. Loss: 0.294766366481781\n",
      "Iteration: 511. Loss: 0.36345043778419495\n",
      "Iteration: 512. Loss: 0.25671452283859253\n",
      "Iteration: 513. Loss: 0.33842605352401733\n",
      "Iteration: 514. Loss: 0.24905413389205933\n",
      "Iteration: 515. Loss: 0.4580696225166321\n",
      "Iteration: 516. Loss: 0.2886700928211212\n",
      "Iteration: 517. Loss: 0.26437151432037354\n",
      "Iteration: 518. Loss: 0.3024227023124695\n",
      "Iteration: 519. Loss: 0.2569328248500824\n",
      "Iteration: 520. Loss: 0.363137423992157\n",
      "Iteration: 521. Loss: 0.20636774599552155\n",
      "Iteration: 522. Loss: 0.4474314749240875\n",
      "Iteration: 523. Loss: 0.34603703022003174\n",
      "Iteration: 524. Loss: 0.3099673390388489\n",
      "Iteration: 525. Loss: 0.4083975553512573\n",
      "Iteration: 526. Loss: 0.35275864601135254\n",
      "Iteration: 527. Loss: 0.22619694471359253\n",
      "Iteration: 528. Loss: 0.30944108963012695\n",
      "Iteration: 529. Loss: 0.3260423243045807\n",
      "Iteration: 530. Loss: 0.3697952330112457\n",
      "Iteration: 531. Loss: 0.28692999482154846\n",
      "Iteration: 532. Loss: 0.357130229473114\n",
      "Iteration: 533. Loss: 0.33633533120155334\n",
      "Iteration: 534. Loss: 0.30897223949432373\n",
      "Iteration: 535. Loss: 0.4333783984184265\n",
      "Iteration: 536. Loss: 0.3751543462276459\n",
      "Iteration: 537. Loss: 0.2594551742076874\n",
      "Iteration: 538. Loss: 0.27784356474876404\n",
      "Iteration: 539. Loss: 0.4220811426639557\n",
      "Iteration: 540. Loss: 0.3888837397098541\n",
      "Iteration: 541. Loss: 0.26792487502098083\n",
      "Iteration: 542. Loss: 0.39117133617401123\n",
      "Iteration: 543. Loss: 0.2972717583179474\n",
      "Iteration: 544. Loss: 0.2869996130466461\n",
      "Iteration: 545. Loss: 0.3654075562953949\n",
      "Iteration: 546. Loss: 0.28389155864715576\n",
      "Iteration: 547. Loss: 0.27238601446151733\n",
      "Iteration: 548. Loss: 0.24533066153526306\n",
      "Iteration: 549. Loss: 0.2435324341058731\n",
      "Iteration: 550. Loss: 0.40063679218292236\n",
      "Iteration: 551. Loss: 0.23754487931728363\n",
      "Iteration: 552. Loss: 0.2989453375339508\n",
      "Iteration: 553. Loss: 0.25730809569358826\n",
      "Iteration: 554. Loss: 0.19211557507514954\n",
      "Iteration: 555. Loss: 0.38740044832229614\n",
      "Iteration: 556. Loss: 0.2972586154937744\n",
      "Iteration: 557. Loss: 0.34387776255607605\n",
      "Iteration: 558. Loss: 0.3272554874420166\n",
      "Iteration: 559. Loss: 0.2822415828704834\n",
      "Iteration: 560. Loss: 0.278498113155365\n",
      "Iteration: 561. Loss: 0.45460471510887146\n",
      "Iteration: 562. Loss: 0.3517577648162842\n",
      "Iteration: 563. Loss: 0.22019976377487183\n",
      "Iteration: 564. Loss: 0.2428842931985855\n",
      "Iteration: 565. Loss: 0.2571468949317932\n",
      "Iteration: 566. Loss: 0.30643871426582336\n",
      "Iteration: 567. Loss: 0.3426385521888733\n",
      "Iteration: 568. Loss: 0.3666115999221802\n",
      "Iteration: 569. Loss: 0.23852676153182983\n",
      "Iteration: 570. Loss: 0.4731372594833374\n",
      "Iteration: 571. Loss: 0.31663069128990173\n",
      "Iteration: 572. Loss: 0.28264468908309937\n",
      "Iteration: 573. Loss: 0.20829498767852783\n",
      "Iteration: 574. Loss: 0.3087323009967804\n",
      "Iteration: 575. Loss: 0.2727513611316681\n",
      "Iteration: 576. Loss: 0.20112501084804535\n",
      "Iteration: 577. Loss: 0.3755764365196228\n",
      "Iteration: 578. Loss: 0.39466023445129395\n",
      "Iteration: 579. Loss: 0.3026547133922577\n",
      "Iteration: 580. Loss: 0.15079708397388458\n",
      "Iteration: 581. Loss: 0.39463669061660767\n",
      "Iteration: 582. Loss: 0.48763465881347656\n",
      "Iteration: 583. Loss: 0.21114614605903625\n",
      "Iteration: 584. Loss: 0.2205280363559723\n",
      "Iteration: 585. Loss: 0.3082675039768219\n",
      "Iteration: 586. Loss: 0.17784595489501953\n",
      "Iteration: 587. Loss: 0.3042457103729248\n",
      "Iteration: 588. Loss: 0.364493191242218\n",
      "Iteration: 589. Loss: 0.3004809617996216\n",
      "Iteration: 590. Loss: 0.23008757829666138\n",
      "Iteration: 591. Loss: 0.3401109576225281\n",
      "Iteration: 592. Loss: 0.46280544996261597\n",
      "Iteration: 593. Loss: 0.4088866412639618\n",
      "Iteration: 594. Loss: 0.4393001198768616\n",
      "Iteration: 595. Loss: 0.3457483947277069\n",
      "Iteration: 596. Loss: 0.19521956145763397\n",
      "Iteration: 597. Loss: 0.31326982378959656\n",
      "Iteration: 598. Loss: 0.18038518726825714\n",
      "Iteration: 599. Loss: 0.14759893715381622\n",
      "Iteration: 600. Loss: 0.4275279641151428\n",
      "Iteration: 601. Loss: 0.24537895619869232\n",
      "Iteration: 602. Loss: 0.3215479552745819\n",
      "Iteration: 603. Loss: 0.42787933349609375\n",
      "Iteration: 604. Loss: 0.2823936939239502\n",
      "Iteration: 605. Loss: 0.5386185050010681\n",
      "Iteration: 606. Loss: 0.3945704698562622\n",
      "Iteration: 607. Loss: 0.31297194957733154\n",
      "Iteration: 608. Loss: 0.18540048599243164\n",
      "Iteration: 609. Loss: 0.28377053141593933\n",
      "Iteration: 610. Loss: 0.30033236742019653\n",
      "Iteration: 611. Loss: 0.3037796914577484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 612. Loss: 0.16440154612064362\n",
      "Iteration: 613. Loss: 0.39137348532676697\n",
      "Iteration: 614. Loss: 0.3546876609325409\n",
      "Iteration: 615. Loss: 0.24832050502300262\n",
      "Iteration: 616. Loss: 0.3685187101364136\n",
      "Iteration: 617. Loss: 0.2993236780166626\n",
      "Iteration: 618. Loss: 0.2928968369960785\n",
      "Iteration: 619. Loss: 0.30072033405303955\n",
      "Iteration: 620. Loss: 0.2824627459049225\n",
      "Iteration: 621. Loss: 0.34662628173828125\n",
      "Iteration: 622. Loss: 0.3142697811126709\n",
      "Iteration: 623. Loss: 0.22728778421878815\n",
      "Iteration: 624. Loss: 0.4158063530921936\n",
      "Iteration: 625. Loss: 0.2628464996814728\n",
      "Iteration: 626. Loss: 0.3587667942047119\n",
      "Iteration: 627. Loss: 0.19639061391353607\n",
      "Iteration: 628. Loss: 0.3282468020915985\n",
      "Iteration: 629. Loss: 0.370172917842865\n",
      "Iteration: 630. Loss: 0.3250950872898102\n",
      "Iteration: 631. Loss: 0.23230987787246704\n",
      "Iteration: 632. Loss: 0.4244445860385895\n",
      "Iteration: 633. Loss: 0.24572056531906128\n",
      "Iteration: 634. Loss: 0.2186688929796219\n",
      "Iteration: 635. Loss: 0.2472248524427414\n",
      "Iteration: 636. Loss: 0.3003031611442566\n",
      "Iteration: 637. Loss: 0.2550148665904999\n",
      "Iteration: 638. Loss: 0.30731332302093506\n",
      "Iteration: 639. Loss: 0.31112125515937805\n",
      "Iteration: 640. Loss: 0.32197314500808716\n",
      "Iteration: 641. Loss: 0.44412916898727417\n",
      "Iteration: 642. Loss: 0.1722063273191452\n",
      "Iteration: 643. Loss: 0.2713344991207123\n",
      "Iteration: 644. Loss: 0.2545580565929413\n",
      "Iteration: 645. Loss: 0.4081222116947174\n",
      "Iteration: 646. Loss: 0.33073437213897705\n",
      "Iteration: 647. Loss: 0.32878467440605164\n",
      "Iteration: 648. Loss: 0.3985179662704468\n",
      "Iteration: 649. Loss: 0.3544023036956787\n",
      "Iteration: 650. Loss: 0.2205697000026703\n",
      "Iteration: 651. Loss: 0.2406165450811386\n",
      "Iteration: 652. Loss: 0.2571413516998291\n",
      "Iteration: 653. Loss: 0.23469434678554535\n",
      "Iteration: 654. Loss: 0.22087015211582184\n",
      "Iteration: 655. Loss: 0.3642897307872772\n",
      "Iteration: 656. Loss: 0.15223781764507294\n",
      "Iteration: 657. Loss: 0.295975923538208\n",
      "Iteration: 658. Loss: 0.3760066628456116\n",
      "Iteration: 659. Loss: 0.2820451259613037\n",
      "Iteration: 660. Loss: 0.22932548820972443\n",
      "Iteration: 661. Loss: 0.3436065912246704\n",
      "Iteration: 662. Loss: 0.3713172972202301\n",
      "Iteration: 663. Loss: 0.2784337103366852\n",
      "Iteration: 664. Loss: 0.41007691621780396\n",
      "Iteration: 665. Loss: 0.31556472182273865\n",
      "Iteration: 666. Loss: 0.24571210145950317\n",
      "Iteration: 667. Loss: 0.3862265646457672\n",
      "Iteration: 668. Loss: 0.23198844492435455\n",
      "Iteration: 669. Loss: 0.2421213537454605\n",
      "Iteration: 670. Loss: 0.20532716810703278\n",
      "Iteration: 671. Loss: 0.36160165071487427\n",
      "Iteration: 672. Loss: 0.33638396859169006\n",
      "Iteration: 673. Loss: 0.3044572174549103\n",
      "Iteration: 674. Loss: 0.24009235203266144\n",
      "Iteration: 675. Loss: 0.2147349864244461\n",
      "Iteration: 676. Loss: 0.22501970827579498\n",
      "Iteration: 677. Loss: 0.27667349576950073\n",
      "Iteration: 678. Loss: 0.36588969826698303\n",
      "Iteration: 679. Loss: 0.33105847239494324\n",
      "Iteration: 680. Loss: 0.3943818211555481\n",
      "Iteration: 681. Loss: 0.46258744597435\n",
      "Iteration: 682. Loss: 0.2963383197784424\n",
      "Iteration: 683. Loss: 0.29679083824157715\n",
      "Iteration: 684. Loss: 0.3161638379096985\n",
      "Iteration: 685. Loss: 0.346122145652771\n",
      "Iteration: 686. Loss: 0.22846096754074097\n",
      "Iteration: 687. Loss: 0.24274568259716034\n",
      "Iteration: 688. Loss: 0.2576887011528015\n",
      "Iteration: 689. Loss: 0.25029289722442627\n",
      "Iteration: 690. Loss: 0.2707659602165222\n",
      "Iteration: 691. Loss: 0.38607367873191833\n",
      "Iteration: 692. Loss: 0.2781565189361572\n",
      "Iteration: 693. Loss: 0.2759990990161896\n",
      "Iteration: 694. Loss: 0.2576269805431366\n",
      "Iteration: 695. Loss: 0.2751069962978363\n",
      "Iteration: 696. Loss: 0.20679764449596405\n",
      "Iteration: 697. Loss: 0.22595712542533875\n",
      "Iteration: 698. Loss: 0.2813338339328766\n",
      "Iteration: 699. Loss: 0.36004966497421265\n",
      "Iteration: 700. Loss: 0.30291470885276794\n",
      "Iteration: 701. Loss: 0.34793946146965027\n",
      "Iteration: 702. Loss: 0.27611327171325684\n",
      "Iteration: 703. Loss: 0.3922732472419739\n",
      "Iteration: 704. Loss: 0.2238086313009262\n",
      "Iteration: 705. Loss: 0.13605080544948578\n",
      "Iteration: 706. Loss: 0.32239535450935364\n",
      "Iteration: 707. Loss: 0.32666313648223877\n",
      "Iteration: 708. Loss: 0.3288084864616394\n",
      "Iteration: 709. Loss: 0.25545233488082886\n",
      "Iteration: 710. Loss: 0.3022405505180359\n",
      "Iteration: 711. Loss: 0.3695734143257141\n",
      "Iteration: 712. Loss: 0.22401605546474457\n",
      "Iteration: 713. Loss: 0.31544047594070435\n",
      "Iteration: 714. Loss: 0.32446709275245667\n",
      "Iteration: 715. Loss: 0.37307167053222656\n",
      "Iteration: 716. Loss: 0.27505335211753845\n",
      "Iteration: 717. Loss: 0.21295347809791565\n",
      "Iteration: 718. Loss: 0.1957925260066986\n",
      "Iteration: 719. Loss: 0.3017815351486206\n",
      "Iteration: 720. Loss: 0.22862569987773895\n",
      "Iteration: 721. Loss: 0.20472855865955353\n",
      "Iteration: 722. Loss: 0.21107882261276245\n",
      "Iteration: 723. Loss: 0.1883660852909088\n",
      "Iteration: 724. Loss: 0.20446574687957764\n",
      "Iteration: 725. Loss: 0.39818596839904785\n",
      "Iteration: 726. Loss: 0.16388988494873047\n",
      "Iteration: 727. Loss: 0.25821438431739807\n",
      "Iteration: 728. Loss: 0.2043461799621582\n",
      "Iteration: 729. Loss: 0.20917804539203644\n",
      "Iteration: 730. Loss: 0.30503636598587036\n",
      "Iteration: 731. Loss: 0.21741342544555664\n",
      "Iteration: 732. Loss: 0.2972373962402344\n",
      "Iteration: 733. Loss: 0.275490939617157\n",
      "Iteration: 734. Loss: 0.3032661974430084\n",
      "Iteration: 735. Loss: 0.17115455865859985\n",
      "Iteration: 736. Loss: 0.29403403401374817\n",
      "Iteration: 737. Loss: 0.3716275095939636\n",
      "Iteration: 738. Loss: 0.27200132608413696\n",
      "Iteration: 739. Loss: 0.25607287883758545\n",
      "Iteration: 740. Loss: 0.1805684119462967\n",
      "Iteration: 741. Loss: 0.20278242230415344\n",
      "Iteration: 742. Loss: 0.19579067826271057\n",
      "Iteration: 743. Loss: 0.2684842050075531\n",
      "Iteration: 744. Loss: 0.35634204745292664\n",
      "Iteration: 745. Loss: 0.3211601972579956\n",
      "Iteration: 746. Loss: 0.1364489048719406\n",
      "Iteration: 747. Loss: 0.35841110348701477\n",
      "Iteration: 748. Loss: 0.3314657509326935\n",
      "Iteration: 749. Loss: 0.2880004942417145\n",
      "Iteration: 750. Loss: 0.22745972871780396\n",
      "Iteration: 751. Loss: 0.26122212409973145\n",
      "Iteration: 752. Loss: 0.3137277066707611\n",
      "Iteration: 753. Loss: 0.5991928577423096\n",
      "Iteration: 754. Loss: 0.27331268787384033\n",
      "Iteration: 755. Loss: 0.39574065804481506\n",
      "Iteration: 756. Loss: 0.30728915333747864\n",
      "Iteration: 757. Loss: 0.2494761347770691\n",
      "Iteration: 758. Loss: 0.17995931208133698\n",
      "Iteration: 759. Loss: 0.1762388050556183\n",
      "Iteration: 760. Loss: 0.2244855761528015\n",
      "Iteration: 761. Loss: 0.3094397485256195\n",
      "Iteration: 762. Loss: 0.45787376165390015\n",
      "Iteration: 763. Loss: 0.19304729998111725\n",
      "Iteration: 764. Loss: 0.29047688841819763\n",
      "Iteration: 765. Loss: 0.4282383620738983\n",
      "Iteration: 766. Loss: 0.1380312144756317\n",
      "Iteration: 767. Loss: 0.21730150282382965\n",
      "Iteration: 768. Loss: 0.3157450556755066\n",
      "Iteration: 769. Loss: 0.18814249336719513\n",
      "Iteration: 770. Loss: 0.3052654564380646\n",
      "Iteration: 771. Loss: 0.36347347497940063\n",
      "Iteration: 772. Loss: 0.21380038559436798\n",
      "Iteration: 773. Loss: 0.25168874859809875\n",
      "Iteration: 774. Loss: 0.2182057946920395\n",
      "Iteration: 775. Loss: 0.23601575195789337\n",
      "Iteration: 776. Loss: 0.3009236454963684\n",
      "Iteration: 777. Loss: 0.2813473343849182\n",
      "Iteration: 778. Loss: 0.21066652238368988\n",
      "Iteration: 779. Loss: 0.3736129403114319\n",
      "Iteration: 780. Loss: 0.34888678789138794\n",
      "Iteration: 781. Loss: 0.22027045488357544\n",
      "Iteration: 782. Loss: 0.25959548354148865\n",
      "Iteration: 783. Loss: 0.21371017396450043\n",
      "Iteration: 784. Loss: 0.3521149456501007\n",
      "Iteration: 785. Loss: 0.3075391352176666\n",
      "Iteration: 786. Loss: 0.17406539618968964\n",
      "Iteration: 787. Loss: 0.16034521162509918\n",
      "Iteration: 788. Loss: 0.16859197616577148\n",
      "Iteration: 789. Loss: 0.323657751083374\n",
      "Iteration: 790. Loss: 0.3123162090778351\n",
      "Iteration: 791. Loss: 0.23724296689033508\n",
      "Iteration: 792. Loss: 0.33007508516311646\n",
      "Iteration: 793. Loss: 0.2557346522808075\n",
      "Iteration: 794. Loss: 0.2555859684944153\n",
      "Iteration: 795. Loss: 0.22195520997047424\n",
      "Iteration: 796. Loss: 0.34221866726875305\n",
      "Iteration: 797. Loss: 0.25557711720466614\n",
      "Iteration: 798. Loss: 0.24120517075061798\n",
      "Iteration: 799. Loss: 0.26845717430114746\n",
      "Iteration: 800. Loss: 0.19863618910312653\n",
      "Iteration: 801. Loss: 0.27082207798957825\n",
      "Iteration: 802. Loss: 0.24651505053043365\n",
      "Iteration: 803. Loss: 0.20454508066177368\n",
      "Iteration: 804. Loss: 0.1873970776796341\n",
      "Iteration: 805. Loss: 0.1725049465894699\n",
      "Iteration: 806. Loss: 0.4428359568119049\n",
      "Iteration: 807. Loss: 0.31704437732696533\n",
      "Iteration: 808. Loss: 0.3491085469722748\n",
      "Iteration: 809. Loss: 0.4400060772895813\n",
      "Iteration: 810. Loss: 0.3920823037624359\n",
      "Iteration: 811. Loss: 0.2171064168214798\n",
      "Iteration: 812. Loss: 0.2733791470527649\n",
      "Iteration: 813. Loss: 0.24898125231266022\n",
      "Iteration: 814. Loss: 0.1877838671207428\n",
      "Iteration: 815. Loss: 0.3077247142791748\n",
      "Iteration: 816. Loss: 0.2504144012928009\n",
      "Iteration: 817. Loss: 0.4288089871406555\n",
      "Iteration: 818. Loss: 0.3194902837276459\n",
      "Iteration: 819. Loss: 0.27995428442955017\n",
      "Iteration: 820. Loss: 0.2199055552482605\n",
      "Iteration: 821. Loss: 0.2223397046327591\n",
      "Iteration: 822. Loss: 0.11005467176437378\n",
      "Iteration: 823. Loss: 0.1480540931224823\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 824. Loss: 0.24659128487110138\n",
      "Iteration: 825. Loss: 0.4101550281047821\n",
      "Iteration: 826. Loss: 0.31281226873397827\n",
      "Iteration: 827. Loss: 0.27498817443847656\n",
      "Iteration: 828. Loss: 0.30572569370269775\n",
      "Iteration: 829. Loss: 0.14504849910736084\n",
      "Iteration: 830. Loss: 0.25713661313056946\n",
      "Iteration: 831. Loss: 0.26928916573524475\n",
      "Iteration: 832. Loss: 0.258341521024704\n",
      "Iteration: 833. Loss: 0.3603942096233368\n",
      "Iteration: 834. Loss: 0.3328808844089508\n",
      "Iteration: 835. Loss: 0.3000672459602356\n",
      "Iteration: 836. Loss: 0.25485312938690186\n",
      "Iteration: 837. Loss: 0.21895742416381836\n",
      "Iteration: 838. Loss: 0.20827454328536987\n",
      "Iteration: 839. Loss: 0.4009447395801544\n",
      "Iteration: 840. Loss: 0.17800608277320862\n",
      "Iteration: 841. Loss: 0.3095327913761139\n",
      "Iteration: 842. Loss: 0.19958843290805817\n",
      "Iteration: 843. Loss: 0.5074097514152527\n",
      "Iteration: 844. Loss: 0.40717679262161255\n",
      "Iteration: 845. Loss: 0.4204385280609131\n",
      "Iteration: 846. Loss: 0.2891896963119507\n",
      "Iteration: 847. Loss: 0.3133215606212616\n",
      "Iteration: 848. Loss: 0.23971979320049286\n",
      "Iteration: 849. Loss: 0.3286973237991333\n",
      "Iteration: 850. Loss: 0.23538441956043243\n",
      "Iteration: 851. Loss: 0.2421608567237854\n",
      "Iteration: 852. Loss: 0.2249293327331543\n",
      "Iteration: 853. Loss: 0.3001770079135895\n",
      "Iteration: 854. Loss: 0.26580557227134705\n",
      "Iteration: 855. Loss: 0.30632689595222473\n",
      "Iteration: 856. Loss: 0.22985081374645233\n",
      "Iteration: 857. Loss: 0.24527429044246674\n",
      "Iteration: 858. Loss: 0.2903604805469513\n",
      "Iteration: 859. Loss: 0.17007820308208466\n",
      "Iteration: 860. Loss: 0.25347045063972473\n",
      "Iteration: 861. Loss: 0.3300158977508545\n",
      "Iteration: 862. Loss: 0.17406851053237915\n",
      "Iteration: 863. Loss: 0.42919421195983887\n",
      "Iteration: 864. Loss: 0.31102538108825684\n",
      "Iteration: 865. Loss: 0.21609464287757874\n",
      "Iteration: 866. Loss: 0.12174630910158157\n",
      "Iteration: 867. Loss: 0.20686376094818115\n",
      "Iteration: 868. Loss: 0.3614570200443268\n",
      "Iteration: 869. Loss: 0.2489088773727417\n",
      "Iteration: 870. Loss: 0.29745277762413025\n",
      "Iteration: 871. Loss: 0.3394050896167755\n",
      "Iteration: 872. Loss: 0.15762031078338623\n",
      "Iteration: 873. Loss: 0.4059711694717407\n",
      "Iteration: 874. Loss: 0.2287152260541916\n",
      "Iteration: 875. Loss: 0.16089144349098206\n",
      "Iteration: 876. Loss: 0.3799867331981659\n",
      "Iteration: 877. Loss: 0.24567005038261414\n",
      "Iteration: 878. Loss: 0.25427713990211487\n",
      "Iteration: 879. Loss: 0.1867501586675644\n",
      "Iteration: 880. Loss: 0.2505873143672943\n",
      "Iteration: 881. Loss: 0.2933858335018158\n",
      "Iteration: 882. Loss: 0.19929777085781097\n",
      "Iteration: 883. Loss: 0.3679368495941162\n",
      "Iteration: 884. Loss: 0.2355797439813614\n",
      "Iteration: 885. Loss: 0.30905431509017944\n",
      "Iteration: 886. Loss: 0.27082133293151855\n",
      "Iteration: 887. Loss: 0.2764774560928345\n",
      "Iteration: 888. Loss: 0.28218093514442444\n",
      "Iteration: 889. Loss: 0.1994013637304306\n",
      "Iteration: 890. Loss: 0.19466370344161987\n",
      "Iteration: 891. Loss: 0.31869393587112427\n",
      "Iteration: 892. Loss: 0.22265176475048065\n",
      "Iteration: 893. Loss: 0.2762850522994995\n",
      "Iteration: 894. Loss: 0.2143130898475647\n",
      "Iteration: 895. Loss: 0.14119119942188263\n",
      "Iteration: 896. Loss: 0.25973644852638245\n",
      "Iteration: 897. Loss: 0.514962911605835\n",
      "Iteration: 898. Loss: 0.1842951774597168\n",
      "Iteration: 899. Loss: 0.2316470742225647\n",
      "Iteration: 900. Loss: 0.2082793116569519\n",
      "Iteration: 901. Loss: 0.3248726725578308\n",
      "Iteration: 902. Loss: 0.1894294023513794\n",
      "Iteration: 903. Loss: 0.28015372157096863\n",
      "Iteration: 904. Loss: 0.21742431819438934\n",
      "Iteration: 905. Loss: 0.15998651087284088\n",
      "Iteration: 906. Loss: 0.32438504695892334\n",
      "Iteration: 907. Loss: 0.20931556820869446\n",
      "Iteration: 908. Loss: 0.1867934614419937\n",
      "Iteration: 909. Loss: 0.22151239216327667\n",
      "Iteration: 910. Loss: 0.498819500207901\n",
      "Iteration: 911. Loss: 0.27663949131965637\n",
      "Iteration: 912. Loss: 0.25913533568382263\n",
      "Iteration: 913. Loss: 0.2597736716270447\n",
      "Iteration: 914. Loss: 0.22090516984462738\n",
      "Iteration: 915. Loss: 0.2352004200220108\n",
      "Iteration: 916. Loss: 0.29081639647483826\n",
      "Iteration: 917. Loss: 0.3842920958995819\n",
      "Iteration: 918. Loss: 0.3487195670604706\n",
      "Iteration: 919. Loss: 0.3099215626716614\n",
      "Iteration: 920. Loss: 0.42003586888313293\n",
      "Iteration: 921. Loss: 0.30266451835632324\n",
      "Iteration: 922. Loss: 0.2226814329624176\n",
      "Iteration: 923. Loss: 0.40455886721611023\n",
      "Iteration: 924. Loss: 0.31222325563430786\n",
      "Iteration: 925. Loss: 0.16172336041927338\n",
      "Iteration: 926. Loss: 0.4604761004447937\n",
      "Iteration: 927. Loss: 0.23257380723953247\n",
      "Iteration: 928. Loss: 0.48735281825065613\n",
      "Iteration: 929. Loss: 0.27002230286598206\n",
      "Iteration: 930. Loss: 0.36234748363494873\n",
      "Iteration: 931. Loss: 0.24145837128162384\n",
      "Iteration: 932. Loss: 0.18731068074703217\n",
      "Iteration: 933. Loss: 0.19542032480239868\n",
      "Iteration: 934. Loss: 0.22395378351211548\n",
      "Iteration: 935. Loss: 0.3051861822605133\n",
      "Iteration: 936. Loss: 0.18565645813941956\n",
      "Iteration: 937. Loss: 0.327642023563385\n",
      "Iteration: 938. Loss: 0.12401014566421509\n",
      "Iteration: 939. Loss: 0.22889724373817444\n",
      "Iteration: 940. Loss: 0.22976505756378174\n",
      "Iteration: 941. Loss: 0.27528440952301025\n",
      "Iteration: 942. Loss: 0.30856457352638245\n",
      "Iteration: 943. Loss: 0.20952333509922028\n",
      "Iteration: 944. Loss: 0.21086522936820984\n",
      "Iteration: 945. Loss: 0.324778288602829\n",
      "Iteration: 946. Loss: 0.16119913756847382\n",
      "Iteration: 947. Loss: 0.3089502453804016\n",
      "Iteration: 948. Loss: 0.21340212225914001\n",
      "Iteration: 949. Loss: 0.2486824244260788\n",
      "Iteration: 950. Loss: 0.28605926036834717\n",
      "Iteration: 951. Loss: 0.2840457856655121\n",
      "Iteration: 952. Loss: 0.33123520016670227\n",
      "Iteration: 953. Loss: 0.22989606857299805\n",
      "Iteration: 954. Loss: 0.22373399138450623\n",
      "Iteration: 955. Loss: 0.21992725133895874\n",
      "Iteration: 956. Loss: 0.3169563412666321\n",
      "Iteration: 957. Loss: 0.19511708617210388\n",
      "Iteration: 958. Loss: 0.2016056627035141\n",
      "Iteration: 959. Loss: 0.24859283864498138\n",
      "Iteration: 960. Loss: 0.2752789258956909\n",
      "Iteration: 961. Loss: 0.18916431069374084\n",
      "Iteration: 962. Loss: 0.1930549293756485\n",
      "Iteration: 963. Loss: 0.2031513750553131\n",
      "Iteration: 964. Loss: 0.3505595326423645\n",
      "Iteration: 965. Loss: 0.29996562004089355\n",
      "Iteration: 966. Loss: 0.29090362787246704\n",
      "Iteration: 967. Loss: 0.19878274202346802\n",
      "Iteration: 968. Loss: 0.33277589082717896\n",
      "Iteration: 969. Loss: 0.17985644936561584\n",
      "Iteration: 970. Loss: 0.18955259025096893\n",
      "Iteration: 971. Loss: 0.12127882987260818\n",
      "Iteration: 972. Loss: 0.3007878363132477\n",
      "Iteration: 973. Loss: 0.18031011521816254\n",
      "Iteration: 974. Loss: 0.34574347734451294\n",
      "Iteration: 975. Loss: 0.3476223051548004\n",
      "Iteration: 976. Loss: 0.36176812648773193\n",
      "Iteration: 977. Loss: 0.22623516619205475\n",
      "Iteration: 978. Loss: 0.23213191330432892\n",
      "Iteration: 979. Loss: 0.21382299065589905\n",
      "Iteration: 980. Loss: 0.13674551248550415\n",
      "Iteration: 981. Loss: 0.25519266724586487\n",
      "Iteration: 982. Loss: 0.21035200357437134\n",
      "Iteration: 983. Loss: 0.2327195703983307\n",
      "Iteration: 984. Loss: 0.31356948614120483\n",
      "Iteration: 985. Loss: 0.241084024310112\n",
      "Iteration: 986. Loss: 0.4261412024497986\n",
      "Iteration: 987. Loss: 0.3343419134616852\n",
      "Iteration: 988. Loss: 0.28191500902175903\n",
      "Iteration: 989. Loss: 0.18889734148979187\n",
      "Iteration: 990. Loss: 0.27070507407188416\n",
      "Iteration: 991. Loss: 0.2742809057235718\n",
      "Iteration: 992. Loss: 0.20398706197738647\n",
      "Iteration: 993. Loss: 0.20904532074928284\n",
      "Iteration: 994. Loss: 0.29967454075813293\n",
      "Iteration: 995. Loss: 0.21752358973026276\n",
      "Iteration: 996. Loss: 0.34818020462989807\n",
      "Iteration: 997. Loss: 0.1618463397026062\n",
      "Iteration: 998. Loss: 0.1682550609111786\n",
      "Iteration: 999. Loss: 0.13624148070812225\n",
      "Iteration: 1000. Loss: 0.19816389679908752\n",
      "Iteration: 1001. Loss: 0.3667185306549072\n",
      "Iteration: 1002. Loss: 0.23462648689746857\n",
      "Iteration: 1003. Loss: 0.30015289783477783\n",
      "Iteration: 1004. Loss: 0.1867203563451767\n",
      "Iteration: 1005. Loss: 0.25906336307525635\n",
      "Iteration: 1006. Loss: 0.4422217607498169\n",
      "Iteration: 1007. Loss: 0.35009539127349854\n",
      "Iteration: 1008. Loss: 0.11768396198749542\n",
      "Iteration: 1009. Loss: 0.3265824615955353\n",
      "Iteration: 1010. Loss: 0.3694080710411072\n",
      "Iteration: 1011. Loss: 0.2898801565170288\n",
      "Iteration: 1012. Loss: 0.2850015163421631\n",
      "Iteration: 1013. Loss: 0.28265753388404846\n",
      "Iteration: 1014. Loss: 0.19168655574321747\n",
      "Iteration: 1015. Loss: 0.18679916858673096\n",
      "Iteration: 1016. Loss: 0.1941753625869751\n",
      "Iteration: 1017. Loss: 0.2735826373100281\n",
      "Iteration: 1018. Loss: 0.2617220878601074\n",
      "Iteration: 1019. Loss: 0.30021369457244873\n",
      "Iteration: 1020. Loss: 0.26303020119667053\n",
      "Iteration: 1021. Loss: 0.309422105550766\n",
      "Iteration: 1022. Loss: 0.32445529103279114\n",
      "Iteration: 1023. Loss: 0.4419420659542084\n",
      "Iteration: 1024. Loss: 0.3627353310585022\n",
      "Iteration: 1025. Loss: 0.26070207357406616\n",
      "Iteration: 1026. Loss: 0.23582212626934052\n",
      "Iteration: 1027. Loss: 0.31978896260261536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1028. Loss: 0.37624451518058777\n",
      "Iteration: 1029. Loss: 0.29608094692230225\n",
      "Iteration: 1030. Loss: 0.17548829317092896\n",
      "Iteration: 1031. Loss: 0.21877221763134003\n",
      "Iteration: 1032. Loss: 0.2381265014410019\n",
      "Iteration: 1033. Loss: 0.35741522908210754\n",
      "Iteration: 1034. Loss: 0.15997014939785004\n",
      "Iteration: 1035. Loss: 0.28210222721099854\n",
      "Iteration: 1036. Loss: 0.3225540220737457\n",
      "Iteration: 1037. Loss: 0.45175105333328247\n",
      "Iteration: 1038. Loss: 0.33664214611053467\n",
      "Iteration: 1039. Loss: 0.15884919464588165\n",
      "Iteration: 1040. Loss: 0.3041130006313324\n",
      "Iteration: 1041. Loss: 0.22215613722801208\n",
      "Iteration: 1042. Loss: 0.39013031125068665\n",
      "Iteration: 1043. Loss: 0.22709445655345917\n",
      "Iteration: 1044. Loss: 0.18047964572906494\n",
      "Iteration: 1045. Loss: 0.26782429218292236\n",
      "Iteration: 1046. Loss: 0.2682103216648102\n",
      "Iteration: 1047. Loss: 0.26157423853874207\n",
      "Iteration: 1048. Loss: 0.23561958968639374\n",
      "Iteration: 1049. Loss: 0.26627910137176514\n",
      "Iteration: 1050. Loss: 0.29873332381248474\n",
      "Iteration: 1051. Loss: 0.2991694211959839\n",
      "Iteration: 1052. Loss: 0.166506826877594\n",
      "Iteration: 1053. Loss: 0.24974896013736725\n",
      "Iteration: 1054. Loss: 0.27090880274772644\n",
      "Iteration: 1055. Loss: 0.31394967436790466\n",
      "Iteration: 1056. Loss: 0.3321915864944458\n",
      "Iteration: 1057. Loss: 0.33245280385017395\n",
      "Iteration: 1058. Loss: 0.2713638246059418\n",
      "Iteration: 1059. Loss: 0.17879976332187653\n",
      "Iteration: 1060. Loss: 0.3386389911174774\n",
      "Iteration: 1061. Loss: 0.3252001106739044\n",
      "Iteration: 1062. Loss: 0.29012250900268555\n",
      "Iteration: 1063. Loss: 0.19643016159534454\n",
      "Iteration: 1064. Loss: 0.37993213534355164\n",
      "Iteration: 1065. Loss: 0.306794136762619\n",
      "Iteration: 1066. Loss: 0.29494425654411316\n",
      "Iteration: 1067. Loss: 0.1460719257593155\n",
      "Iteration: 1068. Loss: 0.2356562465429306\n",
      "Iteration: 1069. Loss: 0.22416584193706512\n",
      "Iteration: 1070. Loss: 0.21538229286670685\n",
      "Iteration: 1071. Loss: 0.12885552644729614\n",
      "Iteration: 1072. Loss: 0.39878004789352417\n",
      "Iteration: 1073. Loss: 0.13549832999706268\n",
      "Iteration: 1074. Loss: 0.1693456619977951\n",
      "Iteration: 1075. Loss: 0.32948005199432373\n",
      "Iteration: 1076. Loss: 0.24344909191131592\n",
      "Iteration: 1077. Loss: 0.23859092593193054\n",
      "Iteration: 1078. Loss: 0.4701385796070099\n",
      "Iteration: 1079. Loss: 0.3876010477542877\n",
      "Iteration: 1080. Loss: 0.23070783913135529\n",
      "Iteration: 1081. Loss: 0.1213451698422432\n",
      "Iteration: 1082. Loss: 0.18921108543872833\n",
      "Iteration: 1083. Loss: 0.18812771141529083\n",
      "Iteration: 1084. Loss: 0.21389518678188324\n",
      "Iteration: 1085. Loss: 0.3998844027519226\n",
      "Iteration: 1086. Loss: 0.2799205183982849\n",
      "Iteration: 1087. Loss: 0.2529447078704834\n",
      "Iteration: 1088. Loss: 0.321969598531723\n",
      "Iteration: 1089. Loss: 0.2722400724887848\n",
      "Iteration: 1090. Loss: 0.2553621530532837\n",
      "Iteration: 1091. Loss: 0.22157178819179535\n",
      "Iteration: 1092. Loss: 0.19575104117393494\n",
      "Iteration: 1093. Loss: 0.2163975089788437\n",
      "Iteration: 1094. Loss: 0.43275752663612366\n",
      "Iteration: 1095. Loss: 0.25052574276924133\n",
      "Iteration: 1096. Loss: 0.1503487229347229\n",
      "Iteration: 1097. Loss: 0.24996605515480042\n",
      "Iteration: 1098. Loss: 0.2978401482105255\n",
      "Iteration: 1099. Loss: 0.3485329747200012\n",
      "Iteration: 1100. Loss: 0.26433658599853516\n",
      "Iteration: 1101. Loss: 0.18288689851760864\n",
      "Iteration: 1102. Loss: 0.21671268343925476\n",
      "Iteration: 1103. Loss: 0.12291239947080612\n",
      "Iteration: 1104. Loss: 0.2976653575897217\n",
      "Iteration: 1105. Loss: 0.367096483707428\n",
      "Iteration: 1106. Loss: 0.12178605049848557\n",
      "Iteration: 1107. Loss: 0.21738047897815704\n",
      "Iteration: 1108. Loss: 0.193004310131073\n",
      "Iteration: 1109. Loss: 0.2908027768135071\n",
      "Iteration: 1110. Loss: 0.26238393783569336\n",
      "Iteration: 1111. Loss: 0.23493996262550354\n",
      "Iteration: 1112. Loss: 0.3179754912853241\n",
      "Iteration: 1113. Loss: 0.193802148103714\n",
      "Iteration: 1114. Loss: 0.3412739038467407\n",
      "Iteration: 1115. Loss: 0.20100244879722595\n",
      "Iteration: 1116. Loss: 0.2726398706436157\n",
      "Iteration: 1117. Loss: 0.19598141312599182\n",
      "Iteration: 1118. Loss: 0.33328017592430115\n",
      "Iteration: 1119. Loss: 0.2529403567314148\n",
      "Iteration: 1120. Loss: 0.20192348957061768\n",
      "Iteration: 1121. Loss: 0.2159593552350998\n",
      "Iteration: 1122. Loss: 0.3905234932899475\n",
      "Iteration: 1123. Loss: 0.2227632850408554\n",
      "Iteration: 1124. Loss: 0.14149245619773865\n",
      "Iteration: 1125. Loss: 0.2724592685699463\n",
      "Iteration: 1126. Loss: 0.3461766541004181\n",
      "Iteration: 1127. Loss: 0.3091203272342682\n",
      "Iteration: 1128. Loss: 0.2377462238073349\n",
      "Iteration: 1129. Loss: 0.3305372893810272\n",
      "Iteration: 1130. Loss: 0.1489572674036026\n",
      "Iteration: 1131. Loss: 0.20142336189746857\n",
      "Iteration: 1132. Loss: 0.26023975014686584\n",
      "Iteration: 1133. Loss: 0.17920328676700592\n",
      "Iteration: 1134. Loss: 0.2597888112068176\n",
      "Iteration: 1135. Loss: 0.22091123461723328\n",
      "Iteration: 1136. Loss: 0.2788446545600891\n",
      "Iteration: 1137. Loss: 0.15695789456367493\n",
      "Iteration: 1138. Loss: 0.24938176572322845\n",
      "Iteration: 1139. Loss: 0.21256786584854126\n",
      "Iteration: 1140. Loss: 0.24584327638149261\n",
      "Iteration: 1141. Loss: 0.3785243332386017\n",
      "Iteration: 1142. Loss: 0.1919384002685547\n",
      "Iteration: 1143. Loss: 0.1653296798467636\n",
      "Iteration: 1144. Loss: 0.28888624906539917\n",
      "Iteration: 1145. Loss: 0.3247819244861603\n",
      "Iteration: 1146. Loss: 0.24984173476696014\n",
      "Iteration: 1147. Loss: 0.1899167001247406\n",
      "Iteration: 1148. Loss: 0.20104056596755981\n",
      "Iteration: 1149. Loss: 0.22174383699893951\n",
      "Iteration: 1150. Loss: 0.25546085834503174\n",
      "Iteration: 1151. Loss: 0.22782790660858154\n",
      "Iteration: 1152. Loss: 0.17081934213638306\n",
      "Iteration: 1153. Loss: 0.27801448106765747\n",
      "Iteration: 1154. Loss: 0.16637209057807922\n",
      "Iteration: 1155. Loss: 0.1924544721841812\n",
      "Iteration: 1156. Loss: 0.24210359156131744\n",
      "Iteration: 1157. Loss: 0.42916324734687805\n",
      "Iteration: 1158. Loss: 0.27603861689567566\n",
      "Iteration: 1159. Loss: 0.3609341084957123\n",
      "Iteration: 1160. Loss: 0.1481747031211853\n",
      "Iteration: 1161. Loss: 0.22235120832920074\n",
      "Iteration: 1162. Loss: 0.3557242155075073\n",
      "Iteration: 1163. Loss: 0.2091408520936966\n",
      "Iteration: 1164. Loss: 0.16330333054065704\n",
      "Iteration: 1165. Loss: 0.15266135334968567\n",
      "Iteration: 1166. Loss: 0.15096066892147064\n",
      "Iteration: 1167. Loss: 0.25103241205215454\n",
      "Iteration: 1168. Loss: 0.39855802059173584\n",
      "Iteration: 1169. Loss: 0.20807428658008575\n",
      "Iteration: 1170. Loss: 0.3392820656299591\n",
      "Iteration: 1171. Loss: 0.18585187196731567\n",
      "Iteration: 1172. Loss: 0.1698918342590332\n",
      "Iteration: 1173. Loss: 0.26752665638923645\n",
      "Iteration: 1174. Loss: 0.14275076985359192\n",
      "Iteration: 1175. Loss: 0.29423901438713074\n",
      "Iteration: 1176. Loss: 0.3088608384132385\n",
      "Iteration: 1177. Loss: 0.14319413900375366\n",
      "Iteration: 1178. Loss: 0.449146568775177\n",
      "Iteration: 1179. Loss: 0.1347515732049942\n",
      "Iteration: 1180. Loss: 0.18013539910316467\n",
      "Iteration: 1181. Loss: 0.24272580444812775\n",
      "Iteration: 1182. Loss: 0.4411619305610657\n",
      "Iteration: 1183. Loss: 0.21001729369163513\n",
      "Iteration: 1184. Loss: 0.2721545100212097\n",
      "Iteration: 1185. Loss: 0.17563265562057495\n",
      "Iteration: 1186. Loss: 0.15169189870357513\n",
      "Iteration: 1187. Loss: 0.2723008990287781\n",
      "Iteration: 1188. Loss: 0.2022068202495575\n",
      "Iteration: 1189. Loss: 0.21792247891426086\n",
      "Iteration: 1190. Loss: 0.42674967646598816\n",
      "Iteration: 1191. Loss: 0.2020246386528015\n",
      "Iteration: 1192. Loss: 0.22532960772514343\n",
      "Iteration: 1193. Loss: 0.2464996874332428\n",
      "Iteration: 1194. Loss: 0.28483855724334717\n",
      "Iteration: 1195. Loss: 0.25721389055252075\n",
      "Iteration: 1196. Loss: 0.45653286576271057\n",
      "Iteration: 1197. Loss: 0.21731984615325928\n",
      "Iteration: 1198. Loss: 0.15824024379253387\n",
      "Iteration: 1199. Loss: 0.27828845381736755\n",
      "Iteration: 1200. Loss: 0.23739591240882874\n",
      "Iteration: 1201. Loss: 0.17717069387435913\n",
      "Iteration: 1202. Loss: 0.17039476335048676\n",
      "Iteration: 1203. Loss: 0.4134374260902405\n",
      "Iteration: 1204. Loss: 0.21215161681175232\n",
      "Iteration: 1205. Loss: 0.26301711797714233\n",
      "Iteration: 1206. Loss: 0.3097875714302063\n",
      "Iteration: 1207. Loss: 0.14430679380893707\n",
      "Iteration: 1208. Loss: 0.22131142020225525\n",
      "Iteration: 1209. Loss: 0.19608965516090393\n",
      "Iteration: 1210. Loss: 0.24389831721782684\n",
      "Iteration: 1211. Loss: 0.1780964434146881\n",
      "Iteration: 1212. Loss: 0.31936272978782654\n",
      "Iteration: 1213. Loss: 0.2959231436252594\n",
      "Iteration: 1214. Loss: 0.19827473163604736\n",
      "Iteration: 1215. Loss: 0.320883572101593\n",
      "Iteration: 1216. Loss: 0.25282910466194153\n",
      "Iteration: 1217. Loss: 0.2702753245830536\n",
      "Iteration: 1218. Loss: 0.25142720341682434\n",
      "Iteration: 1219. Loss: 0.20109251141548157\n",
      "Iteration: 1220. Loss: 0.149091437458992\n",
      "Iteration: 1221. Loss: 0.23981927335262299\n",
      "Iteration: 1222. Loss: 0.1907089352607727\n",
      "Iteration: 1223. Loss: 0.30497458577156067\n",
      "Iteration: 1224. Loss: 0.19688518345355988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1225. Loss: 0.3377651274204254\n",
      "Iteration: 1226. Loss: 0.1510055661201477\n",
      "Iteration: 1227. Loss: 0.13750514388084412\n",
      "Iteration: 1228. Loss: 0.24227018654346466\n",
      "Iteration: 1229. Loss: 0.20090751349925995\n",
      "Iteration: 1230. Loss: 0.3429960608482361\n",
      "Iteration: 1231. Loss: 0.22574740648269653\n",
      "Iteration: 1232. Loss: 0.18681031465530396\n",
      "Iteration: 1233. Loss: 0.14235299825668335\n",
      "Iteration: 1234. Loss: 0.17668873071670532\n",
      "Iteration: 1235. Loss: 0.37574100494384766\n",
      "Iteration: 1236. Loss: 0.3932279348373413\n",
      "Iteration: 1237. Loss: 0.2977563142776489\n",
      "Iteration: 1238. Loss: 0.3750622868537903\n",
      "Iteration: 1239. Loss: 0.15460847318172455\n",
      "Iteration: 1240. Loss: 0.18708282709121704\n",
      "Iteration: 1241. Loss: 0.1341070979833603\n",
      "Iteration: 1242. Loss: 0.2085917890071869\n",
      "Iteration: 1243. Loss: 0.37515684962272644\n",
      "Iteration: 1244. Loss: 0.24393826723098755\n",
      "Iteration: 1245. Loss: 0.4149033725261688\n",
      "Iteration: 1246. Loss: 0.1494196057319641\n",
      "Iteration: 1247. Loss: 0.23583900928497314\n",
      "Iteration: 1248. Loss: 0.23651427030563354\n",
      "Iteration: 1249. Loss: 0.1438681185245514\n",
      "Iteration: 1250. Loss: 0.150549054145813\n",
      "Iteration: 1251. Loss: 0.237568199634552\n",
      "Iteration: 1252. Loss: 0.23045457899570465\n",
      "Iteration: 1253. Loss: 0.24859938025474548\n",
      "Iteration: 1254. Loss: 0.16703055799007416\n",
      "Iteration: 1255. Loss: 0.22193436324596405\n",
      "Iteration: 1256. Loss: 0.2025933861732483\n",
      "Iteration: 1257. Loss: 0.2602534294128418\n",
      "Iteration: 1258. Loss: 0.19539587199687958\n",
      "Iteration: 1259. Loss: 0.3214806020259857\n",
      "Iteration: 1260. Loss: 0.26685938239097595\n",
      "Iteration: 1261. Loss: 0.3122066557407379\n",
      "Iteration: 1262. Loss: 0.2295084446668625\n",
      "Iteration: 1263. Loss: 0.21261648833751678\n",
      "Iteration: 1264. Loss: 0.29578065872192383\n",
      "Iteration: 1265. Loss: 0.3155059218406677\n",
      "Iteration: 1266. Loss: 0.33061400055885315\n",
      "Iteration: 1267. Loss: 0.1813437044620514\n",
      "Iteration: 1268. Loss: 0.33205512166023254\n",
      "Iteration: 1269. Loss: 0.2912687063217163\n",
      "Iteration: 1270. Loss: 0.31284910440444946\n",
      "Iteration: 1271. Loss: 0.1938617080450058\n",
      "Iteration: 1272. Loss: 0.24925445020198822\n",
      "Iteration: 1273. Loss: 0.1986882984638214\n",
      "Iteration: 1274. Loss: 0.25344008207321167\n",
      "Iteration: 1275. Loss: 0.15760469436645508\n",
      "Iteration: 1276. Loss: 0.28083956241607666\n",
      "Iteration: 1277. Loss: 0.2011421173810959\n",
      "Iteration: 1278. Loss: 0.2291986495256424\n",
      "Iteration: 1279. Loss: 0.19264860451221466\n",
      "Iteration: 1280. Loss: 0.3007727265357971\n",
      "Iteration: 1281. Loss: 0.13093136250972748\n",
      "Iteration: 1282. Loss: 0.23842009902000427\n",
      "Iteration: 1283. Loss: 0.19002555310726166\n",
      "Iteration: 1284. Loss: 0.16471242904663086\n",
      "Iteration: 1285. Loss: 0.14839482307434082\n",
      "Iteration: 1286. Loss: 0.23232501745224\n",
      "Iteration: 1287. Loss: 0.21908551454544067\n",
      "Iteration: 1288. Loss: 0.3475484848022461\n",
      "Iteration: 1289. Loss: 0.3162055015563965\n",
      "Iteration: 1290. Loss: 0.24624179303646088\n",
      "Iteration: 1291. Loss: 0.18325302004814148\n",
      "Iteration: 1292. Loss: 0.1963987797498703\n",
      "Iteration: 1293. Loss: 0.23992733657360077\n",
      "Iteration: 1294. Loss: 0.21070492267608643\n",
      "Iteration: 1295. Loss: 0.22372493147850037\n",
      "Iteration: 1296. Loss: 0.20079515874385834\n",
      "Iteration: 1297. Loss: 0.16315850615501404\n",
      "Iteration: 1298. Loss: 0.31871017813682556\n",
      "Iteration: 1299. Loss: 0.23999269306659698\n",
      "Iteration: 1300. Loss: 0.16943150758743286\n",
      "Iteration: 1301. Loss: 0.12093517929315567\n",
      "Iteration: 1302. Loss: 0.17816829681396484\n",
      "Iteration: 1303. Loss: 0.35958099365234375\n",
      "Iteration: 1304. Loss: 0.20440034568309784\n",
      "Iteration: 1305. Loss: 0.1536938101053238\n",
      "Iteration: 1306. Loss: 0.3490215539932251\n",
      "Iteration: 1307. Loss: 0.4011955261230469\n",
      "Iteration: 1308. Loss: 0.30296143889427185\n",
      "Iteration: 1309. Loss: 0.19549892842769623\n",
      "Iteration: 1310. Loss: 0.21136118471622467\n",
      "Iteration: 1311. Loss: 0.22325953841209412\n",
      "Iteration: 1312. Loss: 0.09998705983161926\n",
      "Iteration: 1313. Loss: 0.29882338643074036\n",
      "Iteration: 1314. Loss: 0.3033182919025421\n",
      "Iteration: 1315. Loss: 0.2233010083436966\n",
      "Iteration: 1316. Loss: 0.2676408290863037\n",
      "Iteration: 1317. Loss: 0.18003840744495392\n",
      "Iteration: 1318. Loss: 0.19765491783618927\n",
      "Iteration: 1319. Loss: 0.1854122132062912\n",
      "Iteration: 1320. Loss: 0.21366886794567108\n",
      "Iteration: 1321. Loss: 0.20832669734954834\n",
      "Iteration: 1322. Loss: 0.23769669234752655\n",
      "Iteration: 1323. Loss: 0.11353430151939392\n",
      "Iteration: 1324. Loss: 0.2612948417663574\n",
      "Iteration: 1325. Loss: 0.11205149441957474\n",
      "Iteration: 1326. Loss: 0.38416409492492676\n",
      "Iteration: 1327. Loss: 0.17910726368427277\n",
      "Iteration: 1328. Loss: 0.19116243720054626\n",
      "Iteration: 1329. Loss: 0.28503337502479553\n",
      "Iteration: 1330. Loss: 0.23982077836990356\n",
      "Iteration: 1331. Loss: 0.1402841955423355\n",
      "Iteration: 1332. Loss: 0.3587176501750946\n",
      "Iteration: 1333. Loss: 0.12259580940008163\n",
      "Iteration: 1334. Loss: 0.2867646813392639\n",
      "Iteration: 1335. Loss: 0.15213458240032196\n",
      "Iteration: 1336. Loss: 0.10682648420333862\n",
      "Iteration: 1337. Loss: 0.12368083000183105\n",
      "Iteration: 1338. Loss: 0.1674976795911789\n",
      "Iteration: 1339. Loss: 0.16357654333114624\n",
      "Iteration: 1340. Loss: 0.2733823359012604\n",
      "Iteration: 1341. Loss: 0.41149696707725525\n",
      "Iteration: 1342. Loss: 0.17167003452777863\n",
      "Iteration: 1343. Loss: 0.0733586922287941\n",
      "Iteration: 1344. Loss: 0.24338039755821228\n",
      "Iteration: 1345. Loss: 0.16843561828136444\n",
      "Iteration: 1346. Loss: 0.25342896580696106\n",
      "Iteration: 1347. Loss: 0.3816472291946411\n",
      "Iteration: 1348. Loss: 0.2929452359676361\n",
      "Iteration: 1349. Loss: 0.279127836227417\n",
      "Iteration: 1350. Loss: 0.18149101734161377\n",
      "Iteration: 1351. Loss: 0.23867402970790863\n",
      "Iteration: 1352. Loss: 0.18869629502296448\n",
      "Iteration: 1353. Loss: 0.3523026704788208\n",
      "Iteration: 1354. Loss: 0.21062150597572327\n",
      "Iteration: 1355. Loss: 0.16400690376758575\n",
      "Iteration: 1356. Loss: 0.20227032899856567\n",
      "Iteration: 1357. Loss: 0.3460998833179474\n",
      "Iteration: 1358. Loss: 0.13970333337783813\n",
      "Iteration: 1359. Loss: 0.22639574110507965\n",
      "Iteration: 1360. Loss: 0.16770249605178833\n",
      "Iteration: 1361. Loss: 0.15187697112560272\n",
      "Iteration: 1362. Loss: 0.22964470088481903\n",
      "Iteration: 1363. Loss: 0.21867747604846954\n",
      "Iteration: 1364. Loss: 0.3521801233291626\n",
      "Iteration: 1365. Loss: 0.19066183269023895\n",
      "Iteration: 1366. Loss: 0.316145122051239\n",
      "Iteration: 1367. Loss: 0.33758220076560974\n",
      "Iteration: 1368. Loss: 0.2530938982963562\n",
      "Iteration: 1369. Loss: 0.21280226111412048\n",
      "Iteration: 1370. Loss: 0.1427341103553772\n",
      "Iteration: 1371. Loss: 0.146590456366539\n",
      "Iteration: 1372. Loss: 0.22763468325138092\n",
      "Iteration: 1373. Loss: 0.2639314532279968\n",
      "Iteration: 1374. Loss: 0.15974658727645874\n",
      "Iteration: 1375. Loss: 0.21172890067100525\n",
      "Iteration: 1376. Loss: 0.18537399172782898\n",
      "Iteration: 1377. Loss: 0.24260303378105164\n",
      "Iteration: 1378. Loss: 0.2138436883687973\n",
      "Iteration: 1379. Loss: 0.20418275892734528\n",
      "Iteration: 1380. Loss: 0.22790025174617767\n",
      "Iteration: 1381. Loss: 0.15625980496406555\n",
      "Iteration: 1382. Loss: 0.18922168016433716\n",
      "Iteration: 1383. Loss: 0.2099279910326004\n",
      "Iteration: 1384. Loss: 0.14321431517601013\n",
      "Iteration: 1385. Loss: 0.1985541582107544\n",
      "Iteration: 1386. Loss: 0.3006151020526886\n",
      "Iteration: 1387. Loss: 0.2519244849681854\n",
      "Iteration: 1388. Loss: 0.205756276845932\n",
      "Iteration: 1389. Loss: 0.27656468749046326\n",
      "Iteration: 1390. Loss: 0.2930014133453369\n",
      "Iteration: 1391. Loss: 0.27131062746047974\n",
      "Iteration: 1392. Loss: 0.24765227735042572\n",
      "Iteration: 1393. Loss: 0.17165954411029816\n",
      "Iteration: 1394. Loss: 0.30439844727516174\n",
      "Iteration: 1395. Loss: 0.20020608603954315\n",
      "Iteration: 1396. Loss: 0.22550611197948456\n",
      "Iteration: 1397. Loss: 0.29938626289367676\n",
      "Iteration: 1398. Loss: 0.1537572294473648\n",
      "Iteration: 1399. Loss: 0.1300797313451767\n",
      "Iteration: 1400. Loss: 0.17859262228012085\n",
      "Iteration: 1401. Loss: 0.2541700005531311\n",
      "Iteration: 1402. Loss: 0.21664898097515106\n",
      "Iteration: 1403. Loss: 0.20526431500911713\n",
      "Iteration: 1404. Loss: 0.18551014363765717\n",
      "Iteration: 1405. Loss: 0.17062391340732574\n",
      "Iteration: 1406. Loss: 0.15983398258686066\n",
      "Iteration: 1407. Loss: 0.2115515172481537\n",
      "Iteration: 1408. Loss: 0.11984306573867798\n",
      "Iteration: 1409. Loss: 0.33327293395996094\n",
      "Iteration: 1410. Loss: 0.15963540971279144\n",
      "Iteration: 1411. Loss: 0.35412055253982544\n",
      "Iteration: 1412. Loss: 0.21910279989242554\n",
      "Iteration: 1413. Loss: 0.46071773767471313\n",
      "Iteration: 1414. Loss: 0.16821520030498505\n",
      "Iteration: 1415. Loss: 0.14942200481891632\n",
      "Iteration: 1416. Loss: 0.3419211208820343\n",
      "Iteration: 1417. Loss: 0.1684156209230423\n",
      "Iteration: 1418. Loss: 0.1273467242717743\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1419. Loss: 0.20645734667778015\n",
      "Iteration: 1420. Loss: 0.12919531762599945\n",
      "Iteration: 1421. Loss: 0.3289169669151306\n",
      "Iteration: 1422. Loss: 0.18347862362861633\n",
      "Iteration: 1423. Loss: 0.2419375479221344\n",
      "Iteration: 1424. Loss: 0.20907045900821686\n",
      "Iteration: 1425. Loss: 0.28089961409568787\n",
      "Iteration: 1426. Loss: 0.28641271591186523\n",
      "Iteration: 1427. Loss: 0.3515685200691223\n",
      "Iteration: 1428. Loss: 0.2067272961139679\n",
      "Iteration: 1429. Loss: 0.1501569300889969\n",
      "Iteration: 1430. Loss: 0.19468531012535095\n",
      "Iteration: 1431. Loss: 0.30890700221061707\n",
      "Iteration: 1432. Loss: 0.21185246109962463\n",
      "Iteration: 1433. Loss: 0.18643276393413544\n",
      "Iteration: 1434. Loss: 0.3772789239883423\n",
      "Iteration: 1435. Loss: 0.13562460243701935\n",
      "Iteration: 1436. Loss: 0.2890397608280182\n",
      "Iteration: 1437. Loss: 0.2752493619918823\n",
      "Iteration: 1438. Loss: 0.1537800133228302\n",
      "Iteration: 1439. Loss: 0.22531431913375854\n",
      "Iteration: 1440. Loss: 0.20611946284770966\n",
      "Iteration: 1441. Loss: 0.1690521091222763\n",
      "Iteration: 1442. Loss: 0.19694070518016815\n",
      "Iteration: 1443. Loss: 0.2651943266391754\n",
      "Iteration: 1444. Loss: 0.31318047642707825\n",
      "Iteration: 1445. Loss: 0.1949344426393509\n",
      "Iteration: 1446. Loss: 0.09662498533725739\n",
      "Iteration: 1447. Loss: 0.16955548524856567\n",
      "Iteration: 1448. Loss: 0.14205396175384521\n",
      "Iteration: 1449. Loss: 0.16499429941177368\n",
      "Iteration: 1450. Loss: 0.19246099889278412\n",
      "Iteration: 1451. Loss: 0.24349074065685272\n",
      "Iteration: 1452. Loss: 0.29742270708084106\n",
      "Iteration: 1453. Loss: 0.13725614547729492\n",
      "Iteration: 1454. Loss: 0.23204556107521057\n",
      "Iteration: 1455. Loss: 0.24377644062042236\n",
      "Iteration: 1456. Loss: 0.18442392349243164\n",
      "Iteration: 1457. Loss: 0.13223400712013245\n",
      "Iteration: 1458. Loss: 0.2724478840827942\n",
      "Iteration: 1459. Loss: 0.17048323154449463\n",
      "Iteration: 1460. Loss: 0.11293957382440567\n",
      "Iteration: 1461. Loss: 0.2925243675708771\n",
      "Iteration: 1462. Loss: 0.17716237902641296\n",
      "Iteration: 1463. Loss: 0.09899681061506271\n",
      "Iteration: 1464. Loss: 0.2957330048084259\n",
      "Iteration: 1465. Loss: 0.17495210468769073\n",
      "Iteration: 1466. Loss: 0.18029388785362244\n",
      "Iteration: 1467. Loss: 0.24524281919002533\n",
      "Iteration: 1468. Loss: 0.17614459991455078\n",
      "Iteration: 1469. Loss: 0.1502947360277176\n",
      "Iteration: 1470. Loss: 0.3228241205215454\n",
      "Iteration: 1471. Loss: 0.22034868597984314\n",
      "Iteration: 1472. Loss: 0.28277599811553955\n",
      "Iteration: 1473. Loss: 0.43741875886917114\n",
      "Iteration: 1474. Loss: 0.14475323259830475\n",
      "Iteration: 1475. Loss: 0.27285662293434143\n",
      "Iteration: 1476. Loss: 0.2454659640789032\n",
      "Iteration: 1477. Loss: 0.30278387665748596\n",
      "Iteration: 1478. Loss: 0.32958984375\n",
      "Iteration: 1479. Loss: 0.11097098141908646\n",
      "Iteration: 1480. Loss: 0.20210881531238556\n",
      "Iteration: 1481. Loss: 0.220036119222641\n",
      "Iteration: 1482. Loss: 0.17209196090698242\n",
      "Iteration: 1483. Loss: 0.22094613313674927\n",
      "Iteration: 1484. Loss: 0.10014905035495758\n",
      "Iteration: 1485. Loss: 0.19093313813209534\n",
      "Iteration: 1486. Loss: 0.33516913652420044\n",
      "Iteration: 1487. Loss: 0.20115166902542114\n",
      "Iteration: 1488. Loss: 0.21986806392669678\n",
      "Iteration: 1489. Loss: 0.0973326787352562\n",
      "Iteration: 1490. Loss: 0.26371046900749207\n",
      "Iteration: 1491. Loss: 0.28532448410987854\n",
      "Iteration: 1492. Loss: 0.23197758197784424\n",
      "Iteration: 1493. Loss: 0.2165648341178894\n",
      "Iteration: 1494. Loss: 0.25125718116760254\n",
      "Iteration: 1495. Loss: 0.2277332842350006\n",
      "Iteration: 1496. Loss: 0.3185400664806366\n",
      "Iteration: 1497. Loss: 0.16357798874378204\n",
      "Iteration: 1498. Loss: 0.2995738387107849\n",
      "Iteration: 1499. Loss: 0.148920476436615\n",
      "Iteration: 1500. Loss: 0.2856815457344055\n",
      "Iteration: 1501. Loss: 0.20985883474349976\n",
      "Iteration: 1502. Loss: 0.29164963960647583\n",
      "Iteration: 1503. Loss: 0.25787806510925293\n",
      "Iteration: 1504. Loss: 0.16750074923038483\n",
      "Iteration: 1505. Loss: 0.12413118034601212\n",
      "Iteration: 1506. Loss: 0.2784806489944458\n",
      "Iteration: 1507. Loss: 0.11916092783212662\n",
      "Iteration: 1508. Loss: 0.17492498457431793\n",
      "Iteration: 1509. Loss: 0.15686629712581635\n",
      "Iteration: 1510. Loss: 0.21676920354366302\n",
      "Iteration: 1511. Loss: 0.3868280053138733\n",
      "Iteration: 1512. Loss: 0.24248410761356354\n",
      "Iteration: 1513. Loss: 0.27539947628974915\n",
      "Iteration: 1514. Loss: 0.17876319587230682\n",
      "Iteration: 1515. Loss: 0.30103522539138794\n",
      "Iteration: 1516. Loss: 0.20973709225654602\n",
      "Iteration: 1517. Loss: 0.18764366209506989\n",
      "Iteration: 1518. Loss: 0.28326866030693054\n",
      "Iteration: 1519. Loss: 0.1624453365802765\n",
      "Iteration: 1520. Loss: 0.27630192041397095\n",
      "Iteration: 1521. Loss: 0.3243757486343384\n",
      "Iteration: 1522. Loss: 0.19387608766555786\n",
      "Iteration: 1523. Loss: 0.18184755742549896\n",
      "Iteration: 1524. Loss: 0.31710806488990784\n",
      "Iteration: 1525. Loss: 0.29433074593544006\n",
      "Iteration: 1526. Loss: 0.1758158802986145\n",
      "Iteration: 1527. Loss: 0.14799918234348297\n",
      "Iteration: 1528. Loss: 0.22909905016422272\n",
      "Iteration: 1529. Loss: 0.32352909445762634\n",
      "Iteration: 1530. Loss: 0.19097642600536346\n",
      "Iteration: 1531. Loss: 0.10304666310548782\n",
      "Iteration: 1532. Loss: 0.21211139857769012\n",
      "Iteration: 1533. Loss: 0.07938270270824432\n",
      "Iteration: 1534. Loss: 0.17527920007705688\n",
      "Iteration: 1535. Loss: 0.33886000514030457\n",
      "Iteration: 1536. Loss: 0.19402764737606049\n",
      "Iteration: 1537. Loss: 0.28034088015556335\n",
      "Iteration: 1538. Loss: 0.14886704087257385\n",
      "Iteration: 1539. Loss: 0.15130001306533813\n",
      "Iteration: 1540. Loss: 0.23634162545204163\n",
      "Iteration: 1541. Loss: 0.11727160215377808\n",
      "Iteration: 1542. Loss: 0.16920484602451324\n",
      "Iteration: 1543. Loss: 0.16404709219932556\n",
      "Iteration: 1544. Loss: 0.22832824289798737\n",
      "Iteration: 1545. Loss: 0.18589678406715393\n",
      "Iteration: 1546. Loss: 0.31673160195350647\n",
      "Iteration: 1547. Loss: 0.22161702811717987\n",
      "Iteration: 1548. Loss: 0.1117614358663559\n",
      "Iteration: 1549. Loss: 0.17450931668281555\n",
      "Iteration: 1550. Loss: 0.3666929304599762\n",
      "Iteration: 1551. Loss: 0.17432862520217896\n",
      "Iteration: 1552. Loss: 0.2896464467048645\n",
      "Iteration: 1553. Loss: 0.26386284828186035\n",
      "Iteration: 1554. Loss: 0.19945096969604492\n",
      "Iteration: 1555. Loss: 0.20912742614746094\n",
      "Iteration: 1556. Loss: 0.23391731083393097\n",
      "Iteration: 1557. Loss: 0.15956614911556244\n",
      "Iteration: 1558. Loss: 0.16728611290454865\n",
      "Iteration: 1559. Loss: 0.19371141493320465\n",
      "Iteration: 1560. Loss: 0.4093228876590729\n",
      "Iteration: 1561. Loss: 0.21354301273822784\n",
      "Iteration: 1562. Loss: 0.21834073960781097\n",
      "Iteration: 1563. Loss: 0.1562236100435257\n",
      "Iteration: 1564. Loss: 0.22025251388549805\n",
      "Iteration: 1565. Loss: 0.23903954029083252\n",
      "Iteration: 1566. Loss: 0.2807026207447052\n",
      "Iteration: 1567. Loss: 0.15531742572784424\n",
      "Iteration: 1568. Loss: 0.2243129312992096\n",
      "Iteration: 1569. Loss: 0.1372569054365158\n",
      "Iteration: 1570. Loss: 0.32459789514541626\n",
      "Iteration: 1571. Loss: 0.23561595380306244\n",
      "Iteration: 1572. Loss: 0.18332208693027496\n",
      "Iteration: 1573. Loss: 0.3150756359100342\n",
      "Iteration: 1574. Loss: 0.20255252718925476\n",
      "Iteration: 1575. Loss: 0.23112145066261292\n",
      "Iteration: 1576. Loss: 0.152658611536026\n",
      "Iteration: 1577. Loss: 0.17332559823989868\n",
      "Iteration: 1578. Loss: 0.1975248157978058\n",
      "Iteration: 1579. Loss: 0.0824379250407219\n",
      "Iteration: 1580. Loss: 0.24940204620361328\n",
      "Iteration: 1581. Loss: 0.3291057348251343\n",
      "Iteration: 1582. Loss: 0.22367876768112183\n",
      "Iteration: 1583. Loss: 0.3300119638442993\n",
      "Iteration: 1584. Loss: 0.24404238164424896\n",
      "Iteration: 1585. Loss: 0.23767711222171783\n",
      "Iteration: 1586. Loss: 0.1611631214618683\n",
      "Iteration: 1587. Loss: 0.201045423746109\n",
      "Iteration: 1588. Loss: 0.11924441158771515\n",
      "Iteration: 1589. Loss: 0.23225325345993042\n",
      "Iteration: 1590. Loss: 0.07209569960832596\n",
      "Iteration: 1591. Loss: 0.17762258648872375\n",
      "Iteration: 1592. Loss: 0.1930163949728012\n",
      "Iteration: 1593. Loss: 0.20555409789085388\n",
      "Iteration: 1594. Loss: 0.1887953132390976\n",
      "Iteration: 1595. Loss: 0.47249743342399597\n",
      "Iteration: 1596. Loss: 0.4258141815662384\n",
      "Iteration: 1597. Loss: 0.19737808406352997\n",
      "Iteration: 1598. Loss: 0.263979434967041\n",
      "Iteration: 1599. Loss: 0.21015626192092896\n",
      "Iteration: 1600. Loss: 0.14080673456192017\n",
      "Iteration: 1601. Loss: 0.14364047348499298\n",
      "Iteration: 1602. Loss: 0.16218557953834534\n",
      "Iteration: 1603. Loss: 0.20370464026927948\n",
      "Iteration: 1604. Loss: 0.25715380907058716\n",
      "Iteration: 1605. Loss: 0.15322433412075043\n",
      "Iteration: 1606. Loss: 0.11949624866247177\n",
      "Iteration: 1607. Loss: 0.2229383885860443\n",
      "Iteration: 1608. Loss: 0.21385011076927185\n",
      "Iteration: 1609. Loss: 0.2381310611963272\n",
      "Iteration: 1610. Loss: 0.1940225213766098\n",
      "Iteration: 1611. Loss: 0.35155728459358215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1612. Loss: 0.1413833051919937\n",
      "Iteration: 1613. Loss: 0.26445379853248596\n",
      "Iteration: 1614. Loss: 0.1399165391921997\n",
      "Iteration: 1615. Loss: 0.17188526690006256\n",
      "Iteration: 1616. Loss: 0.07198075950145721\n",
      "Iteration: 1617. Loss: 0.16511230170726776\n",
      "Iteration: 1618. Loss: 0.19421285390853882\n",
      "Iteration: 1619. Loss: 0.08826715499162674\n",
      "Iteration: 1620. Loss: 0.11019890755414963\n",
      "Iteration: 1621. Loss: 0.1990506798028946\n",
      "Iteration: 1622. Loss: 0.3915640711784363\n",
      "Iteration: 1623. Loss: 0.22777879238128662\n",
      "Iteration: 1624. Loss: 0.20724527537822723\n",
      "Iteration: 1625. Loss: 0.23680754005908966\n",
      "Iteration: 1626. Loss: 0.29040470719337463\n",
      "Iteration: 1627. Loss: 0.07336445152759552\n",
      "Iteration: 1628. Loss: 0.09537165611982346\n",
      "Iteration: 1629. Loss: 0.0965089201927185\n",
      "Iteration: 1630. Loss: 0.2868637144565582\n",
      "Iteration: 1631. Loss: 0.3140818178653717\n",
      "Iteration: 1632. Loss: 0.2618210017681122\n",
      "Iteration: 1633. Loss: 0.20459431409835815\n",
      "Iteration: 1634. Loss: 0.1821574568748474\n",
      "Iteration: 1635. Loss: 0.14878098666667938\n",
      "Iteration: 1636. Loss: 0.34593337774276733\n",
      "Iteration: 1637. Loss: 0.2737785875797272\n",
      "Iteration: 1638. Loss: 0.22804881632328033\n",
      "Iteration: 1639. Loss: 0.16390037536621094\n",
      "Iteration: 1640. Loss: 0.241676926612854\n",
      "Iteration: 1641. Loss: 0.207030788064003\n",
      "Iteration: 1642. Loss: 0.21088697016239166\n",
      "Iteration: 1643. Loss: 0.10597625374794006\n",
      "Iteration: 1644. Loss: 0.23570379614830017\n",
      "Iteration: 1645. Loss: 0.22095511853694916\n",
      "Iteration: 1646. Loss: 0.2496897131204605\n",
      "Iteration: 1647. Loss: 0.26551154255867004\n",
      "Iteration: 1648. Loss: 0.16707377135753632\n",
      "Iteration: 1649. Loss: 0.29813018441200256\n",
      "Iteration: 1650. Loss: 0.13632391393184662\n",
      "Iteration: 1651. Loss: 0.18138369917869568\n",
      "Iteration: 1652. Loss: 0.1476629227399826\n",
      "Iteration: 1653. Loss: 0.2325926274061203\n",
      "Iteration: 1654. Loss: 0.14857301115989685\n",
      "Iteration: 1655. Loss: 0.2857532501220703\n",
      "Iteration: 1656. Loss: 0.2402178943157196\n",
      "Iteration: 1657. Loss: 0.19174563884735107\n",
      "Iteration: 1658. Loss: 0.14914588630199432\n",
      "Iteration: 1659. Loss: 0.19436657428741455\n",
      "Iteration: 1660. Loss: 0.2942976653575897\n",
      "Iteration: 1661. Loss: 0.21097998321056366\n",
      "Iteration: 1662. Loss: 0.18975405395030975\n",
      "Iteration: 1663. Loss: 0.15573114156723022\n",
      "Iteration: 1664. Loss: 0.17135566473007202\n",
      "Iteration: 1665. Loss: 0.2998235523700714\n",
      "Iteration: 1666. Loss: 0.18820662796497345\n",
      "Iteration: 1667. Loss: 0.10730025172233582\n",
      "Iteration: 1668. Loss: 0.22317974269390106\n",
      "Iteration: 1669. Loss: 0.1945342868566513\n",
      "Iteration: 1670. Loss: 0.16157713532447815\n",
      "Iteration: 1671. Loss: 0.24740192294120789\n",
      "Iteration: 1672. Loss: 0.29494431614875793\n",
      "Iteration: 1673. Loss: 0.13724854588508606\n",
      "Iteration: 1674. Loss: 0.10018303990364075\n",
      "Iteration: 1675. Loss: 0.1851174533367157\n",
      "Iteration: 1676. Loss: 0.20471927523612976\n",
      "Iteration: 1677. Loss: 0.14873558282852173\n",
      "Iteration: 1678. Loss: 0.1362680047750473\n",
      "Iteration: 1679. Loss: 0.2655033767223358\n",
      "Iteration: 1680. Loss: 0.28027859330177307\n",
      "Iteration: 1681. Loss: 0.1280864179134369\n",
      "Iteration: 1682. Loss: 0.20141743123531342\n",
      "Iteration: 1683. Loss: 0.18419769406318665\n",
      "Iteration: 1684. Loss: 0.22029657661914825\n",
      "Iteration: 1685. Loss: 0.16122891008853912\n",
      "Iteration: 1686. Loss: 0.2853044867515564\n",
      "Iteration: 1687. Loss: 0.1510476917028427\n",
      "Iteration: 1688. Loss: 0.19337867200374603\n",
      "Iteration: 1689. Loss: 0.1834009885787964\n",
      "Iteration: 1690. Loss: 0.1173042356967926\n",
      "Iteration: 1691. Loss: 0.11093337833881378\n",
      "Iteration: 1692. Loss: 0.30430299043655396\n",
      "Iteration: 1693. Loss: 0.2704213559627533\n",
      "Iteration: 1694. Loss: 0.3355218172073364\n",
      "Iteration: 1695. Loss: 0.21637597680091858\n",
      "Iteration: 1696. Loss: 0.12776780128479004\n",
      "Iteration: 1697. Loss: 0.26097145676612854\n",
      "Iteration: 1698. Loss: 0.2218034565448761\n",
      "Iteration: 1699. Loss: 0.2156691700220108\n",
      "Iteration: 1700. Loss: 0.23851342499256134\n",
      "Iteration: 1701. Loss: 0.21999439597129822\n",
      "Iteration: 1702. Loss: 0.16344894468784332\n",
      "Iteration: 1703. Loss: 0.313425213098526\n",
      "Iteration: 1704. Loss: 0.12653519213199615\n",
      "Iteration: 1705. Loss: 0.3186565637588501\n",
      "Iteration: 1706. Loss: 0.17503303289413452\n",
      "Iteration: 1707. Loss: 0.1793152242898941\n",
      "Iteration: 1708. Loss: 0.22729526460170746\n",
      "Iteration: 1709. Loss: 0.23642778396606445\n",
      "Iteration: 1710. Loss: 0.26703929901123047\n",
      "Iteration: 1711. Loss: 0.16240717470645905\n",
      "Iteration: 1712. Loss: 0.22163665294647217\n",
      "Iteration: 1713. Loss: 0.22182749211788177\n",
      "Iteration: 1714. Loss: 0.21837423741817474\n",
      "Iteration: 1715. Loss: 0.18380026519298553\n",
      "Iteration: 1716. Loss: 0.1405676007270813\n",
      "Iteration: 1717. Loss: 0.20398689806461334\n",
      "Iteration: 1718. Loss: 0.18719562888145447\n",
      "Iteration: 1719. Loss: 0.21478904783725739\n",
      "Iteration: 1720. Loss: 0.1691981703042984\n",
      "Iteration: 1721. Loss: 0.17765183746814728\n",
      "Iteration: 1722. Loss: 0.24276377260684967\n",
      "Iteration: 1723. Loss: 0.2635592818260193\n",
      "Iteration: 1724. Loss: 0.154826819896698\n",
      "Iteration: 1725. Loss: 0.17483915388584137\n",
      "Iteration: 1726. Loss: 0.1591680347919464\n",
      "Iteration: 1727. Loss: 0.21871334314346313\n",
      "Iteration: 1728. Loss: 0.1754569113254547\n",
      "Iteration: 1729. Loss: 0.180659219622612\n",
      "Iteration: 1730. Loss: 0.37709519267082214\n",
      "Iteration: 1731. Loss: 0.21359682083129883\n",
      "Iteration: 1732. Loss: 0.21017415821552277\n",
      "Iteration: 1733. Loss: 0.11566369235515594\n",
      "Iteration: 1734. Loss: 0.20411688089370728\n",
      "Iteration: 1735. Loss: 0.2692672610282898\n",
      "Iteration: 1736. Loss: 0.22196513414382935\n",
      "Iteration: 1737. Loss: 0.1551257222890854\n",
      "Iteration: 1738. Loss: 0.22181367874145508\n",
      "Iteration: 1739. Loss: 0.11836004257202148\n",
      "Iteration: 1740. Loss: 0.14238673448562622\n",
      "Iteration: 1741. Loss: 0.20292603969573975\n",
      "Iteration: 1742. Loss: 0.15350106358528137\n",
      "Iteration: 1743. Loss: 0.15570244193077087\n",
      "Iteration: 1744. Loss: 0.12250611186027527\n",
      "Iteration: 1745. Loss: 0.25042134523391724\n",
      "Iteration: 1746. Loss: 0.16675446927547455\n",
      "Iteration: 1747. Loss: 0.21528565883636475\n",
      "Iteration: 1748. Loss: 0.2511672079563141\n",
      "Iteration: 1749. Loss: 0.13499312102794647\n",
      "Iteration: 1750. Loss: 0.08076851814985275\n",
      "Iteration: 1751. Loss: 0.1279841512441635\n",
      "Iteration: 1752. Loss: 0.19074536859989166\n",
      "Iteration: 1753. Loss: 0.3718816339969635\n",
      "Iteration: 1754. Loss: 0.3167549669742584\n",
      "Iteration: 1755. Loss: 0.17692483961582184\n",
      "Iteration: 1756. Loss: 0.24899299442768097\n",
      "Iteration: 1757. Loss: 0.12899067997932434\n",
      "Iteration: 1758. Loss: 0.07093223929405212\n",
      "Iteration: 1759. Loss: 0.2573744058609009\n",
      "Iteration: 1760. Loss: 0.2176445573568344\n",
      "Iteration: 1761. Loss: 0.16852548718452454\n",
      "Iteration: 1762. Loss: 0.19411645829677582\n",
      "Iteration: 1763. Loss: 0.19916361570358276\n",
      "Iteration: 1764. Loss: 0.23692283034324646\n",
      "Iteration: 1765. Loss: 0.13029246032238007\n",
      "Iteration: 1766. Loss: 0.30871543288230896\n",
      "Iteration: 1767. Loss: 0.18850137293338776\n",
      "Iteration: 1768. Loss: 0.20087194442749023\n",
      "Iteration: 1769. Loss: 0.16387498378753662\n",
      "Iteration: 1770. Loss: 0.3022945523262024\n",
      "Iteration: 1771. Loss: 0.3368333876132965\n",
      "Iteration: 1772. Loss: 0.21110133826732635\n",
      "Iteration: 1773. Loss: 0.19382867217063904\n",
      "Iteration: 1774. Loss: 0.10795493423938751\n",
      "Iteration: 1775. Loss: 0.23058897256851196\n",
      "Iteration: 1776. Loss: 0.10460986196994781\n",
      "Iteration: 1777. Loss: 0.21364238858222961\n",
      "Iteration: 1778. Loss: 0.14487320184707642\n",
      "Iteration: 1779. Loss: 0.2196560800075531\n",
      "Iteration: 1780. Loss: 0.1977396011352539\n",
      "Iteration: 1781. Loss: 0.20292839407920837\n",
      "Iteration: 1782. Loss: 0.15299245715141296\n",
      "Iteration: 1783. Loss: 0.4182126224040985\n",
      "Iteration: 1784. Loss: 0.19333258271217346\n",
      "Iteration: 1785. Loss: 0.11796845495700836\n",
      "Iteration: 1786. Loss: 0.14345139265060425\n",
      "Iteration: 1787. Loss: 0.24342364072799683\n",
      "Iteration: 1788. Loss: 0.18459327518939972\n",
      "Iteration: 1789. Loss: 0.22343763709068298\n",
      "Iteration: 1790. Loss: 0.09791087359189987\n",
      "Iteration: 1791. Loss: 0.14016364514827728\n",
      "Iteration: 1792. Loss: 0.24690128862857819\n",
      "Iteration: 1793. Loss: 0.22373878955841064\n",
      "Iteration: 1794. Loss: 0.08274441957473755\n",
      "Iteration: 1795. Loss: 0.13427576422691345\n",
      "Iteration: 1796. Loss: 0.09575948119163513\n",
      "Iteration: 1797. Loss: 0.2134697288274765\n",
      "Iteration: 1798. Loss: 0.14514726400375366\n",
      "Iteration: 1799. Loss: 0.16381840407848358\n",
      "Iteration: 1800. Loss: 0.26634761691093445\n",
      "Iteration: 1801. Loss: 0.12261715531349182\n",
      "Iteration: 1802. Loss: 0.2096707969903946\n",
      "Iteration: 1803. Loss: 0.18907009065151215\n",
      "Iteration: 1804. Loss: 0.19733764231204987\n",
      "Iteration: 1805. Loss: 0.2177843302488327\n",
      "Iteration: 1806. Loss: 0.24708129465579987\n",
      "Iteration: 1807. Loss: 0.16686779260635376\n",
      "Iteration: 1808. Loss: 0.17774371802806854\n",
      "Iteration: 1809. Loss: 0.19344229996204376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1810. Loss: 0.2607179880142212\n",
      "Iteration: 1811. Loss: 0.15278324484825134\n",
      "Iteration: 1812. Loss: 0.1364121288061142\n",
      "Iteration: 1813. Loss: 0.2074587196111679\n",
      "Iteration: 1814. Loss: 0.2813507318496704\n",
      "Iteration: 1815. Loss: 0.2465590536594391\n",
      "Iteration: 1816. Loss: 0.169688880443573\n",
      "Iteration: 1817. Loss: 0.11161284148693085\n",
      "Iteration: 1818. Loss: 0.11925289779901505\n",
      "Iteration: 1819. Loss: 0.18854796886444092\n",
      "Iteration: 1820. Loss: 0.14992386102676392\n",
      "Iteration: 1821. Loss: 0.2212287336587906\n",
      "Iteration: 1822. Loss: 0.2436596155166626\n",
      "Iteration: 1823. Loss: 0.28311625123023987\n",
      "Iteration: 1824. Loss: 0.30323129892349243\n",
      "Iteration: 1825. Loss: 0.08380066603422165\n",
      "Iteration: 1826. Loss: 0.19997285306453705\n",
      "Iteration: 1827. Loss: 0.16798517107963562\n",
      "Iteration: 1828. Loss: 0.16840948164463043\n",
      "Iteration: 1829. Loss: 0.1575298309326172\n",
      "Iteration: 1830. Loss: 0.11741120368242264\n",
      "Iteration: 1831. Loss: 0.2104654461145401\n",
      "Iteration: 1832. Loss: 0.1755039244890213\n",
      "Iteration: 1833. Loss: 0.19870500266551971\n",
      "Iteration: 1834. Loss: 0.12756052613258362\n",
      "Iteration: 1835. Loss: 0.2168704867362976\n",
      "Iteration: 1836. Loss: 0.15781204402446747\n",
      "Iteration: 1837. Loss: 0.15661877393722534\n",
      "Iteration: 1838. Loss: 0.2644149661064148\n",
      "Iteration: 1839. Loss: 0.13830217719078064\n",
      "Iteration: 1840. Loss: 0.2554810643196106\n",
      "Iteration: 1841. Loss: 0.14366871118545532\n",
      "Iteration: 1842. Loss: 0.16853883862495422\n",
      "Iteration: 1843. Loss: 0.12102396786212921\n",
      "Iteration: 1844. Loss: 0.2156689614057541\n",
      "Iteration: 1845. Loss: 0.17257235944271088\n",
      "Iteration: 1846. Loss: 0.1756688952445984\n",
      "Iteration: 1847. Loss: 0.22559234499931335\n",
      "Iteration: 1848. Loss: 0.2008596956729889\n",
      "Iteration: 1849. Loss: 0.1793234497308731\n",
      "Iteration: 1850. Loss: 0.18660159409046173\n",
      "Iteration: 1851. Loss: 0.19703826308250427\n",
      "Iteration: 1852. Loss: 0.20117169618606567\n",
      "Iteration: 1853. Loss: 0.3070397973060608\n",
      "Iteration: 1854. Loss: 0.18179507553577423\n",
      "Iteration: 1855. Loss: 0.17720481753349304\n",
      "Iteration: 1856. Loss: 0.09431235492229462\n",
      "Iteration: 1857. Loss: 0.09529998153448105\n",
      "Iteration: 1858. Loss: 0.3161422908306122\n",
      "Iteration: 1859. Loss: 0.08809515088796616\n",
      "Iteration: 1860. Loss: 0.19913189113140106\n",
      "Iteration: 1861. Loss: 0.11766800284385681\n",
      "Iteration: 1862. Loss: 0.28077298402786255\n",
      "Iteration: 1863. Loss: 0.10406384617090225\n",
      "Iteration: 1864. Loss: 0.2059018313884735\n",
      "Iteration: 1865. Loss: 0.18134886026382446\n",
      "Iteration: 1866. Loss: 0.22829307615756989\n",
      "Iteration: 1867. Loss: 0.17467078566551208\n",
      "Iteration: 1868. Loss: 0.2445906698703766\n",
      "Iteration: 1869. Loss: 0.2562035024166107\n",
      "Iteration: 1870. Loss: 0.18824368715286255\n",
      "Iteration: 1871. Loss: 0.2521853744983673\n",
      "Iteration: 1872. Loss: 0.3531116843223572\n",
      "Iteration: 1873. Loss: 0.1365782469511032\n",
      "Iteration: 1874. Loss: 0.22520649433135986\n",
      "Iteration: 1875. Loss: 0.17648597061634064\n",
      "Iteration: 1876. Loss: 0.18114091455936432\n",
      "Iteration: 1877. Loss: 0.2824847996234894\n",
      "Iteration: 1878. Loss: 0.183854877948761\n",
      "Iteration: 1879. Loss: 0.17347125709056854\n",
      "Iteration: 1880. Loss: 0.1598813682794571\n",
      "Iteration: 1881. Loss: 0.14845877885818481\n",
      "Iteration: 1882. Loss: 0.20329919457435608\n",
      "Iteration: 1883. Loss: 0.21661095321178436\n",
      "Iteration: 1884. Loss: 0.2632933259010315\n",
      "Iteration: 1885. Loss: 0.19773396849632263\n",
      "Iteration: 1886. Loss: 0.23507682979106903\n",
      "Iteration: 1887. Loss: 0.17935049533843994\n",
      "Iteration: 1888. Loss: 0.2402835339307785\n",
      "Iteration: 1889. Loss: 0.3206338584423065\n",
      "Iteration: 1890. Loss: 0.16373005509376526\n",
      "Iteration: 1891. Loss: 0.21458706259727478\n",
      "Iteration: 1892. Loss: 0.2670176327228546\n",
      "Iteration: 1893. Loss: 0.11378110945224762\n",
      "Iteration: 1894. Loss: 0.17888320982456207\n",
      "Iteration: 1895. Loss: 0.2608155310153961\n",
      "Iteration: 1896. Loss: 0.1540747880935669\n",
      "Iteration: 1897. Loss: 0.24146714806556702\n",
      "Iteration: 1898. Loss: 0.16668163239955902\n",
      "Iteration: 1899. Loss: 0.16323670744895935\n",
      "Iteration: 1900. Loss: 0.1702909618616104\n",
      "Iteration: 1901. Loss: 0.12776267528533936\n",
      "Iteration: 1902. Loss: 0.2424875795841217\n",
      "Iteration: 1903. Loss: 0.1659170687198639\n",
      "Iteration: 1904. Loss: 0.1346990317106247\n",
      "Iteration: 1905. Loss: 0.11244311928749084\n",
      "Iteration: 1906. Loss: 0.18084725737571716\n",
      "Iteration: 1907. Loss: 0.3962121903896332\n",
      "Iteration: 1908. Loss: 0.17077413201332092\n",
      "Iteration: 1909. Loss: 0.16135741770267487\n",
      "Iteration: 1910. Loss: 0.10438943654298782\n",
      "Iteration: 1911. Loss: 0.13049229979515076\n",
      "Iteration: 1912. Loss: 0.1589282751083374\n",
      "Iteration: 1913. Loss: 0.17484945058822632\n",
      "Iteration: 1914. Loss: 0.10556841641664505\n",
      "Iteration: 1915. Loss: 0.2281031757593155\n",
      "Iteration: 1916. Loss: 0.27290287613868713\n",
      "Iteration: 1917. Loss: 0.15827339887619019\n",
      "Iteration: 1918. Loss: 0.09314793348312378\n",
      "Iteration: 1919. Loss: 0.24014215171337128\n",
      "Iteration: 1920. Loss: 0.2640950083732605\n",
      "Iteration: 1921. Loss: 0.14492425322532654\n",
      "Iteration: 1922. Loss: 0.19423742592334747\n",
      "Iteration: 1923. Loss: 0.2973730266094208\n",
      "Iteration: 1924. Loss: 0.1488940417766571\n",
      "Iteration: 1925. Loss: 0.4103265702724457\n",
      "Iteration: 1926. Loss: 0.14972198009490967\n",
      "Iteration: 1927. Loss: 0.14405809342861176\n",
      "Iteration: 1928. Loss: 0.16658267378807068\n",
      "Iteration: 1929. Loss: 0.16751886904239655\n",
      "Iteration: 1930. Loss: 0.16119424998760223\n",
      "Iteration: 1931. Loss: 0.1406109780073166\n",
      "Iteration: 1932. Loss: 0.16486991941928864\n",
      "Iteration: 1933. Loss: 0.18022087216377258\n",
      "Iteration: 1934. Loss: 0.19369712471961975\n",
      "Iteration: 1935. Loss: 0.18318869173526764\n",
      "Iteration: 1936. Loss: 0.2737293541431427\n",
      "Iteration: 1937. Loss: 0.1690167486667633\n",
      "Iteration: 1938. Loss: 0.15503472089767456\n",
      "Iteration: 1939. Loss: 0.20595672726631165\n",
      "Iteration: 1940. Loss: 0.21990841627120972\n",
      "Iteration: 1941. Loss: 0.25205186009407043\n",
      "Iteration: 1942. Loss: 0.14128996431827545\n",
      "Iteration: 1943. Loss: 0.16583551466464996\n",
      "Iteration: 1944. Loss: 0.17285627126693726\n",
      "Iteration: 1945. Loss: 0.0727783814072609\n",
      "Iteration: 1946. Loss: 0.19418485462665558\n",
      "Iteration: 1947. Loss: 0.10612481087446213\n",
      "Iteration: 1948. Loss: 0.1578645557165146\n",
      "Iteration: 1949. Loss: 0.1419278234243393\n",
      "Iteration: 1950. Loss: 0.26458075642585754\n",
      "Iteration: 1951. Loss: 0.179777130484581\n",
      "Iteration: 1952. Loss: 0.2430870682001114\n",
      "Iteration: 1953. Loss: 0.3511962592601776\n",
      "Iteration: 1954. Loss: 0.16253109276294708\n",
      "Iteration: 1955. Loss: 0.20401763916015625\n",
      "Iteration: 1956. Loss: 0.10764036327600479\n",
      "Iteration: 1957. Loss: 0.2282433956861496\n",
      "Iteration: 1958. Loss: 0.15432552993297577\n",
      "Iteration: 1959. Loss: 0.21071358025074005\n",
      "Iteration: 1960. Loss: 0.08371011912822723\n",
      "Iteration: 1961. Loss: 0.14374032616615295\n",
      "Iteration: 1962. Loss: 0.16662776470184326\n",
      "Iteration: 1963. Loss: 0.17228420078754425\n",
      "Iteration: 1964. Loss: 0.13527819514274597\n",
      "Iteration: 1965. Loss: 0.17322027683258057\n",
      "Iteration: 1966. Loss: 0.24689683318138123\n",
      "Iteration: 1967. Loss: 0.2357056736946106\n",
      "Iteration: 1968. Loss: 0.22913409769535065\n",
      "Iteration: 1969. Loss: 0.23399491608142853\n",
      "Iteration: 1970. Loss: 0.15065419673919678\n",
      "Iteration: 1971. Loss: 0.22816379368305206\n",
      "Iteration: 1972. Loss: 0.12534064054489136\n",
      "Iteration: 1973. Loss: 0.21501590311527252\n",
      "Iteration: 1974. Loss: 0.17895185947418213\n",
      "Iteration: 1975. Loss: 0.21478085219860077\n",
      "Iteration: 1976. Loss: 0.24601727724075317\n",
      "Iteration: 1977. Loss: 0.1888817995786667\n",
      "Iteration: 1978. Loss: 0.3156856298446655\n",
      "Iteration: 1979. Loss: 0.1657976359128952\n",
      "Iteration: 1980. Loss: 0.21778883039951324\n",
      "Iteration: 1981. Loss: 0.11432023346424103\n",
      "Iteration: 1982. Loss: 0.24079443514347076\n",
      "Iteration: 1983. Loss: 0.1590932309627533\n",
      "Iteration: 1984. Loss: 0.2029072642326355\n",
      "Iteration: 1985. Loss: 0.2232598066329956\n",
      "Iteration: 1986. Loss: 0.18996931612491608\n",
      "Iteration: 1987. Loss: 0.2028643786907196\n",
      "Iteration: 1988. Loss: 0.22449655830860138\n",
      "Iteration: 1989. Loss: 0.1354459524154663\n",
      "Iteration: 1990. Loss: 0.2620248794555664\n",
      "Iteration: 1991. Loss: 0.07519263029098511\n",
      "Iteration: 1992. Loss: 0.10857908427715302\n",
      "Iteration: 1993. Loss: 0.19090279936790466\n",
      "Iteration: 1994. Loss: 0.14782024919986725\n",
      "Iteration: 1995. Loss: 0.25462302565574646\n",
      "Iteration: 1996. Loss: 0.15041080117225647\n",
      "Iteration: 1997. Loss: 0.19659166038036346\n",
      "Iteration: 1998. Loss: 0.21600289642810822\n",
      "Iteration: 1999. Loss: 0.1737498790025711\n",
      "Iteration: 2000. Loss: 0.2555015981197357\n",
      "Iteration: 2001. Loss: 0.23924002051353455\n",
      "Iteration: 2002. Loss: 0.17558395862579346\n",
      "Iteration: 2003. Loss: 0.17347751557826996\n",
      "Iteration: 2004. Loss: 0.0881088450551033\n",
      "Iteration: 2005. Loss: 0.15611997246742249\n",
      "Iteration: 2006. Loss: 0.15692882239818573\n",
      "Iteration: 2007. Loss: 0.2792023718357086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2008. Loss: 0.12249980866909027\n",
      "Iteration: 2009. Loss: 0.19577422738075256\n",
      "Iteration: 2010. Loss: 0.09928983449935913\n",
      "Iteration: 2011. Loss: 0.14621582627296448\n",
      "Iteration: 2012. Loss: 0.16118916869163513\n",
      "Iteration: 2013. Loss: 0.31068405508995056\n",
      "Iteration: 2014. Loss: 0.17626141011714935\n",
      "Iteration: 2015. Loss: 0.14702481031417847\n",
      "Iteration: 2016. Loss: 0.050419557839632034\n",
      "Iteration: 2017. Loss: 0.3290940821170807\n",
      "Iteration: 2018. Loss: 0.16454319655895233\n",
      "Iteration: 2019. Loss: 0.19786600768566132\n",
      "Iteration: 2020. Loss: 0.19661098718643188\n",
      "Iteration: 2021. Loss: 0.2417369782924652\n",
      "Iteration: 2022. Loss: 0.1504179835319519\n",
      "Iteration: 2023. Loss: 0.14495201408863068\n",
      "Iteration: 2024. Loss: 0.14689326286315918\n",
      "Iteration: 2025. Loss: 0.2575259804725647\n",
      "Iteration: 2026. Loss: 0.19287458062171936\n",
      "Iteration: 2027. Loss: 0.16580243408679962\n",
      "Iteration: 2028. Loss: 0.09949758648872375\n",
      "Iteration: 2029. Loss: 0.0668022483587265\n",
      "Iteration: 2030. Loss: 0.15115393698215485\n",
      "Iteration: 2031. Loss: 0.11146829277276993\n",
      "Iteration: 2032. Loss: 0.11907074600458145\n",
      "Iteration: 2033. Loss: 0.2404850572347641\n",
      "Iteration: 2034. Loss: 0.10268624126911163\n",
      "Iteration: 2035. Loss: 0.11445168405771255\n",
      "Iteration: 2036. Loss: 0.20212823152542114\n",
      "Iteration: 2037. Loss: 0.3742474615573883\n",
      "Iteration: 2038. Loss: 0.14740681648254395\n",
      "Iteration: 2039. Loss: 0.174198716878891\n",
      "Iteration: 2040. Loss: 0.2186441421508789\n",
      "Iteration: 2041. Loss: 0.21444299817085266\n",
      "Iteration: 2042. Loss: 0.1557522565126419\n",
      "Iteration: 2043. Loss: 0.14827749133110046\n",
      "Iteration: 2044. Loss: 0.19822774827480316\n",
      "Iteration: 2045. Loss: 0.0960923507809639\n",
      "Iteration: 2046. Loss: 0.21459920704364777\n",
      "Iteration: 2047. Loss: 0.2012842893600464\n",
      "Iteration: 2048. Loss: 0.2429875135421753\n",
      "Iteration: 2049. Loss: 0.17911562323570251\n",
      "Iteration: 2050. Loss: 0.14362609386444092\n",
      "Iteration: 2051. Loss: 0.1535162329673767\n",
      "Iteration: 2052. Loss: 0.11127033829689026\n",
      "Iteration: 2053. Loss: 0.23493169248104095\n",
      "Iteration: 2054. Loss: 0.12964119017124176\n",
      "Iteration: 2055. Loss: 0.11587084084749222\n",
      "Iteration: 2056. Loss: 0.15418347716331482\n",
      "Iteration: 2057. Loss: 0.2023756355047226\n",
      "Iteration: 2058. Loss: 0.25260141491889954\n",
      "Iteration: 2059. Loss: 0.26948946714401245\n",
      "Iteration: 2060. Loss: 0.12581275403499603\n",
      "Iteration: 2061. Loss: 0.1925300806760788\n",
      "Iteration: 2062. Loss: 0.21034960448741913\n",
      "Iteration: 2063. Loss: 0.1529185175895691\n",
      "Iteration: 2064. Loss: 0.2963140606880188\n",
      "Iteration: 2065. Loss: 0.1159023866057396\n",
      "Iteration: 2066. Loss: 0.263696551322937\n",
      "Iteration: 2067. Loss: 0.3378099203109741\n",
      "Iteration: 2068. Loss: 0.21944597363471985\n",
      "Iteration: 2069. Loss: 0.17431215941905975\n",
      "Iteration: 2070. Loss: 0.19844917953014374\n",
      "Iteration: 2071. Loss: 0.21352313458919525\n",
      "Iteration: 2072. Loss: 0.14493364095687866\n",
      "Iteration: 2073. Loss: 0.2195417433977127\n",
      "Iteration: 2074. Loss: 0.13938643038272858\n",
      "Iteration: 2075. Loss: 0.24904035031795502\n",
      "Iteration: 2076. Loss: 0.18013706803321838\n",
      "Iteration: 2077. Loss: 0.1647365689277649\n",
      "Iteration: 2078. Loss: 0.18925514817237854\n",
      "Iteration: 2079. Loss: 0.1472519338130951\n",
      "Iteration: 2080. Loss: 0.3341916799545288\n",
      "Iteration: 2081. Loss: 0.22629456222057343\n",
      "Iteration: 2082. Loss: 0.2142733931541443\n",
      "Iteration: 2083. Loss: 0.09774044156074524\n",
      "Iteration: 2084. Loss: 0.24070289731025696\n",
      "Iteration: 2085. Loss: 0.12731707096099854\n",
      "Iteration: 2086. Loss: 0.2287888377904892\n",
      "Iteration: 2087. Loss: 0.09525895863771439\n",
      "Iteration: 2088. Loss: 0.08355014026165009\n",
      "Iteration: 2089. Loss: 0.1518019139766693\n",
      "Iteration: 2090. Loss: 0.19335046410560608\n",
      "Iteration: 2091. Loss: 0.17056626081466675\n",
      "Iteration: 2092. Loss: 0.24867138266563416\n",
      "Iteration: 2093. Loss: 0.18446965515613556\n",
      "Iteration: 2094. Loss: 0.18515335023403168\n",
      "Iteration: 2095. Loss: 0.13198043406009674\n",
      "Iteration: 2096. Loss: 0.14227311313152313\n",
      "Iteration: 2097. Loss: 0.2726980149745941\n",
      "Iteration: 2098. Loss: 0.2323993742465973\n",
      "Iteration: 2099. Loss: 0.1568787544965744\n",
      "Iteration: 2100. Loss: 0.21411669254302979\n",
      "Iteration: 2101. Loss: 0.17345024645328522\n",
      "Iteration: 2102. Loss: 0.23952509462833405\n",
      "Iteration: 2103. Loss: 0.2962595820426941\n",
      "Iteration: 2104. Loss: 0.2185976207256317\n",
      "Iteration: 2105. Loss: 0.14072705805301666\n",
      "Iteration: 2106. Loss: 0.15980368852615356\n",
      "Iteration: 2107. Loss: 0.13778215646743774\n",
      "Iteration: 2108. Loss: 0.08446096628904343\n",
      "Iteration: 2109. Loss: 0.2223721146583557\n",
      "Iteration: 2110. Loss: 0.15603110194206238\n",
      "Iteration: 2111. Loss: 0.3342154324054718\n",
      "Iteration: 2112. Loss: 0.12881328165531158\n",
      "Iteration: 2113. Loss: 0.2353622019290924\n",
      "Iteration: 2114. Loss: 0.24023251235485077\n",
      "Iteration: 2115. Loss: 0.30387771129608154\n",
      "Iteration: 2116. Loss: 0.17143692076206207\n",
      "Iteration: 2117. Loss: 0.2851562798023224\n",
      "Iteration: 2118. Loss: 0.12988877296447754\n",
      "Iteration: 2119. Loss: 0.18528449535369873\n",
      "Iteration: 2120. Loss: 0.1461319923400879\n",
      "Iteration: 2121. Loss: 0.1891183853149414\n",
      "Iteration: 2122. Loss: 0.13876307010650635\n",
      "Iteration: 2123. Loss: 0.15480130910873413\n",
      "Iteration: 2124. Loss: 0.10278160125017166\n",
      "Iteration: 2125. Loss: 0.15719375014305115\n",
      "Iteration: 2126. Loss: 0.23488065600395203\n",
      "Iteration: 2127. Loss: 0.14819301664829254\n",
      "Iteration: 2128. Loss: 0.22625316679477692\n",
      "Iteration: 2129. Loss: 0.10144445300102234\n",
      "Iteration: 2130. Loss: 0.2338874191045761\n",
      "Iteration: 2131. Loss: 0.20899519324302673\n",
      "Iteration: 2132. Loss: 0.24788552522659302\n",
      "Iteration: 2133. Loss: 0.1337175965309143\n",
      "Iteration: 2134. Loss: 0.17529451847076416\n",
      "Iteration: 2135. Loss: 0.16369710862636566\n",
      "Iteration: 2136. Loss: 0.135240837931633\n",
      "Iteration: 2137. Loss: 0.20581230521202087\n",
      "Iteration: 2138. Loss: 0.21433503925800323\n",
      "Iteration: 2139. Loss: 0.13879618048667908\n",
      "Iteration: 2140. Loss: 0.14696183800697327\n",
      "Iteration: 2141. Loss: 0.2594722807407379\n",
      "Iteration: 2142. Loss: 0.11074856668710709\n",
      "Iteration: 2143. Loss: 0.1065981313586235\n",
      "Iteration: 2144. Loss: 0.18693651258945465\n",
      "Iteration: 2145. Loss: 0.16165490448474884\n",
      "Iteration: 2146. Loss: 0.22094665467739105\n",
      "Iteration: 2147. Loss: 0.4801934063434601\n",
      "Iteration: 2148. Loss: 0.15905725955963135\n",
      "Iteration: 2149. Loss: 0.10914289206266403\n",
      "Iteration: 2150. Loss: 0.1347375512123108\n",
      "Iteration: 2151. Loss: 0.20192450284957886\n",
      "Iteration: 2152. Loss: 0.08843793720006943\n",
      "Iteration: 2153. Loss: 0.19867047667503357\n",
      "Iteration: 2154. Loss: 0.15778546035289764\n",
      "Iteration: 2155. Loss: 0.21618638932704926\n",
      "Iteration: 2156. Loss: 0.1155339777469635\n",
      "Iteration: 2157. Loss: 0.149119034409523\n",
      "Iteration: 2158. Loss: 0.10898688435554504\n",
      "Iteration: 2159. Loss: 0.058086227625608444\n",
      "Iteration: 2160. Loss: 0.19340866804122925\n",
      "Iteration: 2161. Loss: 0.16565965116024017\n",
      "Iteration: 2162. Loss: 0.3039604425430298\n",
      "Iteration: 2163. Loss: 0.14766378700733185\n",
      "Iteration: 2164. Loss: 0.14965638518333435\n",
      "Iteration: 2165. Loss: 0.2369478940963745\n",
      "Iteration: 2166. Loss: 0.1955651491880417\n",
      "Iteration: 2167. Loss: 0.1775672882795334\n",
      "Iteration: 2168. Loss: 0.303064227104187\n",
      "Iteration: 2169. Loss: 0.12952092289924622\n",
      "Iteration: 2170. Loss: 0.07293452322483063\n",
      "Iteration: 2171. Loss: 0.22372721135616302\n",
      "Iteration: 2172. Loss: 0.1859978288412094\n",
      "Iteration: 2173. Loss: 0.35308024287223816\n",
      "Iteration: 2174. Loss: 0.0890754982829094\n",
      "Iteration: 2175. Loss: 0.10730539262294769\n",
      "Iteration: 2176. Loss: 0.15230682492256165\n",
      "Iteration: 2177. Loss: 0.11607841402292252\n",
      "Iteration: 2178. Loss: 0.13721910119056702\n",
      "Iteration: 2179. Loss: 0.11919916421175003\n",
      "Iteration: 2180. Loss: 0.17194756865501404\n",
      "Iteration: 2181. Loss: 0.25477874279022217\n",
      "Iteration: 2182. Loss: 0.14860621094703674\n",
      "Iteration: 2183. Loss: 0.24786776304244995\n",
      "Iteration: 2184. Loss: 0.11624637246131897\n",
      "Iteration: 2185. Loss: 0.20284391939640045\n",
      "Iteration: 2186. Loss: 0.1933884471654892\n",
      "Iteration: 2187. Loss: 0.16070018708705902\n",
      "Iteration: 2188. Loss: 0.08320912718772888\n",
      "Iteration: 2189. Loss: 0.2554788291454315\n",
      "Iteration: 2190. Loss: 0.12467674911022186\n",
      "Iteration: 2191. Loss: 0.15546613931655884\n",
      "Iteration: 2192. Loss: 0.23105114698410034\n",
      "Iteration: 2193. Loss: 0.1336635947227478\n",
      "Iteration: 2194. Loss: 0.2105637788772583\n",
      "Iteration: 2195. Loss: 0.13183806836605072\n",
      "Iteration: 2196. Loss: 0.22052977979183197\n",
      "Iteration: 2197. Loss: 0.28482797741889954\n",
      "Iteration: 2198. Loss: 0.16820688545703888\n",
      "Iteration: 2199. Loss: 0.16481426358222961\n",
      "Iteration: 2200. Loss: 0.1299719512462616\n",
      "Iteration: 2201. Loss: 0.1136678159236908\n",
      "Iteration: 2202. Loss: 0.07096204906702042\n",
      "Iteration: 2203. Loss: 0.3108263909816742\n",
      "Iteration: 2204. Loss: 0.16496433317661285\n",
      "Iteration: 2205. Loss: 0.1638953536748886\n",
      "Iteration: 2206. Loss: 0.19893543422222137\n",
      "Iteration: 2207. Loss: 0.08738350868225098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2208. Loss: 0.22179128229618073\n",
      "Iteration: 2209. Loss: 0.20823746919631958\n",
      "Iteration: 2210. Loss: 0.14986364543437958\n",
      "Iteration: 2211. Loss: 0.204135000705719\n",
      "Iteration: 2212. Loss: 0.17678675055503845\n",
      "Iteration: 2213. Loss: 0.18532438576221466\n",
      "Iteration: 2214. Loss: 0.15293288230895996\n",
      "Iteration: 2215. Loss: 0.21967476606369019\n",
      "Iteration: 2216. Loss: 0.20126764476299286\n",
      "Iteration: 2217. Loss: 0.17531976103782654\n",
      "Iteration: 2218. Loss: 0.17303365468978882\n",
      "Iteration: 2219. Loss: 0.2018737941980362\n",
      "Iteration: 2220. Loss: 0.21257320046424866\n",
      "Iteration: 2221. Loss: 0.1797393560409546\n",
      "Iteration: 2222. Loss: 0.3134385645389557\n",
      "Iteration: 2223. Loss: 0.0722198486328125\n",
      "Iteration: 2224. Loss: 0.1861933171749115\n",
      "Iteration: 2225. Loss: 0.18223509192466736\n",
      "Iteration: 2226. Loss: 0.24545682966709137\n",
      "Iteration: 2227. Loss: 0.14781539142131805\n",
      "Iteration: 2228. Loss: 0.13433457911014557\n",
      "Iteration: 2229. Loss: 0.2510499954223633\n",
      "Iteration: 2230. Loss: 0.14322423934936523\n",
      "Iteration: 2231. Loss: 0.11908838152885437\n",
      "Iteration: 2232. Loss: 0.22028924524784088\n",
      "Iteration: 2233. Loss: 0.14401130378246307\n",
      "Iteration: 2234. Loss: 0.16544750332832336\n",
      "Iteration: 2235. Loss: 0.21732331812381744\n",
      "Iteration: 2236. Loss: 0.1402614563703537\n",
      "Iteration: 2237. Loss: 0.23983538150787354\n",
      "Iteration: 2238. Loss: 0.2244339883327484\n",
      "Iteration: 2239. Loss: 0.1360265612602234\n",
      "Iteration: 2240. Loss: 0.08759330958127975\n",
      "Iteration: 2241. Loss: 0.2869745194911957\n",
      "Iteration: 2242. Loss: 0.17774009704589844\n",
      "Iteration: 2243. Loss: 0.129380464553833\n",
      "Iteration: 2244. Loss: 0.28911086916923523\n",
      "Iteration: 2245. Loss: 0.13870705664157867\n",
      "Iteration: 2246. Loss: 0.17700208723545074\n",
      "Iteration: 2247. Loss: 0.09077627956867218\n",
      "Iteration: 2248. Loss: 0.177011638879776\n",
      "Iteration: 2249. Loss: 0.16971784830093384\n",
      "Iteration: 2250. Loss: 0.19701705873012543\n",
      "Iteration: 2251. Loss: 0.19152520596981049\n",
      "Iteration: 2252. Loss: 0.15751346945762634\n",
      "Iteration: 2253. Loss: 0.11322001367807388\n",
      "Iteration: 2254. Loss: 0.21856418251991272\n",
      "Iteration: 2255. Loss: 0.30665287375450134\n",
      "Iteration: 2256. Loss: 0.1151084154844284\n",
      "Iteration: 2257. Loss: 0.2493358552455902\n",
      "Iteration: 2258. Loss: 0.1789451390504837\n",
      "Iteration: 2259. Loss: 0.15127600729465485\n",
      "Iteration: 2260. Loss: 0.08895298093557358\n",
      "Iteration: 2261. Loss: 0.13557098805904388\n",
      "Iteration: 2262. Loss: 0.1687762290239334\n",
      "Iteration: 2263. Loss: 0.12764976918697357\n",
      "Iteration: 2264. Loss: 0.18779216706752777\n",
      "Iteration: 2265. Loss: 0.19354300200939178\n",
      "Iteration: 2266. Loss: 0.1630735546350479\n",
      "Iteration: 2267. Loss: 0.11285553127527237\n",
      "Iteration: 2268. Loss: 0.23939983546733856\n",
      "Iteration: 2269. Loss: 0.21789126098155975\n",
      "Iteration: 2270. Loss: 0.09036796540021896\n",
      "Iteration: 2271. Loss: 0.13185599446296692\n",
      "Iteration: 2272. Loss: 0.24616491794586182\n",
      "Iteration: 2273. Loss: 0.1308438777923584\n",
      "Iteration: 2274. Loss: 0.16279157996177673\n",
      "Iteration: 2275. Loss: 0.17927345633506775\n",
      "Iteration: 2276. Loss: 0.2084970474243164\n",
      "Iteration: 2277. Loss: 0.08487334102392197\n",
      "Iteration: 2278. Loss: 0.14238591492176056\n",
      "Iteration: 2279. Loss: 0.09936604648828506\n",
      "Iteration: 2280. Loss: 0.1663740575313568\n",
      "Iteration: 2281. Loss: 0.1519993543624878\n",
      "Iteration: 2282. Loss: 0.2552575170993805\n",
      "Iteration: 2283. Loss: 0.20659561455249786\n",
      "Iteration: 2284. Loss: 0.17889434099197388\n",
      "Iteration: 2285. Loss: 0.2270224690437317\n",
      "Iteration: 2286. Loss: 0.16288898885250092\n",
      "Iteration: 2287. Loss: 0.11764072626829147\n",
      "Iteration: 2288. Loss: 0.13982048630714417\n",
      "Iteration: 2289. Loss: 0.1368895322084427\n",
      "Iteration: 2290. Loss: 0.24068714678287506\n",
      "Iteration: 2291. Loss: 0.17983666062355042\n",
      "Iteration: 2292. Loss: 0.1421554982662201\n",
      "Iteration: 2293. Loss: 0.0964016541838646\n",
      "Iteration: 2294. Loss: 0.18798351287841797\n",
      "Iteration: 2295. Loss: 0.17510908842086792\n",
      "Iteration: 2296. Loss: 0.11326850205659866\n",
      "Iteration: 2297. Loss: 0.10104520618915558\n",
      "Iteration: 2298. Loss: 0.21833892166614532\n",
      "Iteration: 2299. Loss: 0.16759377717971802\n",
      "Iteration: 2300. Loss: 0.15395350754261017\n",
      "Iteration: 2301. Loss: 0.15035146474838257\n",
      "Iteration: 2302. Loss: 0.19446289539337158\n",
      "Iteration: 2303. Loss: 0.2203349620103836\n",
      "Iteration: 2304. Loss: 0.14448320865631104\n",
      "Iteration: 2305. Loss: 0.30731967091560364\n",
      "Iteration: 2306. Loss: 0.12901736795902252\n",
      "Iteration: 2307. Loss: 0.20758739113807678\n",
      "Iteration: 2308. Loss: 0.1526721566915512\n",
      "Iteration: 2309. Loss: 0.19644129276275635\n",
      "Iteration: 2310. Loss: 0.06436939537525177\n",
      "Iteration: 2311. Loss: 0.18756143748760223\n",
      "Iteration: 2312. Loss: 0.25809550285339355\n",
      "Iteration: 2313. Loss: 0.23390668630599976\n",
      "Iteration: 2314. Loss: 0.11493700742721558\n",
      "Iteration: 2315. Loss: 0.12702633440494537\n",
      "Iteration: 2316. Loss: 0.10856460779905319\n",
      "Iteration: 2317. Loss: 0.3400672674179077\n",
      "Iteration: 2318. Loss: 0.2754092514514923\n",
      "Iteration: 2319. Loss: 0.2804228365421295\n",
      "Iteration: 2320. Loss: 0.17569145560264587\n",
      "Iteration: 2321. Loss: 0.1768263429403305\n",
      "Iteration: 2322. Loss: 0.16482920944690704\n",
      "Iteration: 2323. Loss: 0.19084224104881287\n",
      "Iteration: 2324. Loss: 0.1913304328918457\n",
      "Iteration: 2325. Loss: 0.21424873173236847\n",
      "Iteration: 2326. Loss: 0.201584130525589\n",
      "Iteration: 2327. Loss: 0.07597529143095016\n",
      "Iteration: 2328. Loss: 0.11271588504314423\n",
      "Iteration: 2329. Loss: 0.1081174686551094\n",
      "Iteration: 2330. Loss: 0.24337276816368103\n",
      "Iteration: 2331. Loss: 0.16190093755722046\n",
      "Iteration: 2332. Loss: 0.13946786522865295\n",
      "Iteration: 2333. Loss: 0.11419365555047989\n",
      "Iteration: 2334. Loss: 0.06998562067747116\n",
      "Iteration: 2335. Loss: 0.14344720542430878\n",
      "Iteration: 2336. Loss: 0.10115984082221985\n",
      "Iteration: 2337. Loss: 0.28022220730781555\n",
      "Iteration: 2338. Loss: 0.24542801082134247\n",
      "Iteration: 2339. Loss: 0.07632113248109818\n",
      "Iteration: 2340. Loss: 0.4830206036567688\n",
      "Iteration: 2341. Loss: 0.1739215850830078\n",
      "Iteration: 2342. Loss: 0.18437238037586212\n",
      "Iteration: 2343. Loss: 0.2583473324775696\n",
      "Iteration: 2344. Loss: 0.13278694450855255\n",
      "Iteration: 2345. Loss: 0.19968149065971375\n",
      "Iteration: 2346. Loss: 0.16703632473945618\n",
      "Iteration: 2347. Loss: 0.1390557438135147\n",
      "Iteration: 2348. Loss: 0.13414810597896576\n",
      "Iteration: 2349. Loss: 0.09729836136102676\n",
      "Iteration: 2350. Loss: 0.08865522593259811\n",
      "Iteration: 2351. Loss: 0.12158389389514923\n",
      "Iteration: 2352. Loss: 0.2520335018634796\n",
      "Iteration: 2353. Loss: 0.07431776821613312\n",
      "Iteration: 2354. Loss: 0.1925981640815735\n",
      "Iteration: 2355. Loss: 0.18935143947601318\n",
      "Iteration: 2356. Loss: 0.15926900506019592\n",
      "Iteration: 2357. Loss: 0.12257714569568634\n",
      "Iteration: 2358. Loss: 0.13815103471279144\n",
      "Iteration: 2359. Loss: 0.17164793610572815\n",
      "Iteration: 2360. Loss: 0.14709414541721344\n",
      "Iteration: 2361. Loss: 0.1610867828130722\n",
      "Iteration: 2362. Loss: 0.25199806690216064\n",
      "Iteration: 2363. Loss: 0.2500366270542145\n",
      "Iteration: 2364. Loss: 0.17505653202533722\n",
      "Iteration: 2365. Loss: 0.12817008793354034\n",
      "Iteration: 2366. Loss: 0.20771360397338867\n",
      "Iteration: 2367. Loss: 0.1452401727437973\n",
      "Iteration: 2368. Loss: 0.16927386820316315\n",
      "Iteration: 2369. Loss: 0.09014639258384705\n",
      "Iteration: 2370. Loss: 0.20127059519290924\n",
      "Iteration: 2371. Loss: 0.1457483321428299\n",
      "Iteration: 2372. Loss: 0.14444421231746674\n",
      "Iteration: 2373. Loss: 0.16986504197120667\n",
      "Iteration: 2374. Loss: 0.17513729631900787\n",
      "Iteration: 2375. Loss: 0.15033559501171112\n",
      "Iteration: 2376. Loss: 0.12113871425390244\n",
      "Iteration: 2377. Loss: 0.16076460480690002\n",
      "Iteration: 2378. Loss: 0.1928808093070984\n",
      "Iteration: 2379. Loss: 0.1571889966726303\n",
      "Iteration: 2380. Loss: 0.08989357948303223\n",
      "Iteration: 2381. Loss: 0.2026825100183487\n",
      "Iteration: 2382. Loss: 0.2350921481847763\n",
      "Iteration: 2383. Loss: 0.2208871692419052\n",
      "Iteration: 2384. Loss: 0.2404395341873169\n",
      "Iteration: 2385. Loss: 0.1826496124267578\n",
      "Iteration: 2386. Loss: 0.06615819782018661\n",
      "Iteration: 2387. Loss: 0.2649267315864563\n",
      "Iteration: 2388. Loss: 0.1630943864583969\n",
      "Iteration: 2389. Loss: 0.17132893204689026\n",
      "Iteration: 2390. Loss: 0.2560536563396454\n",
      "Iteration: 2391. Loss: 0.18295535445213318\n",
      "Iteration: 2392. Loss: 0.12153144925832748\n",
      "Iteration: 2393. Loss: 0.2011469304561615\n",
      "Iteration: 2394. Loss: 0.11558563262224197\n",
      "Iteration: 2395. Loss: 0.22039508819580078\n",
      "Iteration: 2396. Loss: 0.09562318027019501\n",
      "Iteration: 2397. Loss: 0.2257218360900879\n",
      "Iteration: 2398. Loss: 0.2586742043495178\n",
      "Iteration: 2399. Loss: 0.2134242206811905\n",
      "Iteration: 2400. Loss: 0.15249232947826385\n",
      "Iteration: 2401. Loss: 0.07153519988059998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2402. Loss: 0.13262048363685608\n",
      "Iteration: 2403. Loss: 0.10716726630926132\n",
      "Iteration: 2404. Loss: 0.13295511901378632\n",
      "Iteration: 2405. Loss: 0.19110575318336487\n",
      "Iteration: 2406. Loss: 0.14108160138130188\n",
      "Iteration: 2407. Loss: 0.22115600109100342\n",
      "Iteration: 2408. Loss: 0.15860013663768768\n",
      "Iteration: 2409. Loss: 0.16200672090053558\n",
      "Iteration: 2410. Loss: 0.31049492955207825\n",
      "Iteration: 2411. Loss: 0.2641516923904419\n",
      "Iteration: 2412. Loss: 0.16929058730602264\n",
      "Iteration: 2413. Loss: 0.11028463393449783\n",
      "Iteration: 2414. Loss: 0.13789358735084534\n",
      "Iteration: 2415. Loss: 0.18767976760864258\n",
      "Iteration: 2416. Loss: 0.15068426728248596\n",
      "Iteration: 2417. Loss: 0.07807954400777817\n",
      "Iteration: 2418. Loss: 0.19171804189682007\n",
      "Iteration: 2419. Loss: 0.27916041016578674\n",
      "Iteration: 2420. Loss: 0.15740135312080383\n",
      "Iteration: 2421. Loss: 0.46648460626602173\n",
      "Iteration: 2422. Loss: 0.102653369307518\n",
      "Iteration: 2423. Loss: 0.10008301585912704\n",
      "Iteration: 2424. Loss: 0.09032284468412399\n",
      "Iteration: 2425. Loss: 0.17019061744213104\n",
      "Iteration: 2426. Loss: 0.11168906837701797\n",
      "Iteration: 2427. Loss: 0.17586450278759003\n",
      "Iteration: 2428. Loss: 0.13141554594039917\n",
      "Iteration: 2429. Loss: 0.15248650312423706\n",
      "Iteration: 2430. Loss: 0.1522960364818573\n",
      "Iteration: 2431. Loss: 0.12194442003965378\n",
      "Iteration: 2432. Loss: 0.12148062884807587\n",
      "Iteration: 2433. Loss: 0.2230576127767563\n",
      "Iteration: 2434. Loss: 0.20064695179462433\n",
      "Iteration: 2435. Loss: 0.10172972828149796\n",
      "Iteration: 2436. Loss: 0.1721530705690384\n",
      "Iteration: 2437. Loss: 0.1625920683145523\n",
      "Iteration: 2438. Loss: 0.16429907083511353\n",
      "Iteration: 2439. Loss: 0.20777438580989838\n",
      "Iteration: 2440. Loss: 0.1177540197968483\n",
      "Iteration: 2441. Loss: 0.13835829496383667\n",
      "Iteration: 2442. Loss: 0.27939268946647644\n",
      "Iteration: 2443. Loss: 0.10954702645540237\n",
      "Iteration: 2444. Loss: 0.1805046647787094\n",
      "Iteration: 2445. Loss: 0.1967528909444809\n",
      "Iteration: 2446. Loss: 0.06945186853408813\n",
      "Iteration: 2447. Loss: 0.2497428059577942\n",
      "Iteration: 2448. Loss: 0.13921964168548584\n",
      "Iteration: 2449. Loss: 0.18947069346904755\n",
      "Iteration: 2450. Loss: 0.18283788859844208\n",
      "Iteration: 2451. Loss: 0.11431333422660828\n",
      "Iteration: 2452. Loss: 0.1258295625448227\n",
      "Iteration: 2453. Loss: 0.17375235259532928\n",
      "Iteration: 2454. Loss: 0.10643713176250458\n",
      "Iteration: 2455. Loss: 0.17729806900024414\n",
      "Iteration: 2456. Loss: 0.20216535031795502\n",
      "Iteration: 2457. Loss: 0.12206894159317017\n",
      "Iteration: 2458. Loss: 0.29982542991638184\n",
      "Iteration: 2459. Loss: 0.16763049364089966\n",
      "Iteration: 2460. Loss: 0.15526574850082397\n",
      "Iteration: 2461. Loss: 0.1781049519777298\n",
      "Iteration: 2462. Loss: 0.2935894727706909\n",
      "Iteration: 2463. Loss: 0.2516573667526245\n",
      "Iteration: 2464. Loss: 0.13563239574432373\n",
      "Iteration: 2465. Loss: 0.055160947144031525\n",
      "Iteration: 2466. Loss: 0.17711381614208221\n",
      "Iteration: 2467. Loss: 0.1939155012369156\n",
      "Iteration: 2468. Loss: 0.0812743678689003\n",
      "Iteration: 2469. Loss: 0.17892053723335266\n",
      "Iteration: 2470. Loss: 0.08985419571399689\n",
      "Iteration: 2471. Loss: 0.19060903787612915\n",
      "Iteration: 2472. Loss: 0.1223902478814125\n",
      "Iteration: 2473. Loss: 0.12815356254577637\n",
      "Iteration: 2474. Loss: 0.3178692162036896\n",
      "Iteration: 2475. Loss: 0.14299364387989044\n",
      "Iteration: 2476. Loss: 0.23506806790828705\n",
      "Iteration: 2477. Loss: 0.1694764941930771\n",
      "Iteration: 2478. Loss: 0.1450224369764328\n",
      "Iteration: 2479. Loss: 0.31794846057891846\n",
      "Iteration: 2480. Loss: 0.09185329079627991\n",
      "Iteration: 2481. Loss: 0.2364628165960312\n",
      "Iteration: 2482. Loss: 0.14215335249900818\n",
      "Iteration: 2483. Loss: 0.1684485375881195\n",
      "Iteration: 2484. Loss: 0.2434452772140503\n",
      "Iteration: 2485. Loss: 0.14546634256839752\n",
      "Iteration: 2486. Loss: 0.15907640755176544\n",
      "Iteration: 2487. Loss: 0.16156700253486633\n",
      "Iteration: 2488. Loss: 0.12698259949684143\n",
      "Iteration: 2489. Loss: 0.12819641828536987\n",
      "Iteration: 2490. Loss: 0.19180938601493835\n",
      "Iteration: 2491. Loss: 0.2868826985359192\n",
      "Iteration: 2492. Loss: 0.18490983545780182\n",
      "Iteration: 2493. Loss: 0.1951301544904709\n",
      "Iteration: 2494. Loss: 0.1993967443704605\n",
      "Iteration: 2495. Loss: 0.10665658861398697\n",
      "Iteration: 2496. Loss: 0.2774324417114258\n",
      "Iteration: 2497. Loss: 0.10848098993301392\n",
      "Iteration: 2498. Loss: 0.1971997767686844\n",
      "Iteration: 2499. Loss: 0.0928669199347496\n",
      "Iteration: 2500. Loss: 0.16304321587085724\n",
      "Iteration: 2501. Loss: 0.2532570958137512\n",
      "Iteration: 2502. Loss: 0.2741045355796814\n",
      "Iteration: 2503. Loss: 0.15893390774726868\n",
      "Iteration: 2504. Loss: 0.14851370453834534\n",
      "Iteration: 2505. Loss: 0.11501451581716537\n",
      "Iteration: 2506. Loss: 0.0941200852394104\n",
      "Iteration: 2507. Loss: 0.144271120429039\n",
      "Iteration: 2508. Loss: 0.11354818195104599\n",
      "Iteration: 2509. Loss: 0.1575748771429062\n",
      "Iteration: 2510. Loss: 0.23082013428211212\n",
      "Iteration: 2511. Loss: 0.20101024210453033\n",
      "Iteration: 2512. Loss: 0.1623862236738205\n",
      "Iteration: 2513. Loss: 0.18403974175453186\n",
      "Iteration: 2514. Loss: 0.07235438376665115\n",
      "Iteration: 2515. Loss: 0.1158176064491272\n",
      "Iteration: 2516. Loss: 0.19470112025737762\n",
      "Iteration: 2517. Loss: 0.22399482131004333\n",
      "Iteration: 2518. Loss: 0.1800374835729599\n",
      "Iteration: 2519. Loss: 0.13891226053237915\n",
      "Iteration: 2520. Loss: 0.11547690629959106\n",
      "Iteration: 2521. Loss: 0.18023258447647095\n",
      "Iteration: 2522. Loss: 0.15824595093727112\n",
      "Iteration: 2523. Loss: 0.162958025932312\n",
      "Iteration: 2524. Loss: 0.22971533238887787\n",
      "Iteration: 2525. Loss: 0.22575591504573822\n",
      "Iteration: 2526. Loss: 0.08058854937553406\n",
      "Iteration: 2527. Loss: 0.2500744163990021\n",
      "Iteration: 2528. Loss: 0.12669283151626587\n",
      "Iteration: 2529. Loss: 0.1309909224510193\n",
      "Iteration: 2530. Loss: 0.11750789731740952\n",
      "Iteration: 2531. Loss: 0.1649487316608429\n",
      "Iteration: 2532. Loss: 0.16227874159812927\n",
      "Iteration: 2533. Loss: 0.13388366997241974\n",
      "Iteration: 2534. Loss: 0.2763570547103882\n",
      "Iteration: 2535. Loss: 0.10241764038801193\n",
      "Iteration: 2536. Loss: 0.15990687906742096\n",
      "Iteration: 2537. Loss: 0.15941476821899414\n",
      "Iteration: 2538. Loss: 0.23935790359973907\n",
      "Iteration: 2539. Loss: 0.23712687194347382\n",
      "Iteration: 2540. Loss: 0.14862465858459473\n",
      "Iteration: 2541. Loss: 0.11716607213020325\n",
      "Iteration: 2542. Loss: 0.26101765036582947\n",
      "Iteration: 2543. Loss: 0.12731163203716278\n",
      "Iteration: 2544. Loss: 0.18378490209579468\n",
      "Iteration: 2545. Loss: 0.07180124521255493\n",
      "Iteration: 2546. Loss: 0.13044849038124084\n",
      "Iteration: 2547. Loss: 0.09615035355091095\n",
      "Iteration: 2548. Loss: 0.3030169606208801\n",
      "Iteration: 2549. Loss: 0.08341589570045471\n",
      "Iteration: 2550. Loss: 0.060491420328617096\n",
      "Iteration: 2551. Loss: 0.14219199120998383\n",
      "Iteration: 2552. Loss: 0.2100190967321396\n",
      "Iteration: 2553. Loss: 0.07943522930145264\n",
      "Iteration: 2554. Loss: 0.1417294144630432\n",
      "Iteration: 2555. Loss: 0.1371404528617859\n",
      "Iteration: 2556. Loss: 0.14547008275985718\n",
      "Iteration: 2557. Loss: 0.14509069919586182\n",
      "Iteration: 2558. Loss: 0.21382132172584534\n",
      "Iteration: 2559. Loss: 0.20428285002708435\n",
      "Iteration: 2560. Loss: 0.18650491535663605\n",
      "Iteration: 2561. Loss: 0.15936890244483948\n",
      "Iteration: 2562. Loss: 0.09609022736549377\n",
      "Iteration: 2563. Loss: 0.0989992544054985\n",
      "Iteration: 2564. Loss: 0.08373787999153137\n",
      "Iteration: 2565. Loss: 0.10423898696899414\n",
      "Iteration: 2566. Loss: 0.1361667662858963\n",
      "Iteration: 2567. Loss: 0.22319982945919037\n",
      "Iteration: 2568. Loss: 0.17500187456607819\n",
      "Iteration: 2569. Loss: 0.16598856449127197\n",
      "Iteration: 2570. Loss: 0.13664080202579498\n",
      "Iteration: 2571. Loss: 0.15010562539100647\n",
      "Iteration: 2572. Loss: 0.0657997727394104\n",
      "Iteration: 2573. Loss: 0.2892090082168579\n",
      "Iteration: 2574. Loss: 0.22630128264427185\n",
      "Iteration: 2575. Loss: 0.18018054962158203\n",
      "Iteration: 2576. Loss: 0.13194486498832703\n",
      "Iteration: 2577. Loss: 0.19219468533992767\n",
      "Iteration: 2578. Loss: 0.08321163058280945\n",
      "Iteration: 2579. Loss: 0.14081138372421265\n",
      "Iteration: 2580. Loss: 0.24945774674415588\n",
      "Iteration: 2581. Loss: 0.16315408051013947\n",
      "Iteration: 2582. Loss: 0.12492866814136505\n",
      "Iteration: 2583. Loss: 0.2983996570110321\n",
      "Iteration: 2584. Loss: 0.1053425595164299\n",
      "Iteration: 2585. Loss: 0.12759503722190857\n",
      "Iteration: 2586. Loss: 0.13693004846572876\n",
      "Iteration: 2587. Loss: 0.11869487911462784\n",
      "Iteration: 2588. Loss: 0.10764233767986298\n",
      "Iteration: 2589. Loss: 0.09169171005487442\n",
      "Iteration: 2590. Loss: 0.1512940376996994\n",
      "Iteration: 2591. Loss: 0.29524004459381104\n",
      "Iteration: 2592. Loss: 0.26011353731155396\n",
      "Iteration: 2593. Loss: 0.09494064003229141\n",
      "Iteration: 2594. Loss: 0.08679474890232086\n",
      "Iteration: 2595. Loss: 0.21622319519519806\n",
      "Iteration: 2596. Loss: 0.060037512332201004\n",
      "Iteration: 2597. Loss: 0.096110999584198\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2598. Loss: 0.19112282991409302\n",
      "Iteration: 2599. Loss: 0.13659274578094482\n",
      "Iteration: 2600. Loss: 0.07510467618703842\n",
      "Iteration: 2601. Loss: 0.18690699338912964\n",
      "Iteration: 2602. Loss: 0.1721367985010147\n",
      "Iteration: 2603. Loss: 0.12056023627519608\n",
      "Iteration: 2604. Loss: 0.15273594856262207\n",
      "Iteration: 2605. Loss: 0.1707308143377304\n",
      "Iteration: 2606. Loss: 0.2931100130081177\n",
      "Iteration: 2607. Loss: 0.13247457146644592\n",
      "Iteration: 2608. Loss: 0.06396307796239853\n",
      "Iteration: 2609. Loss: 0.1742374449968338\n",
      "Iteration: 2610. Loss: 0.07580248266458511\n",
      "Iteration: 2611. Loss: 0.26223573088645935\n",
      "Iteration: 2612. Loss: 0.30909866094589233\n",
      "Iteration: 2613. Loss: 0.16351108253002167\n",
      "Iteration: 2614. Loss: 0.070012666285038\n",
      "Iteration: 2615. Loss: 0.18936091661453247\n",
      "Iteration: 2616. Loss: 0.14219698309898376\n",
      "Iteration: 2617. Loss: 0.14461639523506165\n",
      "Iteration: 2618. Loss: 0.10766172409057617\n",
      "Iteration: 2619. Loss: 0.07996979355812073\n",
      "Iteration: 2620. Loss: 0.18781456351280212\n",
      "Iteration: 2621. Loss: 0.10729958862066269\n",
      "Iteration: 2622. Loss: 0.19345639646053314\n",
      "Iteration: 2623. Loss: 0.15201668441295624\n",
      "Iteration: 2624. Loss: 0.19427181780338287\n",
      "Iteration: 2625. Loss: 0.135369211435318\n",
      "Iteration: 2626. Loss: 0.16675277054309845\n",
      "Iteration: 2627. Loss: 0.10168084502220154\n",
      "Iteration: 2628. Loss: 0.16918325424194336\n",
      "Iteration: 2629. Loss: 0.12489752471446991\n",
      "Iteration: 2630. Loss: 0.20571285486221313\n",
      "Iteration: 2631. Loss: 0.20990146696567535\n",
      "Iteration: 2632. Loss: 0.12913556396961212\n",
      "Iteration: 2633. Loss: 0.15113715827465057\n",
      "Iteration: 2634. Loss: 0.1881367564201355\n",
      "Iteration: 2635. Loss: 0.13977959752082825\n",
      "Iteration: 2636. Loss: 0.1991758495569229\n",
      "Iteration: 2637. Loss: 0.12665927410125732\n",
      "Iteration: 2638. Loss: 0.1231914535164833\n",
      "Iteration: 2639. Loss: 0.09311062097549438\n",
      "Iteration: 2640. Loss: 0.2026764303445816\n",
      "Iteration: 2641. Loss: 0.20724396407604218\n",
      "Iteration: 2642. Loss: 0.13584858179092407\n",
      "Iteration: 2643. Loss: 0.12644453346729279\n",
      "Iteration: 2644. Loss: 0.12571211159229279\n",
      "Iteration: 2645. Loss: 0.14026063680648804\n",
      "Iteration: 2646. Loss: 0.06271873414516449\n",
      "Iteration: 2647. Loss: 0.08316178619861603\n",
      "Iteration: 2648. Loss: 0.1322057545185089\n",
      "Iteration: 2649. Loss: 0.08482030779123306\n",
      "Iteration: 2650. Loss: 0.1953093707561493\n",
      "Iteration: 2651. Loss: 0.11430852115154266\n",
      "Iteration: 2652. Loss: 0.1197839081287384\n",
      "Iteration: 2653. Loss: 0.07415486872196198\n",
      "Iteration: 2654. Loss: 0.09041831642389297\n",
      "Iteration: 2655. Loss: 0.09645338356494904\n",
      "Iteration: 2656. Loss: 0.061475418508052826\n",
      "Iteration: 2657. Loss: 0.14910778403282166\n",
      "Iteration: 2658. Loss: 0.15065498650074005\n",
      "Iteration: 2659. Loss: 0.29052647948265076\n",
      "Iteration: 2660. Loss: 0.14139516651630402\n",
      "Iteration: 2661. Loss: 0.15990769863128662\n",
      "Iteration: 2662. Loss: 0.17598895728588104\n",
      "Iteration: 2663. Loss: 0.22978006303310394\n",
      "Iteration: 2664. Loss: 0.13672441244125366\n",
      "Iteration: 2665. Loss: 0.32776764035224915\n",
      "Iteration: 2666. Loss: 0.24025718867778778\n",
      "Iteration: 2667. Loss: 0.16102765500545502\n",
      "Iteration: 2668. Loss: 0.10201393067836761\n",
      "Iteration: 2669. Loss: 0.14521068334579468\n",
      "Iteration: 2670. Loss: 0.1191336140036583\n",
      "Iteration: 2671. Loss: 0.14587131142616272\n",
      "Iteration: 2672. Loss: 0.20980417728424072\n",
      "Iteration: 2673. Loss: 0.04939406365156174\n",
      "Iteration: 2674. Loss: 0.07384312897920609\n",
      "Iteration: 2675. Loss: 0.10476106405258179\n",
      "Iteration: 2676. Loss: 0.11389012634754181\n",
      "Iteration: 2677. Loss: 0.061596132814884186\n",
      "Iteration: 2678. Loss: 0.16826945543289185\n",
      "Iteration: 2679. Loss: 0.18270954489707947\n",
      "Iteration: 2680. Loss: 0.08214041590690613\n",
      "Iteration: 2681. Loss: 0.1328107863664627\n",
      "Iteration: 2682. Loss: 0.18878942728042603\n",
      "Iteration: 2683. Loss: 0.17467442154884338\n",
      "Iteration: 2684. Loss: 0.08369504660367966\n",
      "Iteration: 2685. Loss: 0.18366888165473938\n",
      "Iteration: 2686. Loss: 0.1424928456544876\n",
      "Iteration: 2687. Loss: 0.20973408222198486\n",
      "Iteration: 2688. Loss: 0.15440687537193298\n",
      "Iteration: 2689. Loss: 0.10557740181684494\n",
      "Iteration: 2690. Loss: 0.21144357323646545\n",
      "Iteration: 2691. Loss: 0.17209327220916748\n",
      "Iteration: 2692. Loss: 0.23445852100849152\n",
      "Iteration: 2693. Loss: 0.23241378366947174\n",
      "Iteration: 2694. Loss: 0.19789505004882812\n",
      "Iteration: 2695. Loss: 0.2126237154006958\n",
      "Iteration: 2696. Loss: 0.0654940977692604\n",
      "Iteration: 2697. Loss: 0.23806056380271912\n",
      "Iteration: 2698. Loss: 0.12154173105955124\n",
      "Iteration: 2699. Loss: 0.1765253245830536\n",
      "Iteration: 2700. Loss: 0.09981659799814224\n",
      "Iteration: 2701. Loss: 0.14803579449653625\n",
      "Iteration: 2702. Loss: 0.12948045134544373\n",
      "Iteration: 2703. Loss: 0.08385257422924042\n",
      "Iteration: 2704. Loss: 0.10591817647218704\n",
      "Iteration: 2705. Loss: 0.11031215637922287\n",
      "Iteration: 2706. Loss: 0.16141511499881744\n",
      "Iteration: 2707. Loss: 0.23459993302822113\n",
      "Iteration: 2708. Loss: 0.0941036120057106\n",
      "Iteration: 2709. Loss: 0.06457585096359253\n",
      "Iteration: 2710. Loss: 0.1903197169303894\n",
      "Iteration: 2711. Loss: 0.1990220993757248\n",
      "Iteration: 2712. Loss: 0.15570610761642456\n",
      "Iteration: 2713. Loss: 0.10233619064092636\n",
      "Iteration: 2714. Loss: 0.12933288514614105\n",
      "Iteration: 2715. Loss: 0.12316032499074936\n",
      "Iteration: 2716. Loss: 0.20955243706703186\n",
      "Iteration: 2717. Loss: 0.0808546245098114\n",
      "Iteration: 2718. Loss: 0.14428754150867462\n",
      "Iteration: 2719. Loss: 0.08731013536453247\n",
      "Iteration: 2720. Loss: 0.11962305754423141\n",
      "Iteration: 2721. Loss: 0.12992335855960846\n",
      "Iteration: 2722. Loss: 0.21951395273208618\n",
      "Iteration: 2723. Loss: 0.2445448935031891\n",
      "Iteration: 2724. Loss: 0.14298242330551147\n",
      "Iteration: 2725. Loss: 0.08197570592164993\n",
      "Iteration: 2726. Loss: 0.10102149844169617\n",
      "Iteration: 2727. Loss: 0.1423923522233963\n",
      "Iteration: 2728. Loss: 0.152232363820076\n",
      "Iteration: 2729. Loss: 0.13086923956871033\n",
      "Iteration: 2730. Loss: 0.26710888743400574\n",
      "Iteration: 2731. Loss: 0.08889757841825485\n",
      "Iteration: 2732. Loss: 0.3187970221042633\n",
      "Iteration: 2733. Loss: 0.11182501167058945\n",
      "Iteration: 2734. Loss: 0.15692047774791718\n",
      "Iteration: 2735. Loss: 0.174705371260643\n",
      "Iteration: 2736. Loss: 0.14201131463050842\n",
      "Iteration: 2737. Loss: 0.14734506607055664\n",
      "Iteration: 2738. Loss: 0.15621957182884216\n",
      "Iteration: 2739. Loss: 0.21397244930267334\n",
      "Iteration: 2740. Loss: 0.11007281392812729\n",
      "Iteration: 2741. Loss: 0.2240835428237915\n",
      "Iteration: 2742. Loss: 0.3110729455947876\n",
      "Iteration: 2743. Loss: 0.22357234358787537\n",
      "Iteration: 2744. Loss: 0.14329902827739716\n",
      "Iteration: 2745. Loss: 0.1215277761220932\n",
      "Iteration: 2746. Loss: 0.05416764318943024\n",
      "Iteration: 2747. Loss: 0.1003209799528122\n",
      "Iteration: 2748. Loss: 0.3315662443637848\n",
      "Iteration: 2749. Loss: 0.15540023148059845\n",
      "Iteration: 2750. Loss: 0.13657820224761963\n",
      "Iteration: 2751. Loss: 0.17151160538196564\n",
      "Iteration: 2752. Loss: 0.08037693798542023\n",
      "Iteration: 2753. Loss: 0.10232196748256683\n",
      "Iteration: 2754. Loss: 0.10818596929311752\n",
      "Iteration: 2755. Loss: 0.15959282219409943\n",
      "Iteration: 2756. Loss: 0.15503673255443573\n",
      "Iteration: 2757. Loss: 0.16335295140743256\n",
      "Iteration: 2758. Loss: 0.10709945857524872\n",
      "Iteration: 2759. Loss: 0.14375044405460358\n",
      "Iteration: 2760. Loss: 0.2577992379665375\n",
      "Iteration: 2761. Loss: 0.16805656254291534\n",
      "Iteration: 2762. Loss: 0.11076340079307556\n",
      "Iteration: 2763. Loss: 0.18655088543891907\n",
      "Iteration: 2764. Loss: 0.08795149624347687\n",
      "Iteration: 2765. Loss: 0.1073949784040451\n",
      "Iteration: 2766. Loss: 0.20994199812412262\n",
      "Iteration: 2767. Loss: 0.07998824864625931\n",
      "Iteration: 2768. Loss: 0.2433648258447647\n",
      "Iteration: 2769. Loss: 0.11171722412109375\n",
      "Iteration: 2770. Loss: 0.12446420639753342\n",
      "Iteration: 2771. Loss: 0.11477188020944595\n",
      "Iteration: 2772. Loss: 0.09305617213249207\n",
      "Iteration: 2773. Loss: 0.17901641130447388\n",
      "Iteration: 2774. Loss: 0.06781357526779175\n",
      "Iteration: 2775. Loss: 0.16466793417930603\n",
      "Iteration: 2776. Loss: 0.13476254045963287\n",
      "Iteration: 2777. Loss: 0.16091316938400269\n",
      "Iteration: 2778. Loss: 0.21754209697246552\n",
      "Iteration: 2779. Loss: 0.14709463715553284\n",
      "Iteration: 2780. Loss: 0.12279564142227173\n",
      "Iteration: 2781. Loss: 0.10276473313570023\n",
      "Iteration: 2782. Loss: 0.10340294986963272\n",
      "Iteration: 2783. Loss: 0.24022196233272552\n",
      "Iteration: 2784. Loss: 0.15626512467861176\n",
      "Iteration: 2785. Loss: 0.07533370703458786\n",
      "Iteration: 2786. Loss: 0.14177848398685455\n",
      "Iteration: 2787. Loss: 0.14886073768138885\n",
      "Iteration: 2788. Loss: 0.15447895228862762\n",
      "Iteration: 2789. Loss: 0.10245805978775024\n",
      "Iteration: 2790. Loss: 0.05035315081477165\n",
      "Iteration: 2791. Loss: 0.20628152787685394\n",
      "Iteration: 2792. Loss: 0.09432236850261688\n",
      "Iteration: 2793. Loss: 0.16208705306053162\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2794. Loss: 0.14922058582305908\n",
      "Iteration: 2795. Loss: 0.1683138608932495\n",
      "Iteration: 2796. Loss: 0.08855738490819931\n",
      "Iteration: 2797. Loss: 0.09587714076042175\n",
      "Iteration: 2798. Loss: 0.08234620839357376\n",
      "Iteration: 2799. Loss: 0.18090450763702393\n",
      "Iteration: 2800. Loss: 0.3131481409072876\n",
      "Iteration: 2801. Loss: 0.15103326737880707\n",
      "Iteration: 2802. Loss: 0.16470466554164886\n",
      "Iteration: 2803. Loss: 0.18993481993675232\n",
      "Iteration: 2804. Loss: 0.1765476018190384\n",
      "Iteration: 2805. Loss: 0.15996013581752777\n",
      "Iteration: 2806. Loss: 0.3103102743625641\n",
      "Iteration: 2807. Loss: 0.2510014772415161\n",
      "Iteration: 2808. Loss: 0.1377321183681488\n",
      "Iteration: 2809. Loss: 0.09223433583974838\n",
      "Iteration: 2810. Loss: 0.12324206531047821\n",
      "Iteration: 2811. Loss: 0.21631424129009247\n",
      "Iteration: 2812. Loss: 0.08788102120161057\n",
      "Iteration: 2813. Loss: 0.1983422487974167\n",
      "Iteration: 2814. Loss: 0.2051394283771515\n",
      "Iteration: 2815. Loss: 0.12290869653224945\n",
      "Iteration: 2816. Loss: 0.19296427071094513\n",
      "Iteration: 2817. Loss: 0.10509920865297318\n",
      "Iteration: 2818. Loss: 0.15643468499183655\n",
      "Iteration: 2819. Loss: 0.13507385551929474\n",
      "Iteration: 2820. Loss: 0.10474248975515366\n",
      "Iteration: 2821. Loss: 0.12403027713298798\n",
      "Iteration: 2822. Loss: 0.12643595039844513\n",
      "Iteration: 2823. Loss: 0.0864890068769455\n",
      "Iteration: 2824. Loss: 0.15746688842773438\n",
      "Iteration: 2825. Loss: 0.2046905905008316\n",
      "Iteration: 2826. Loss: 0.2080177366733551\n",
      "Iteration: 2827. Loss: 0.08295819163322449\n",
      "Iteration: 2828. Loss: 0.1769183725118637\n",
      "Iteration: 2829. Loss: 0.21232303977012634\n",
      "Iteration: 2830. Loss: 0.09429866075515747\n",
      "Iteration: 2831. Loss: 0.1452733874320984\n",
      "Iteration: 2832. Loss: 0.08038582652807236\n",
      "Iteration: 2833. Loss: 0.08534038811922073\n",
      "Iteration: 2834. Loss: 0.10810508579015732\n",
      "Iteration: 2835. Loss: 0.22699607908725739\n",
      "Iteration: 2836. Loss: 0.1135801151394844\n",
      "Iteration: 2837. Loss: 0.17333604395389557\n",
      "Iteration: 2838. Loss: 0.12850791215896606\n",
      "Iteration: 2839. Loss: 0.14184579253196716\n",
      "Iteration: 2840. Loss: 0.1444704383611679\n",
      "Iteration: 2841. Loss: 0.12232548743486404\n",
      "Iteration: 2842. Loss: 0.12012087553739548\n",
      "Iteration: 2843. Loss: 0.06110871210694313\n",
      "Iteration: 2844. Loss: 0.08029749244451523\n",
      "Iteration: 2845. Loss: 0.17402805387973785\n",
      "Iteration: 2846. Loss: 0.11863388866186142\n",
      "Iteration: 2847. Loss: 0.13632407784461975\n",
      "Iteration: 2848. Loss: 0.1296910047531128\n",
      "Iteration: 2849. Loss: 0.11288891732692719\n",
      "Iteration: 2850. Loss: 0.10744288563728333\n",
      "Iteration: 2851. Loss: 0.17590740323066711\n",
      "Iteration: 2852. Loss: 0.2133943885564804\n",
      "Iteration: 2853. Loss: 0.11394844949245453\n",
      "Iteration: 2854. Loss: 0.18283802270889282\n",
      "Iteration: 2855. Loss: 0.25073933601379395\n",
      "Iteration: 2856. Loss: 0.19013231992721558\n",
      "Iteration: 2857. Loss: 0.23979929089546204\n",
      "Iteration: 2858. Loss: 0.0898459404706955\n",
      "Iteration: 2859. Loss: 0.13394173979759216\n",
      "Iteration: 2860. Loss: 0.17993436753749847\n",
      "Iteration: 2861. Loss: 0.1294640600681305\n",
      "Iteration: 2862. Loss: 0.25677043199539185\n",
      "Iteration: 2863. Loss: 0.17506536841392517\n",
      "Iteration: 2864. Loss: 0.09877365082502365\n",
      "Iteration: 2865. Loss: 0.09922263771295547\n",
      "Iteration: 2866. Loss: 0.14532743394374847\n",
      "Iteration: 2867. Loss: 0.12588299810886383\n",
      "Iteration: 2868. Loss: 0.15313012897968292\n",
      "Iteration: 2869. Loss: 0.2115737348794937\n",
      "Iteration: 2870. Loss: 0.16140960156917572\n",
      "Iteration: 2871. Loss: 0.08727715164422989\n",
      "Iteration: 2872. Loss: 0.20429645478725433\n",
      "Iteration: 2873. Loss: 0.22397933900356293\n",
      "Iteration: 2874. Loss: 0.124000683426857\n",
      "Iteration: 2875. Loss: 0.07723228633403778\n",
      "Iteration: 2876. Loss: 0.2419513314962387\n",
      "Iteration: 2877. Loss: 0.16275465488433838\n",
      "Iteration: 2878. Loss: 0.10885302722454071\n",
      "Iteration: 2879. Loss: 0.25657686591148376\n",
      "Iteration: 2880. Loss: 0.15451684594154358\n",
      "Iteration: 2881. Loss: 0.15536531805992126\n",
      "Iteration: 2882. Loss: 0.0912155732512474\n",
      "Iteration: 2883. Loss: 0.12283679097890854\n",
      "Iteration: 2884. Loss: 0.19828975200653076\n",
      "Iteration: 2885. Loss: 0.15129108726978302\n",
      "Iteration: 2886. Loss: 0.14697809517383575\n",
      "Iteration: 2887. Loss: 0.21951836347579956\n",
      "Iteration: 2888. Loss: 0.21090641617774963\n",
      "Iteration: 2889. Loss: 0.15170317888259888\n",
      "Iteration: 2890. Loss: 0.11395062506198883\n",
      "Iteration: 2891. Loss: 0.20429806411266327\n",
      "Iteration: 2892. Loss: 0.22996431589126587\n",
      "Iteration: 2893. Loss: 0.20738859474658966\n",
      "Iteration: 2894. Loss: 0.253974974155426\n",
      "Iteration: 2895. Loss: 0.11558312177658081\n",
      "Iteration: 2896. Loss: 0.12554094195365906\n",
      "Iteration: 2897. Loss: 0.10854169726371765\n",
      "Iteration: 2898. Loss: 0.0995294451713562\n",
      "Iteration: 2899. Loss: 0.10937998443841934\n",
      "Iteration: 2900. Loss: 0.2445361316204071\n",
      "Iteration: 2901. Loss: 0.11773037910461426\n",
      "Iteration: 2902. Loss: 0.18771232664585114\n",
      "Iteration: 2903. Loss: 0.0848335549235344\n",
      "Iteration: 2904. Loss: 0.23096154630184174\n",
      "Iteration: 2905. Loss: 0.22382567822933197\n",
      "Iteration: 2906. Loss: 0.09617719054222107\n",
      "Iteration: 2907. Loss: 0.15000498294830322\n",
      "Iteration: 2908. Loss: 0.1535467952489853\n",
      "Iteration: 2909. Loss: 0.13252945244312286\n",
      "Iteration: 2910. Loss: 0.2478015273809433\n",
      "Iteration: 2911. Loss: 0.2310582399368286\n",
      "Iteration: 2912. Loss: 0.08047925680875778\n",
      "Iteration: 2913. Loss: 0.14748890697956085\n",
      "Iteration: 2914. Loss: 0.3480573892593384\n",
      "Iteration: 2915. Loss: 0.10247454792261124\n",
      "Iteration: 2916. Loss: 0.22732463479042053\n",
      "Iteration: 2917. Loss: 0.09344067424535751\n",
      "Iteration: 2918. Loss: 0.12594619393348694\n",
      "Iteration: 2919. Loss: 0.21385416388511658\n",
      "Iteration: 2920. Loss: 0.09796309471130371\n",
      "Iteration: 2921. Loss: 0.09204734861850739\n",
      "Iteration: 2922. Loss: 0.14788560569286346\n",
      "Iteration: 2923. Loss: 0.25089943408966064\n",
      "Iteration: 2924. Loss: 0.15159407258033752\n",
      "Iteration: 2925. Loss: 0.2921014130115509\n",
      "Iteration: 2926. Loss: 0.19726167619228363\n",
      "Iteration: 2927. Loss: 0.30127206444740295\n",
      "Iteration: 2928. Loss: 0.16887931525707245\n",
      "Iteration: 2929. Loss: 0.1925218552350998\n",
      "Iteration: 2930. Loss: 0.11971769481897354\n",
      "Iteration: 2931. Loss: 0.06948170065879822\n",
      "Iteration: 2932. Loss: 0.06999318301677704\n",
      "Iteration: 2933. Loss: 0.2754965126514435\n",
      "Iteration: 2934. Loss: 0.13477258384227753\n",
      "Iteration: 2935. Loss: 0.08441785722970963\n",
      "Iteration: 2936. Loss: 0.2703743278980255\n",
      "Iteration: 2937. Loss: 0.19849611818790436\n",
      "Iteration: 2938. Loss: 0.139920175075531\n",
      "Iteration: 2939. Loss: 0.05488588288426399\n",
      "Iteration: 2940. Loss: 0.17147260904312134\n",
      "Iteration: 2941. Loss: 0.07386060059070587\n",
      "Iteration: 2942. Loss: 0.2126329094171524\n",
      "Iteration: 2943. Loss: 0.20789091289043427\n",
      "Iteration: 2944. Loss: 0.14168289303779602\n",
      "Iteration: 2945. Loss: 0.111076720058918\n",
      "Iteration: 2946. Loss: 0.1502295285463333\n",
      "Iteration: 2947. Loss: 0.06120843440294266\n",
      "Iteration: 2948. Loss: 0.1405026912689209\n",
      "Iteration: 2949. Loss: 0.10988303273916245\n",
      "Iteration: 2950. Loss: 0.09360121935606003\n",
      "Iteration: 2951. Loss: 0.14560788869857788\n",
      "Iteration: 2952. Loss: 0.08136336505413055\n",
      "Iteration: 2953. Loss: 0.17872968316078186\n",
      "Iteration: 2954. Loss: 0.16684186458587646\n",
      "Iteration: 2955. Loss: 0.16407741606235504\n",
      "Iteration: 2956. Loss: 0.1259019523859024\n",
      "Iteration: 2957. Loss: 0.13906113803386688\n",
      "Iteration: 2958. Loss: 0.1186256855726242\n",
      "Iteration: 2959. Loss: 0.10311706364154816\n",
      "Iteration: 2960. Loss: 0.10019158571958542\n",
      "Iteration: 2961. Loss: 0.30286821722984314\n",
      "Iteration: 2962. Loss: 0.3250136077404022\n",
      "Iteration: 2963. Loss: 0.19992323219776154\n",
      "Iteration: 2964. Loss: 0.08776120841503143\n",
      "Iteration: 2965. Loss: 0.06640976667404175\n",
      "Iteration: 2966. Loss: 0.12642008066177368\n",
      "Iteration: 2967. Loss: 0.19245141744613647\n",
      "Iteration: 2968. Loss: 0.10108931362628937\n",
      "Iteration: 2969. Loss: 0.14724551141262054\n",
      "Iteration: 2970. Loss: 0.0916285514831543\n",
      "Iteration: 2971. Loss: 0.10144100338220596\n",
      "Iteration: 2972. Loss: 0.2472967952489853\n",
      "Iteration: 2973. Loss: 0.103105328977108\n",
      "Iteration: 2974. Loss: 0.2331775724887848\n",
      "Iteration: 2975. Loss: 0.10519535839557648\n",
      "Iteration: 2976. Loss: 0.25813624262809753\n",
      "Iteration: 2977. Loss: 0.21470756828784943\n",
      "Iteration: 2978. Loss: 0.1259595900774002\n",
      "Iteration: 2979. Loss: 0.0951329916715622\n",
      "Iteration: 2980. Loss: 0.09000716358423233\n",
      "Iteration: 2981. Loss: 0.14832298457622528\n",
      "Iteration: 2982. Loss: 0.12663346529006958\n",
      "Iteration: 2983. Loss: 0.2105165719985962\n",
      "Iteration: 2984. Loss: 0.1671425849199295\n",
      "Iteration: 2985. Loss: 0.2016122192144394\n",
      "Iteration: 2986. Loss: 0.10196976363658905\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2987. Loss: 0.1872013807296753\n",
      "Iteration: 2988. Loss: 0.1936473846435547\n",
      "Iteration: 2989. Loss: 0.1593424528837204\n",
      "Iteration: 2990. Loss: 0.1257135570049286\n",
      "Iteration: 2991. Loss: 0.4123833477497101\n",
      "Iteration: 2992. Loss: 0.09793195128440857\n",
      "Iteration: 2993. Loss: 0.10620089620351791\n",
      "Iteration: 2994. Loss: 0.13440407812595367\n",
      "Iteration: 2995. Loss: 0.16766025125980377\n",
      "Iteration: 2996. Loss: 0.23291154205799103\n",
      "Iteration: 2997. Loss: 0.09181895107030869\n",
      "Iteration: 2998. Loss: 0.2166014462709427\n",
      "Iteration: 2999. Loss: 0.15496572852134705\n",
      "Iteration: 3000. Loss: 0.09926022589206696\n"
     ]
    }
   ],
   "source": [
    "# Training \n",
    "# convert input into variable \n",
    "iter=0 \n",
    "for epochs in range(num_epochs):\n",
    "    for i,(images,labels) in enumerate (train_loader): \n",
    "        images = Variable(images.view(-1,28*28))\n",
    "        labels = Variable(labels)\n",
    "        \n",
    "    # clear the gradients\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    # foward pass to obtain the outputs\n",
    "        outputs = model(images)\n",
    "    \n",
    "    #loss \n",
    "        loss = criterion(outputs,labels)\n",
    "    \n",
    "    #backpropagation \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    \n",
    "        iter+=1\n",
    "\n",
    "        print('Iteration: {}. Loss: {}'.format(iter, loss.data[0]))\n",
    "\n",
    "        \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%Iteration: 3000. Loss: 0.09926022589206696. Accuracy: 95.91\n"
     ]
    }
   ],
   "source": [
    "if iter %500 == 0:\n",
    "            # Calculate Accuracy         \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through test dataset\n",
    "            for images, labels in test_loader:\n",
    "               \n",
    "                images = Variable(images.view(-1, 28*28))\n",
    "                \n",
    "                # Forward pass only to get logits/output\n",
    "                outputs = model(images)\n",
    "                \n",
    "                # Get predictions from the maximum value\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                \n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "                \n",
    "                correct += (predicted.cpu() == labels.cpu()).sum()\n",
    "            \n",
    "            accuracy = 100 * correct / total\n",
    "        \n",
    "            # Print Loss\n",
    "            print('%Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.data[0], accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
