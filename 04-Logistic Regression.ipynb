{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input->Linear Function->Softmax Function->Crossentropy Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torchvision.transforms as transforms \n",
    "import torchvision.datasets as dsets \n",
    "from torch.autograd import Variable \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create the train and tets dataset \n",
    "train_dataset = dsets.MNIST(root='./data', \n",
    "                            train=True, \n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='./data', \n",
    "                           train=False, \n",
    "                           transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0][0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x119941f28>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAADgpJREFUeJzt3X+MVfWZx/HPs1j+kKI4aQRCYSnEYJW4082IjSWrxkzV\nDQZHrekkJjQapn8wiU02ZA3/VNNgyCrslmiamaZYSFpKE3VB0iw0otLGZuKIWC0srTFsO3IDNTjy\nwx9kmGf/mEMzxbnfe+fec++5zPN+JeT+eM6558kNnznn3O+592vuLgDx/EPRDQAoBuEHgiL8QFCE\nHwiK8ANBEX4gKMIPBEX4gaAIPxDUZc3cmJlxOSHQYO5u1SxX157fzO40syNm9q6ZPVrPawFoLqv1\n2n4zmybpj5I6JQ1Jel1St7sfSqzDnh9osGbs+ZdJetfd33P3c5J+IWllHa8HoInqCf88SX8Z93go\ne+7vmFmPmQ2a2WAd2wKQs3o+8Jvo0OJzh/Xu3i+pX+KwH2gl9ez5hyTNH/f4y5KO1dcOgGapJ/yv\nS7rGzL5iZtMlfVvSrnzaAtBoNR/2u/uImfVK2iNpmqQt7v6H3DoD0FA1D/XVtDHO+YGGa8pFPgAu\nXYQfCIrwA0ERfiAowg8ERfiBoAg/EBThB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgCD8QFOEHgiL8\nQFCEHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8EVfMU3ZJkZkclnZZ0\nXtKIu3fk0RTyM23atGT9yiuvbOj2e3t7y9Yuv/zy5LpLlixJ1tesWZOsP/XUU2Vr3d3dyXU//fTT\nZH3Dhg3J+uOPP56st4K6wp+5zd0/yOF1ADQRh/1AUPWG3yXtNbM3zKwnj4YANEe9h/3fcPdjZna1\npF+b2f+6+/7xC2R/FPjDALSYuvb87n4suz0h6QVJyyZYpt/dO/gwEGgtNYffzGaY2cwL9yV9U9I7\neTUGoLHqOeyfLekFM7vwOj939//JpSsADVdz+N39PUn/lGMvU9aCBQuS9enTpyfrN998c7K+fPny\nsrVZs2Yl173vvvuS9SINDQ0l65s3b07Wu7q6ytZOnz6dXPett95K1l999dVk/VLAUB8QFOEHgiL8\nQFCEHwiK8ANBEX4gKHP35m3MrHkba6L29vZkfd++fcl6o79W26pGR0eT9YceeihZP3PmTM3bLpVK\nyfqHH36YrB85cqTmbTeau1s1y7HnB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgGOfPQVtbW7I+MDCQ\nrC9atCjPdnJVqffh4eFk/bbbbitbO3fuXHLdqNc/1ItxfgBJhB8IivADQRF+ICjCDwRF+IGgCD8Q\nVB6z9IZ38uTJZH3t2rXJ+ooVK5L1N998M1mv9BPWKQcPHkzWOzs7k/WzZ88m69dff33Z2iOPPJJc\nF43Fnh8IivADQRF+ICjCDwRF+IGgCD8QFOEHgqr4fX4z2yJphaQT7r40e65N0g5JCyUdlfSAu6d/\n6FxT9/v89briiiuS9UrTSff19ZWtPfzww8l1H3zwwWR9+/btyTpaT57f5/+ppDsveu5RSS+5+zWS\nXsoeA7iEVAy/u++XdPElbCslbc3ub5V0T859AWiwWs/5Z7t7SZKy26vzawlAMzT82n4z65HU0+jt\nAJicWvf8x81sriRltyfKLeju/e7e4e4dNW4LQAPUGv5dklZl91dJ2plPOwCapWL4zWy7pN9JWmJm\nQ2b2sKQNkjrN7E+SOrPHAC4hFc/53b27TOn2nHsJ69SpU3Wt/9FHH9W87urVq5P1HTt2JOujo6M1\nbxvF4go/ICjCDwRF+IGgCD8QFOEHgiL8QFBM0T0FzJgxo2ztxRdfTK57yy23JOt33XVXsr53795k\nHc3HFN0Akgg/EBThB4Ii/EBQhB8IivADQRF+ICjG+ae4xYsXJ+sHDhxI1oeHh5P1l19+OVkfHBws\nW3vmmWeS6zbz/+ZUwjg/gCTCDwRF+IGgCD8QFOEHgiL8QFCEHwiKcf7gurq6kvVnn302WZ85c2bN\n2163bl2yvm3btmS9VCrVvO2pjHF+AEmEHwiK8ANBEX4gKMIPBEX4gaAIPxBUxXF+M9siaYWkE+6+\nNHvuMUmrJf01W2ydu/+q4sYY57/kLF26NFnftGlTsn777bXP5N7X15esr1+/Pll///33a972pSzP\ncf6fSrpzguf/093bs38Vgw+gtVQMv7vvl3SyCb0AaKJ6zvl7zez3ZrbFzK7KrSMATVFr+H8kabGk\ndkklSRvLLWhmPWY2aGblf8wNQNPVFH53P+7u5919VNKPJS1LLNvv7h3u3lFrkwDyV1P4zWzuuIdd\nkt7Jpx0AzXJZpQXMbLukWyV9ycyGJH1f0q1m1i7JJR2V9N0G9gigAfg+P+oya9asZP3uu+8uW6v0\nWwFm6eHqffv2JeudnZ3J+lTF9/kBJBF+ICjCDwRF+IGgCD8QFOEHgmKoD4X57LPPkvXLLktfhjIy\nMpKs33HHHWVrr7zySnLdSxlDfQCSCD8QFOEHgiL8QFCEHwiK8ANBEX4gqIrf50dsN9xwQ7J+//33\nJ+s33nhj2VqlcfxKDh06lKzv37+/rtef6tjzA0ERfiAowg8ERfiBoAg/EBThB4Ii/EBQjPNPcUuW\nLEnWe3t7k/V77703WZ8zZ86ke6rW+fPnk/VSqZSsj46O5tnOlMOeHwiK8ANBEX4gKMIPBEX4gaAI\nPxAU4QeCqjjOb2bzJW2TNEfSqKR+d/+hmbVJ2iFpoaSjkh5w9w8b12pclcbSu7u7y9YqjeMvXLiw\nlpZyMTg4mKyvX78+Wd+1a1ee7YRTzZ5/RNK/uftXJX1d0hozu07So5JecvdrJL2UPQZwiagYfncv\nufuB7P5pSYclzZO0UtLWbLGtku5pVJMA8jepc34zWyjpa5IGJM1295I09gdC0tV5Nwegcaq+tt/M\nvijpOUnfc/dTZlVNByYz65HUU1t7ABqlqj2/mX1BY8H/mbs/nz193MzmZvW5kk5MtK6797t7h7t3\n5NEwgHxUDL+N7eJ/Iumwu28aV9olaVV2f5Wknfm3B6BRKk7RbWbLJf1G0tsaG+qTpHUaO+//paQF\nkv4s6VvufrLCa4Wconv27NnJ+nXXXZesP/3008n6tddeO+me8jIwMJCsP/nkk2VrO3em9xd8Jbc2\n1U7RXfGc391/K6nci90+maYAtA6u8AOCIvxAUIQfCIrwA0ERfiAowg8ExU93V6mtra1sra+vL7lu\ne3t7sr5o0aKaesrDa6+9lqxv3LgxWd+zZ0+y/sknn0y6JzQHe34gKMIPBEX4gaAIPxAU4QeCIvxA\nUIQfCCrMOP9NN92UrK9duzZZX7ZsWdnavHnzauopLx9//HHZ2ubNm5PrPvHEE8n62bNna+oJrY89\nPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8EFWacv6urq656PQ4dOpSs7969O1kfGRlJ1lPfuR8eHk6u\ni7jY8wNBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUObu6QXM5kvaJmmOpFFJ/e7+QzN7TNJqSX/NFl3n\n7r+q8FrpjQGom7tbNctVE/65kua6+wEzmynpDUn3SHpA0hl3f6rapgg/0HjVhr/iFX7uXpJUyu6f\nNrPDkor96RoAdZvUOb+ZLZT0NUkD2VO9ZvZ7M9tiZleVWafHzAbNbLCuTgHkquJh/98WNPuipFcl\nrXf3581stqQPJLmkH2js1OChCq/BYT/QYLmd80uSmX1B0m5Je9x90wT1hZJ2u/vSCq9D+IEGqzb8\nFQ/7zcwk/UTS4fHBzz4IvKBL0juTbRJAcar5tH+5pN9IeltjQ32StE5St6R2jR32H5X03ezDwdRr\nsecHGizXw/68EH6g8XI77AcwNRF+ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBEX4\ngaAIPxAU4QeCavYU3R9I+r9xj7+UPdeKWrW3Vu1Lorda5dnbP1a7YFO/z/+5jZsNuntHYQ0ktGpv\nrdqXRG+1Kqo3DvuBoAg/EFTR4e8vePsprdpbq/Yl0VutCumt0HN+AMUpes8PoCCFhN/M7jSzI2b2\nrpk9WkQP5ZjZUTN728wOFj3FWDYN2gkze2fcc21m9msz+1N2O+E0aQX19piZvZ+9dwfN7F8L6m2+\nmb1sZofN7A9m9kj2fKHvXaKvQt63ph/2m9k0SX+U1ClpSNLrkrrd/VBTGynDzI5K6nD3wseEzexf\nJJ2RtO3CbEhm9h+STrr7huwP51Xu/u8t0ttjmuTMzQ3qrdzM0t9Rge9dnjNe56GIPf8ySe+6+3vu\nfk7SLyStLKCPlufu+yWdvOjplZK2Zve3auw/T9OV6a0luHvJ3Q9k909LujCzdKHvXaKvQhQR/nmS\n/jLu8ZBaa8pvl7TXzN4ws56im5nA7AszI2W3Vxfcz8UqztzcTBfNLN0y710tM17nrYjwTzSbSCsN\nOXzD3f9Z0l2S1mSHt6jOjyQt1tg0biVJG4tsJptZ+jlJ33P3U0X2Mt4EfRXyvhUR/iFJ88c9/rKk\nYwX0MSF3P5bdnpD0gsZOU1rJ8QuTpGa3Jwru52/c/bi7n3f3UUk/VoHvXTaz9HOSfubuz2dPF/7e\nTdRXUe9bEeF/XdI1ZvYVM5su6duSdhXQx+eY2YzsgxiZ2QxJ31TrzT68S9Kq7P4qSTsL7OXvtMrM\nzeVmllbB712rzXhdyEU+2VDGf0maJmmLu69vehMTMLNFGtvbS2PfePx5kb2Z2XZJt2rsW1/HJX1f\n0n9L+qWkBZL+LOlb7t70D97K9HarJjlzc4N6Kzez9IAKfO/ynPE6l364wg+IiSv8gKAIPxAU4QeC\nIvxAUIQfCIrwA0ERfiAowg8E9f/Ex0YKZYOZcwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10d679e80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_img = train_dataset[0][0].numpy().reshape(28,28)\n",
    "plt.imshow(show_img,cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1199a2d68>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQknxGq+fLwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x119a3b668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_img = train_dataset[1][0].numpy().reshape(28,28)\n",
    "plt.imshow(show_img,cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "n_iters = 3000\n",
    "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check iterability \n",
    "import collections\n",
    "isinstance (train_loader,collections.Iterable)\n",
    "# return true if iys iterable, return false if its not iterable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Building the model # same as linear regression \n",
    "class LogisticRegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LogisticRegressionModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        out = self.linear(x)\n",
    "        return out\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_dim = 28*28\n",
    "output_dim = 10\n",
    "\n",
    "model = LogisticRegressionModel(input_dim, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CrossEntyropy Loss \n",
    "# Compute the softmax function => probabilities \n",
    "# Compute the crossentropy => compare probabilities with labels  \n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# optimizer \n",
    "learning_rate= 0.001\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Module.parameters at 0x11cd63308>\n",
      "2\n",
      "torch.Size([10, 784])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "# analyse the parameters weights and bias \n",
    "print (model.parameters())\n",
    "print(len(list(model.parameters())))\n",
    "print(list(model.parameters())[0].size())\n",
    "print(list(model.parameters())[1].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1. Loss: 2.352494716644287 \n",
      "Iteration: 2. Loss: 2.3640356063842773 \n",
      "Iteration: 3. Loss: 2.354196548461914 \n",
      "Iteration: 4. Loss: 2.3588695526123047 \n",
      "Iteration: 5. Loss: 2.333890914916992 \n",
      "Iteration: 6. Loss: 2.36274790763855 \n",
      "Iteration: 7. Loss: 2.363837480545044 \n",
      "Iteration: 8. Loss: 2.3529551029205322 \n",
      "Iteration: 9. Loss: 2.2979280948638916 \n",
      "Iteration: 10. Loss: 2.3574304580688477 \n",
      "Iteration: 11. Loss: 2.335150957107544 \n",
      "Iteration: 12. Loss: 2.3416006565093994 \n",
      "Iteration: 13. Loss: 2.376187324523926 \n",
      "Iteration: 14. Loss: 2.3497960567474365 \n",
      "Iteration: 15. Loss: 2.333223581314087 \n",
      "Iteration: 16. Loss: 2.353335380554199 \n",
      "Iteration: 17. Loss: 2.3529579639434814 \n",
      "Iteration: 18. Loss: 2.293706178665161 \n",
      "Iteration: 19. Loss: 2.3724610805511475 \n",
      "Iteration: 20. Loss: 2.340538263320923 \n",
      "Iteration: 21. Loss: 2.3161542415618896 \n",
      "Iteration: 22. Loss: 2.3517041206359863 \n",
      "Iteration: 23. Loss: 2.3183753490448 \n",
      "Iteration: 24. Loss: 2.3337149620056152 \n",
      "Iteration: 25. Loss: 2.355355739593506 \n",
      "Iteration: 26. Loss: 2.349278211593628 \n",
      "Iteration: 27. Loss: 2.3301212787628174 \n",
      "Iteration: 28. Loss: 2.3343257904052734 \n",
      "Iteration: 29. Loss: 2.3182880878448486 \n",
      "Iteration: 30. Loss: 2.346029043197632 \n",
      "Iteration: 31. Loss: 2.2978153228759766 \n",
      "Iteration: 32. Loss: 2.280622959136963 \n",
      "Iteration: 33. Loss: 2.3106610774993896 \n",
      "Iteration: 34. Loss: 2.2966785430908203 \n",
      "Iteration: 35. Loss: 2.3298399448394775 \n",
      "Iteration: 36. Loss: 2.3439254760742188 \n",
      "Iteration: 37. Loss: 2.329017162322998 \n",
      "Iteration: 38. Loss: 2.30818247795105 \n",
      "Iteration: 39. Loss: 2.3196587562561035 \n",
      "Iteration: 40. Loss: 2.269524574279785 \n",
      "Iteration: 41. Loss: 2.312612295150757 \n",
      "Iteration: 42. Loss: 2.278857707977295 \n",
      "Iteration: 43. Loss: 2.2737510204315186 \n",
      "Iteration: 44. Loss: 2.2976884841918945 \n",
      "Iteration: 45. Loss: 2.305669069290161 \n",
      "Iteration: 46. Loss: 2.3057448863983154 \n",
      "Iteration: 47. Loss: 2.2951529026031494 \n",
      "Iteration: 48. Loss: 2.3033065795898438 \n",
      "Iteration: 49. Loss: 2.300506591796875 \n",
      "Iteration: 50. Loss: 2.291715145111084 \n",
      "Iteration: 51. Loss: 2.3004300594329834 \n",
      "Iteration: 52. Loss: 2.2935802936553955 \n",
      "Iteration: 53. Loss: 2.2808077335357666 \n",
      "Iteration: 54. Loss: 2.2967708110809326 \n",
      "Iteration: 55. Loss: 2.2692220211029053 \n",
      "Iteration: 56. Loss: 2.291029453277588 \n",
      "Iteration: 57. Loss: 2.290468215942383 \n",
      "Iteration: 58. Loss: 2.2826967239379883 \n",
      "Iteration: 59. Loss: 2.329580783843994 \n",
      "Iteration: 60. Loss: 2.2712795734405518 \n",
      "Iteration: 61. Loss: 2.279738664627075 \n",
      "Iteration: 62. Loss: 2.2668066024780273 \n",
      "Iteration: 63. Loss: 2.247859001159668 \n",
      "Iteration: 64. Loss: 2.327942132949829 \n",
      "Iteration: 65. Loss: 2.2717931270599365 \n",
      "Iteration: 66. Loss: 2.2770731449127197 \n",
      "Iteration: 67. Loss: 2.282562494277954 \n",
      "Iteration: 68. Loss: 2.27956485748291 \n",
      "Iteration: 69. Loss: 2.285926103591919 \n",
      "Iteration: 70. Loss: 2.2640414237976074 \n",
      "Iteration: 71. Loss: 2.2634100914001465 \n",
      "Iteration: 72. Loss: 2.275430917739868 \n",
      "Iteration: 73. Loss: 2.2845137119293213 \n",
      "Iteration: 74. Loss: 2.2822370529174805 \n",
      "Iteration: 75. Loss: 2.2907135486602783 \n",
      "Iteration: 76. Loss: 2.297210931777954 \n",
      "Iteration: 77. Loss: 2.2651400566101074 \n",
      "Iteration: 78. Loss: 2.2468137741088867 \n",
      "Iteration: 79. Loss: 2.2882182598114014 \n",
      "Iteration: 80. Loss: 2.2394862174987793 \n",
      "Iteration: 81. Loss: 2.277268648147583 \n",
      "Iteration: 82. Loss: 2.247739791870117 \n",
      "Iteration: 83. Loss: 2.2832767963409424 \n",
      "Iteration: 84. Loss: 2.2857871055603027 \n",
      "Iteration: 85. Loss: 2.29128360748291 \n",
      "Iteration: 86. Loss: 2.2298314571380615 \n",
      "Iteration: 87. Loss: 2.2660303115844727 \n",
      "Iteration: 88. Loss: 2.254260778427124 \n",
      "Iteration: 89. Loss: 2.2668564319610596 \n",
      "Iteration: 90. Loss: 2.2533905506134033 \n",
      "Iteration: 91. Loss: 2.233369827270508 \n",
      "Iteration: 92. Loss: 2.2184979915618896 \n",
      "Iteration: 93. Loss: 2.267116069793701 \n",
      "Iteration: 94. Loss: 2.2498035430908203 \n",
      "Iteration: 95. Loss: 2.275918960571289 \n",
      "Iteration: 96. Loss: 2.2330965995788574 \n",
      "Iteration: 97. Loss: 2.2572848796844482 \n",
      "Iteration: 98. Loss: 2.2441422939300537 \n",
      "Iteration: 99. Loss: 2.254220485687256 \n",
      "Iteration: 100. Loss: 2.2429428100585938 \n",
      "Iteration: 101. Loss: 2.218467950820923 \n",
      "Iteration: 102. Loss: 2.2466254234313965 \n",
      "Iteration: 103. Loss: 2.2577834129333496 \n",
      "Iteration: 104. Loss: 2.2223925590515137 \n",
      "Iteration: 105. Loss: 2.2486441135406494 \n",
      "Iteration: 106. Loss: 2.2400856018066406 \n",
      "Iteration: 107. Loss: 2.2240538597106934 \n",
      "Iteration: 108. Loss: 2.234269380569458 \n",
      "Iteration: 109. Loss: 2.2276129722595215 \n",
      "Iteration: 110. Loss: 2.2550697326660156 \n",
      "Iteration: 111. Loss: 2.2162139415740967 \n",
      "Iteration: 112. Loss: 2.2317616939544678 \n",
      "Iteration: 113. Loss: 2.2077138423919678 \n",
      "Iteration: 114. Loss: 2.208334445953369 \n",
      "Iteration: 115. Loss: 2.223245859146118 \n",
      "Iteration: 116. Loss: 2.253716468811035 \n",
      "Iteration: 117. Loss: 2.2482569217681885 \n",
      "Iteration: 118. Loss: 2.2404024600982666 \n",
      "Iteration: 119. Loss: 2.2212650775909424 \n",
      "Iteration: 120. Loss: 2.2091453075408936 \n",
      "Iteration: 121. Loss: 2.2226040363311768 \n",
      "Iteration: 122. Loss: 2.2184929847717285 \n",
      "Iteration: 123. Loss: 2.1999309062957764 \n",
      "Iteration: 124. Loss: 2.2235069274902344 \n",
      "Iteration: 125. Loss: 2.2287511825561523 \n",
      "Iteration: 126. Loss: 2.227882146835327 \n",
      "Iteration: 127. Loss: 2.213265895843506 \n",
      "Iteration: 128. Loss: 2.182049512863159 \n",
      "Iteration: 129. Loss: 2.2060303688049316 \n",
      "Iteration: 130. Loss: 2.20584774017334 \n",
      "Iteration: 131. Loss: 2.1943159103393555 \n",
      "Iteration: 132. Loss: 2.2211554050445557 \n",
      "Iteration: 133. Loss: 2.206369638442993 \n",
      "Iteration: 134. Loss: 2.19266939163208 \n",
      "Iteration: 135. Loss: 2.198451042175293 \n",
      "Iteration: 136. Loss: 2.221933364868164 \n",
      "Iteration: 137. Loss: 2.1867854595184326 \n",
      "Iteration: 138. Loss: 2.225019931793213 \n",
      "Iteration: 139. Loss: 2.207970380783081 \n",
      "Iteration: 140. Loss: 2.1743099689483643 \n",
      "Iteration: 141. Loss: 2.2097082138061523 \n",
      "Iteration: 142. Loss: 2.2005138397216797 \n",
      "Iteration: 143. Loss: 2.185643434524536 \n",
      "Iteration: 144. Loss: 2.1831281185150146 \n",
      "Iteration: 145. Loss: 2.1786513328552246 \n",
      "Iteration: 146. Loss: 2.1788506507873535 \n",
      "Iteration: 147. Loss: 2.165738105773926 \n",
      "Iteration: 148. Loss: 2.1881439685821533 \n",
      "Iteration: 149. Loss: 2.201261043548584 \n",
      "Iteration: 150. Loss: 2.1764822006225586 \n",
      "Iteration: 151. Loss: 2.1959033012390137 \n",
      "Iteration: 152. Loss: 2.2135777473449707 \n",
      "Iteration: 153. Loss: 2.217355251312256 \n",
      "Iteration: 154. Loss: 2.1859281063079834 \n",
      "Iteration: 155. Loss: 2.200762987136841 \n",
      "Iteration: 156. Loss: 2.176894426345825 \n",
      "Iteration: 157. Loss: 2.1678757667541504 \n",
      "Iteration: 158. Loss: 2.1508939266204834 \n",
      "Iteration: 159. Loss: 2.1746582984924316 \n",
      "Iteration: 160. Loss: 2.1951117515563965 \n",
      "Iteration: 161. Loss: 2.1787729263305664 \n",
      "Iteration: 162. Loss: 2.189539909362793 \n",
      "Iteration: 163. Loss: 2.2035131454467773 \n",
      "Iteration: 164. Loss: 2.168194055557251 \n",
      "Iteration: 165. Loss: 2.181248664855957 \n",
      "Iteration: 166. Loss: 2.1836862564086914 \n",
      "Iteration: 167. Loss: 2.173689603805542 \n",
      "Iteration: 168. Loss: 2.166609048843384 \n",
      "Iteration: 169. Loss: 2.1992013454437256 \n",
      "Iteration: 170. Loss: 2.159693717956543 \n",
      "Iteration: 171. Loss: 2.183015823364258 \n",
      "Iteration: 172. Loss: 2.1665782928466797 \n",
      "Iteration: 173. Loss: 2.1675591468811035 \n",
      "Iteration: 174. Loss: 2.1564888954162598 \n",
      "Iteration: 175. Loss: 2.162416934967041 \n",
      "Iteration: 176. Loss: 2.1432700157165527 \n",
      "Iteration: 177. Loss: 2.19832444190979 \n",
      "Iteration: 178. Loss: 2.171515464782715 \n",
      "Iteration: 179. Loss: 2.1720070838928223 \n",
      "Iteration: 180. Loss: 2.1633598804473877 \n",
      "Iteration: 181. Loss: 2.1380672454833984 \n",
      "Iteration: 182. Loss: 2.1547346115112305 \n",
      "Iteration: 183. Loss: 2.1513900756835938 \n",
      "Iteration: 184. Loss: 2.1670005321502686 \n",
      "Iteration: 185. Loss: 2.185507297515869 \n",
      "Iteration: 186. Loss: 2.1666829586029053 \n",
      "Iteration: 187. Loss: 2.174572229385376 \n",
      "Iteration: 188. Loss: 2.166532516479492 \n",
      "Iteration: 189. Loss: 2.1489205360412598 \n",
      "Iteration: 190. Loss: 2.176408052444458 \n",
      "Iteration: 191. Loss: 2.1649861335754395 \n",
      "Iteration: 192. Loss: 2.1476149559020996 \n",
      "Iteration: 193. Loss: 2.1123712062835693 \n",
      "Iteration: 194. Loss: 2.169757127761841 \n",
      "Iteration: 195. Loss: 2.1456832885742188 \n",
      "Iteration: 196. Loss: 2.1450347900390625 \n",
      "Iteration: 197. Loss: 2.145250082015991 \n",
      "Iteration: 198. Loss: 2.14284610748291 \n",
      "Iteration: 199. Loss: 2.153804302215576 \n",
      "Iteration: 200. Loss: 2.1832337379455566 \n",
      "Iteration: 201. Loss: 2.1470084190368652 \n",
      "Iteration: 202. Loss: 2.1397576332092285 \n",
      "Iteration: 203. Loss: 2.1538922786712646 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 204. Loss: 2.1512815952301025 \n",
      "Iteration: 205. Loss: 2.1127662658691406 \n",
      "Iteration: 206. Loss: 2.1361846923828125 \n",
      "Iteration: 207. Loss: 2.141376495361328 \n",
      "Iteration: 208. Loss: 2.1147031784057617 \n",
      "Iteration: 209. Loss: 2.145172119140625 \n",
      "Iteration: 210. Loss: 2.171964168548584 \n",
      "Iteration: 211. Loss: 2.159560203552246 \n",
      "Iteration: 212. Loss: 2.14854097366333 \n",
      "Iteration: 213. Loss: 2.1195967197418213 \n",
      "Iteration: 214. Loss: 2.1374824047088623 \n",
      "Iteration: 215. Loss: 2.1170990467071533 \n",
      "Iteration: 216. Loss: 2.1051132678985596 \n",
      "Iteration: 217. Loss: 2.103813886642456 \n",
      "Iteration: 218. Loss: 2.117260456085205 \n",
      "Iteration: 219. Loss: 2.119572401046753 \n",
      "Iteration: 220. Loss: 2.1150081157684326 \n",
      "Iteration: 221. Loss: 2.149911403656006 \n",
      "Iteration: 222. Loss: 2.1192891597747803 \n",
      "Iteration: 223. Loss: 2.095046043395996 \n",
      "Iteration: 224. Loss: 2.1316006183624268 \n",
      "Iteration: 225. Loss: 2.103111505508423 \n",
      "Iteration: 226. Loss: 2.0941100120544434 \n",
      "Iteration: 227. Loss: 2.129101514816284 \n",
      "Iteration: 228. Loss: 2.142472505569458 \n",
      "Iteration: 229. Loss: 2.1239895820617676 \n",
      "Iteration: 230. Loss: 2.1326966285705566 \n",
      "Iteration: 231. Loss: 2.1184678077697754 \n",
      "Iteration: 232. Loss: 2.0990407466888428 \n",
      "Iteration: 233. Loss: 2.107619285583496 \n",
      "Iteration: 234. Loss: 2.106177568435669 \n",
      "Iteration: 235. Loss: 2.08935546875 \n",
      "Iteration: 236. Loss: 2.110004186630249 \n",
      "Iteration: 237. Loss: 2.1293959617614746 \n",
      "Iteration: 238. Loss: 2.1072134971618652 \n",
      "Iteration: 239. Loss: 2.0544962882995605 \n",
      "Iteration: 240. Loss: 2.1521782875061035 \n",
      "Iteration: 241. Loss: 2.13555908203125 \n",
      "Iteration: 242. Loss: 2.102557420730591 \n",
      "Iteration: 243. Loss: 2.075683355331421 \n",
      "Iteration: 244. Loss: 2.068694829940796 \n",
      "Iteration: 245. Loss: 2.0830047130584717 \n",
      "Iteration: 246. Loss: 2.1240193843841553 \n",
      "Iteration: 247. Loss: 2.102172374725342 \n",
      "Iteration: 248. Loss: 2.1051132678985596 \n",
      "Iteration: 249. Loss: 2.0960848331451416 \n",
      "Iteration: 250. Loss: 2.121755599975586 \n",
      "Iteration: 251. Loss: 2.1235506534576416 \n",
      "Iteration: 252. Loss: 2.0772111415863037 \n",
      "Iteration: 253. Loss: 2.102241277694702 \n",
      "Iteration: 254. Loss: 2.0975427627563477 \n",
      "Iteration: 255. Loss: 2.088249444961548 \n",
      "Iteration: 256. Loss: 2.0742695331573486 \n",
      "Iteration: 257. Loss: 2.1069109439849854 \n",
      "Iteration: 258. Loss: 2.0731725692749023 \n",
      "Iteration: 259. Loss: 2.071012020111084 \n",
      "Iteration: 260. Loss: 2.099109649658203 \n",
      "Iteration: 261. Loss: 2.1158435344696045 \n",
      "Iteration: 262. Loss: 2.0820963382720947 \n",
      "Iteration: 263. Loss: 2.099832057952881 \n",
      "Iteration: 264. Loss: 2.102341413497925 \n",
      "Iteration: 265. Loss: 2.0849504470825195 \n",
      "Iteration: 266. Loss: 2.1022562980651855 \n",
      "Iteration: 267. Loss: 2.0849602222442627 \n",
      "Iteration: 268. Loss: 2.0851120948791504 \n",
      "Iteration: 269. Loss: 2.101881742477417 \n",
      "Iteration: 270. Loss: 2.04830265045166 \n",
      "Iteration: 271. Loss: 2.0662453174591064 \n",
      "Iteration: 272. Loss: 2.0407822132110596 \n",
      "Iteration: 273. Loss: 2.0729799270629883 \n",
      "Iteration: 274. Loss: 2.0856192111968994 \n",
      "Iteration: 275. Loss: 2.082275867462158 \n",
      "Iteration: 276. Loss: 2.0667481422424316 \n",
      "Iteration: 277. Loss: 2.0429399013519287 \n",
      "Iteration: 278. Loss: 2.0541651248931885 \n",
      "Iteration: 279. Loss: 2.0758228302001953 \n",
      "Iteration: 280. Loss: 2.0558931827545166 \n",
      "Iteration: 281. Loss: 2.071925163269043 \n",
      "Iteration: 282. Loss: 2.0581064224243164 \n",
      "Iteration: 283. Loss: 2.056302547454834 \n",
      "Iteration: 284. Loss: 2.0688560009002686 \n",
      "Iteration: 285. Loss: 2.061988353729248 \n",
      "Iteration: 286. Loss: 2.069581985473633 \n",
      "Iteration: 287. Loss: 2.04195499420166 \n",
      "Iteration: 288. Loss: 2.0746822357177734 \n",
      "Iteration: 289. Loss: 2.058867931365967 \n",
      "Iteration: 290. Loss: 2.027282238006592 \n",
      "Iteration: 291. Loss: 2.018672227859497 \n",
      "Iteration: 292. Loss: 2.0551626682281494 \n",
      "Iteration: 293. Loss: 2.070338249206543 \n",
      "Iteration: 294. Loss: 2.0632715225219727 \n",
      "Iteration: 295. Loss: 2.0617799758911133 \n",
      "Iteration: 296. Loss: 2.0699574947357178 \n",
      "Iteration: 297. Loss: 2.076181411743164 \n",
      "Iteration: 298. Loss: 2.0722525119781494 \n",
      "Iteration: 299. Loss: 2.0539941787719727 \n",
      "Iteration: 300. Loss: 2.0586047172546387 \n",
      "Iteration: 301. Loss: 2.0378293991088867 \n",
      "Iteration: 302. Loss: 2.0602829456329346 \n",
      "Iteration: 303. Loss: 2.036729097366333 \n",
      "Iteration: 304. Loss: 2.0509653091430664 \n",
      "Iteration: 305. Loss: 2.0530431270599365 \n",
      "Iteration: 306. Loss: 2.0557169914245605 \n",
      "Iteration: 307. Loss: 2.0339465141296387 \n",
      "Iteration: 308. Loss: 2.0077531337738037 \n",
      "Iteration: 309. Loss: 1.9994548559188843 \n",
      "Iteration: 310. Loss: 2.053021192550659 \n",
      "Iteration: 311. Loss: 2.057211399078369 \n",
      "Iteration: 312. Loss: 2.0121185779571533 \n",
      "Iteration: 313. Loss: 2.029635429382324 \n",
      "Iteration: 314. Loss: 2.037475109100342 \n",
      "Iteration: 315. Loss: 2.071718692779541 \n",
      "Iteration: 316. Loss: 2.048755407333374 \n",
      "Iteration: 317. Loss: 2.043043851852417 \n",
      "Iteration: 318. Loss: 2.0433359146118164 \n",
      "Iteration: 319. Loss: 2.0703728199005127 \n",
      "Iteration: 320. Loss: 1.9993324279785156 \n",
      "Iteration: 321. Loss: 2.041769504547119 \n",
      "Iteration: 322. Loss: 2.0193679332733154 \n",
      "Iteration: 323. Loss: 2.0524017810821533 \n",
      "Iteration: 324. Loss: 2.0534822940826416 \n",
      "Iteration: 325. Loss: 2.0007026195526123 \n",
      "Iteration: 326. Loss: 2.042508125305176 \n",
      "Iteration: 327. Loss: 1.9993772506713867 \n",
      "Iteration: 328. Loss: 2.0109994411468506 \n",
      "Iteration: 329. Loss: 2.0245864391326904 \n",
      "Iteration: 330. Loss: 2.034289836883545 \n",
      "Iteration: 331. Loss: 2.015141010284424 \n",
      "Iteration: 332. Loss: 2.015028715133667 \n",
      "Iteration: 333. Loss: 2.0230133533477783 \n",
      "Iteration: 334. Loss: 2.064925193786621 \n",
      "Iteration: 335. Loss: 2.0399863719940186 \n",
      "Iteration: 336. Loss: 2.012371778488159 \n",
      "Iteration: 337. Loss: 2.022395372390747 \n",
      "Iteration: 338. Loss: 1.9684590101242065 \n",
      "Iteration: 339. Loss: 2.0241198539733887 \n",
      "Iteration: 340. Loss: 2.0158214569091797 \n",
      "Iteration: 341. Loss: 1.9791520833969116 \n",
      "Iteration: 342. Loss: 2.0410425662994385 \n",
      "Iteration: 343. Loss: 2.0085744857788086 \n",
      "Iteration: 344. Loss: 2.0261902809143066 \n",
      "Iteration: 345. Loss: 2.0058095455169678 \n",
      "Iteration: 346. Loss: 2.033095121383667 \n",
      "Iteration: 347. Loss: 2.0365824699401855 \n",
      "Iteration: 348. Loss: 2.0537099838256836 \n",
      "Iteration: 349. Loss: 1.9912550449371338 \n",
      "Iteration: 350. Loss: 2.0432159900665283 \n",
      "Iteration: 351. Loss: 2.0144102573394775 \n",
      "Iteration: 352. Loss: 1.9978392124176025 \n",
      "Iteration: 353. Loss: 2.0453271865844727 \n",
      "Iteration: 354. Loss: 2.0213634967803955 \n",
      "Iteration: 355. Loss: 2.0046682357788086 \n",
      "Iteration: 356. Loss: 1.9901918172836304 \n",
      "Iteration: 357. Loss: 2.0333142280578613 \n",
      "Iteration: 358. Loss: 2.005272150039673 \n",
      "Iteration: 359. Loss: 1.9758111238479614 \n",
      "Iteration: 360. Loss: 1.9874382019042969 \n",
      "Iteration: 361. Loss: 1.9662803411483765 \n",
      "Iteration: 362. Loss: 2.0136168003082275 \n",
      "Iteration: 363. Loss: 1.969053030014038 \n",
      "Iteration: 364. Loss: 1.964452862739563 \n",
      "Iteration: 365. Loss: 1.999693751335144 \n",
      "Iteration: 366. Loss: 1.999407172203064 \n",
      "Iteration: 367. Loss: 2.0117475986480713 \n",
      "Iteration: 368. Loss: 1.9929276704788208 \n",
      "Iteration: 369. Loss: 1.9782658815383911 \n",
      "Iteration: 370. Loss: 1.9629340171813965 \n",
      "Iteration: 371. Loss: 1.9617711305618286 \n",
      "Iteration: 372. Loss: 2.0046164989471436 \n",
      "Iteration: 373. Loss: 1.9833582639694214 \n",
      "Iteration: 374. Loss: 1.991767406463623 \n",
      "Iteration: 375. Loss: 2.00364089012146 \n",
      "Iteration: 376. Loss: 1.9727282524108887 \n",
      "Iteration: 377. Loss: 1.984547734260559 \n",
      "Iteration: 378. Loss: 1.9970632791519165 \n",
      "Iteration: 379. Loss: 2.027352809906006 \n",
      "Iteration: 380. Loss: 1.9516074657440186 \n",
      "Iteration: 381. Loss: 1.966476321220398 \n",
      "Iteration: 382. Loss: 1.961466670036316 \n",
      "Iteration: 383. Loss: 2.02117657661438 \n",
      "Iteration: 384. Loss: 1.9766727685928345 \n",
      "Iteration: 385. Loss: 1.9660460948944092 \n",
      "Iteration: 386. Loss: 1.977810025215149 \n",
      "Iteration: 387. Loss: 1.9550403356552124 \n",
      "Iteration: 388. Loss: 1.9724700450897217 \n",
      "Iteration: 389. Loss: 1.9753528833389282 \n",
      "Iteration: 390. Loss: 1.996675968170166 \n",
      "Iteration: 391. Loss: 2.0334033966064453 \n",
      "Iteration: 392. Loss: 1.9795424938201904 \n",
      "Iteration: 393. Loss: 1.9555463790893555 \n",
      "Iteration: 394. Loss: 1.936410665512085 \n",
      "Iteration: 395. Loss: 1.960016131401062 \n",
      "Iteration: 396. Loss: 1.9996124505996704 \n",
      "Iteration: 397. Loss: 1.9949917793273926 \n",
      "Iteration: 398. Loss: 1.914167046546936 \n",
      "Iteration: 399. Loss: 1.9476901292800903 \n",
      "Iteration: 400. Loss: 1.9481600522994995 \n",
      "Iteration: 401. Loss: 2.002655267715454 \n",
      "Iteration: 402. Loss: 1.9413281679153442 \n",
      "Iteration: 403. Loss: 1.9697433710098267 \n",
      "Iteration: 404. Loss: 1.96491277217865 \n",
      "Iteration: 405. Loss: 2.0073022842407227 \n",
      "Iteration: 406. Loss: 1.9585003852844238 \n",
      "Iteration: 407. Loss: 1.9384312629699707 \n",
      "Iteration: 408. Loss: 1.9540379047393799 \n",
      "Iteration: 409. Loss: 1.993864893913269 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 410. Loss: 1.945443868637085 \n",
      "Iteration: 411. Loss: 1.9410181045532227 \n",
      "Iteration: 412. Loss: 1.9640753269195557 \n",
      "Iteration: 413. Loss: 1.9534715414047241 \n",
      "Iteration: 414. Loss: 1.9756213426589966 \n",
      "Iteration: 415. Loss: 1.938969373703003 \n",
      "Iteration: 416. Loss: 1.942338466644287 \n",
      "Iteration: 417. Loss: 1.9558371305465698 \n",
      "Iteration: 418. Loss: 1.9651288986206055 \n",
      "Iteration: 419. Loss: 1.9646803140640259 \n",
      "Iteration: 420. Loss: 1.9643223285675049 \n",
      "Iteration: 421. Loss: 1.9898767471313477 \n",
      "Iteration: 422. Loss: 1.9203524589538574 \n",
      "Iteration: 423. Loss: 1.9875296354293823 \n",
      "Iteration: 424. Loss: 1.9714860916137695 \n",
      "Iteration: 425. Loss: 1.925176978111267 \n",
      "Iteration: 426. Loss: 1.9635155200958252 \n",
      "Iteration: 427. Loss: 1.9873679876327515 \n",
      "Iteration: 428. Loss: 1.9099855422973633 \n",
      "Iteration: 429. Loss: 1.950187087059021 \n",
      "Iteration: 430. Loss: 1.909071683883667 \n",
      "Iteration: 431. Loss: 1.9270342588424683 \n",
      "Iteration: 432. Loss: 1.9626175165176392 \n",
      "Iteration: 433. Loss: 1.9339301586151123 \n",
      "Iteration: 434. Loss: 1.9313836097717285 \n",
      "Iteration: 435. Loss: 1.9058706760406494 \n",
      "Iteration: 436. Loss: 1.959782361984253 \n",
      "Iteration: 437. Loss: 1.8916391134262085 \n",
      "Iteration: 438. Loss: 1.9870386123657227 \n",
      "Iteration: 439. Loss: 1.9263768196105957 \n",
      "Iteration: 440. Loss: 1.9321584701538086 \n",
      "Iteration: 441. Loss: 1.9207797050476074 \n",
      "Iteration: 442. Loss: 1.973148226737976 \n",
      "Iteration: 443. Loss: 1.9172710180282593 \n",
      "Iteration: 444. Loss: 1.885426640510559 \n",
      "Iteration: 445. Loss: 1.9631657600402832 \n",
      "Iteration: 446. Loss: 1.949712872505188 \n",
      "Iteration: 447. Loss: 1.9560346603393555 \n",
      "Iteration: 448. Loss: 1.951422095298767 \n",
      "Iteration: 449. Loss: 1.9470689296722412 \n",
      "Iteration: 450. Loss: 1.8924286365509033 \n",
      "Iteration: 451. Loss: 1.9449763298034668 \n",
      "Iteration: 452. Loss: 1.9013566970825195 \n",
      "Iteration: 453. Loss: 1.9284144639968872 \n",
      "Iteration: 454. Loss: 1.968652367591858 \n",
      "Iteration: 455. Loss: 1.9248701333999634 \n",
      "Iteration: 456. Loss: 1.8941905498504639 \n",
      "Iteration: 457. Loss: 1.927290916442871 \n",
      "Iteration: 458. Loss: 1.880804419517517 \n",
      "Iteration: 459. Loss: 1.8970277309417725 \n",
      "Iteration: 460. Loss: 1.9573723077774048 \n",
      "Iteration: 461. Loss: 1.8595991134643555 \n",
      "Iteration: 462. Loss: 1.9176706075668335 \n",
      "Iteration: 463. Loss: 1.9172651767730713 \n",
      "Iteration: 464. Loss: 1.9066241979599 \n",
      "Iteration: 465. Loss: 1.8927955627441406 \n",
      "Iteration: 466. Loss: 1.8835182189941406 \n",
      "Iteration: 467. Loss: 1.918054223060608 \n",
      "Iteration: 468. Loss: 1.9446133375167847 \n",
      "Iteration: 469. Loss: 1.9097827672958374 \n",
      "Iteration: 470. Loss: 1.910007357597351 \n",
      "Iteration: 471. Loss: 1.9051190614700317 \n",
      "Iteration: 472. Loss: 1.8811684846878052 \n",
      "Iteration: 473. Loss: 1.9324418306350708 \n",
      "Iteration: 474. Loss: 1.8801562786102295 \n",
      "Iteration: 475. Loss: 1.924055814743042 \n",
      "Iteration: 476. Loss: 1.8991128206253052 \n",
      "Iteration: 477. Loss: 1.9115077257156372 \n",
      "Iteration: 478. Loss: 1.8735322952270508 \n",
      "Iteration: 479. Loss: 1.8777976036071777 \n",
      "Iteration: 480. Loss: 1.8730273246765137 \n",
      "Iteration: 481. Loss: 1.934901237487793 \n",
      "Iteration: 482. Loss: 1.9145264625549316 \n",
      "Iteration: 483. Loss: 1.9027760028839111 \n",
      "Iteration: 484. Loss: 1.846802830696106 \n",
      "Iteration: 485. Loss: 1.8219071626663208 \n",
      "Iteration: 486. Loss: 1.8810685873031616 \n",
      "Iteration: 487. Loss: 1.8865723609924316 \n",
      "Iteration: 488. Loss: 1.8933483362197876 \n",
      "Iteration: 489. Loss: 1.8555608987808228 \n",
      "Iteration: 490. Loss: 1.8965048789978027 \n",
      "Iteration: 491. Loss: 1.8998558521270752 \n",
      "Iteration: 492. Loss: 1.9338908195495605 \n",
      "Iteration: 493. Loss: 1.8492345809936523 \n",
      "Iteration: 494. Loss: 1.9152741432189941 \n",
      "Iteration: 495. Loss: 1.9167187213897705 \n",
      "Iteration: 496. Loss: 1.9447715282440186 \n",
      "Iteration: 497. Loss: 1.877260684967041 \n",
      "Iteration: 498. Loss: 1.8583108186721802 \n",
      "Iteration: 499. Loss: 1.8707906007766724 \n",
      "Iteration: 500. Loss: 1.8859561681747437 \n",
      "Iteration: 501. Loss: 1.8945918083190918 \n",
      "Iteration: 502. Loss: 1.882692575454712 \n",
      "Iteration: 503. Loss: 1.8995890617370605 \n",
      "Iteration: 504. Loss: 1.8774417638778687 \n",
      "Iteration: 505. Loss: 1.8867663145065308 \n",
      "Iteration: 506. Loss: 1.8784745931625366 \n",
      "Iteration: 507. Loss: 1.904518723487854 \n",
      "Iteration: 508. Loss: 1.8815678358078003 \n",
      "Iteration: 509. Loss: 1.8520430326461792 \n",
      "Iteration: 510. Loss: 1.8778254985809326 \n",
      "Iteration: 511. Loss: 1.8394695520401 \n",
      "Iteration: 512. Loss: 1.8776057958602905 \n",
      "Iteration: 513. Loss: 1.8870794773101807 \n",
      "Iteration: 514. Loss: 1.853055715560913 \n",
      "Iteration: 515. Loss: 1.8983676433563232 \n",
      "Iteration: 516. Loss: 1.8983463048934937 \n",
      "Iteration: 517. Loss: 1.8848949670791626 \n",
      "Iteration: 518. Loss: 1.8442602157592773 \n",
      "Iteration: 519. Loss: 1.8872932195663452 \n",
      "Iteration: 520. Loss: 1.8581286668777466 \n",
      "Iteration: 521. Loss: 1.8742938041687012 \n",
      "Iteration: 522. Loss: 1.8193597793579102 \n",
      "Iteration: 523. Loss: 1.802798867225647 \n",
      "Iteration: 524. Loss: 1.8587803840637207 \n",
      "Iteration: 525. Loss: 1.8416370153427124 \n",
      "Iteration: 526. Loss: 1.8837943077087402 \n",
      "Iteration: 527. Loss: 1.8618557453155518 \n",
      "Iteration: 528. Loss: 1.8484060764312744 \n",
      "Iteration: 529. Loss: 1.8739862442016602 \n",
      "Iteration: 530. Loss: 1.8583592176437378 \n",
      "Iteration: 531. Loss: 1.8524452447891235 \n",
      "Iteration: 532. Loss: 1.8771274089813232 \n",
      "Iteration: 533. Loss: 1.9043588638305664 \n",
      "Iteration: 534. Loss: 1.8829867839813232 \n",
      "Iteration: 535. Loss: 1.8911776542663574 \n",
      "Iteration: 536. Loss: 1.8135080337524414 \n",
      "Iteration: 537. Loss: 1.7865604162216187 \n",
      "Iteration: 538. Loss: 1.8463436365127563 \n",
      "Iteration: 539. Loss: 1.8532835245132446 \n",
      "Iteration: 540. Loss: 1.8890767097473145 \n",
      "Iteration: 541. Loss: 1.833746075630188 \n",
      "Iteration: 542. Loss: 1.8413238525390625 \n",
      "Iteration: 543. Loss: 1.8259894847869873 \n",
      "Iteration: 544. Loss: 1.8093922138214111 \n",
      "Iteration: 545. Loss: 1.871436357498169 \n",
      "Iteration: 546. Loss: 1.8481359481811523 \n",
      "Iteration: 547. Loss: 1.8371390104293823 \n",
      "Iteration: 548. Loss: 1.8647301197052002 \n",
      "Iteration: 549. Loss: 1.8248993158340454 \n",
      "Iteration: 550. Loss: 1.8355597257614136 \n",
      "Iteration: 551. Loss: 1.8583307266235352 \n",
      "Iteration: 552. Loss: 1.8601710796356201 \n",
      "Iteration: 553. Loss: 1.8646892309188843 \n",
      "Iteration: 554. Loss: 1.8496071100234985 \n",
      "Iteration: 555. Loss: 1.859554409980774 \n",
      "Iteration: 556. Loss: 1.8887370824813843 \n",
      "Iteration: 557. Loss: 1.8552240133285522 \n",
      "Iteration: 558. Loss: 1.8384804725646973 \n",
      "Iteration: 559. Loss: 1.879846215248108 \n",
      "Iteration: 560. Loss: 1.8070058822631836 \n",
      "Iteration: 561. Loss: 1.843943476676941 \n",
      "Iteration: 562. Loss: 1.8359594345092773 \n",
      "Iteration: 563. Loss: 1.8167606592178345 \n",
      "Iteration: 564. Loss: 1.8610883951187134 \n",
      "Iteration: 565. Loss: 1.7757213115692139 \n",
      "Iteration: 566. Loss: 1.8648245334625244 \n",
      "Iteration: 567. Loss: 1.8782858848571777 \n",
      "Iteration: 568. Loss: 1.8211777210235596 \n",
      "Iteration: 569. Loss: 1.8877270221710205 \n",
      "Iteration: 570. Loss: 1.812455415725708 \n",
      "Iteration: 571. Loss: 1.8432774543762207 \n",
      "Iteration: 572. Loss: 1.8528656959533691 \n",
      "Iteration: 573. Loss: 1.920257568359375 \n",
      "Iteration: 574. Loss: 1.8311210870742798 \n",
      "Iteration: 575. Loss: 1.7801251411437988 \n",
      "Iteration: 576. Loss: 1.8956581354141235 \n",
      "Iteration: 577. Loss: 1.8133856058120728 \n",
      "Iteration: 578. Loss: 1.7877812385559082 \n",
      "Iteration: 579. Loss: 1.8273223638534546 \n",
      "Iteration: 580. Loss: 1.8496747016906738 \n",
      "Iteration: 581. Loss: 1.807387113571167 \n",
      "Iteration: 582. Loss: 1.8368078470230103 \n",
      "Iteration: 583. Loss: 1.846764087677002 \n",
      "Iteration: 584. Loss: 1.8039360046386719 \n",
      "Iteration: 585. Loss: 1.8057266473770142 \n",
      "Iteration: 586. Loss: 1.8260797262191772 \n",
      "Iteration: 587. Loss: 1.8829169273376465 \n",
      "Iteration: 588. Loss: 1.838202714920044 \n",
      "Iteration: 589. Loss: 1.806217908859253 \n",
      "Iteration: 590. Loss: 1.809048056602478 \n",
      "Iteration: 591. Loss: 1.8510866165161133 \n",
      "Iteration: 592. Loss: 1.790504813194275 \n",
      "Iteration: 593. Loss: 1.8115074634552002 \n",
      "Iteration: 594. Loss: 1.855056643486023 \n",
      "Iteration: 595. Loss: 1.8264987468719482 \n",
      "Iteration: 596. Loss: 1.8447688817977905 \n",
      "Iteration: 597. Loss: 1.7942492961883545 \n",
      "Iteration: 598. Loss: 1.8549151420593262 \n",
      "Iteration: 599. Loss: 1.7482669353485107 \n",
      "Iteration: 600. Loss: 1.8316733837127686 \n",
      "Iteration: 601. Loss: 1.816810131072998 \n",
      "Iteration: 602. Loss: 1.8491989374160767 \n",
      "Iteration: 603. Loss: 1.8392994403839111 \n",
      "Iteration: 604. Loss: 1.7771497964859009 \n",
      "Iteration: 605. Loss: 1.8260540962219238 \n",
      "Iteration: 606. Loss: 1.800537109375 \n",
      "Iteration: 607. Loss: 1.8189136981964111 \n",
      "Iteration: 608. Loss: 1.7946168184280396 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 609. Loss: 1.752515435218811 \n",
      "Iteration: 610. Loss: 1.8305832147598267 \n",
      "Iteration: 611. Loss: 1.8126851320266724 \n",
      "Iteration: 612. Loss: 1.8193049430847168 \n",
      "Iteration: 613. Loss: 1.779884696006775 \n",
      "Iteration: 614. Loss: 1.8383352756500244 \n",
      "Iteration: 615. Loss: 1.8366878032684326 \n",
      "Iteration: 616. Loss: 1.7616478204727173 \n",
      "Iteration: 617. Loss: 1.7645344734191895 \n",
      "Iteration: 618. Loss: 1.8364548683166504 \n",
      "Iteration: 619. Loss: 1.8007413148880005 \n",
      "Iteration: 620. Loss: 1.8251054286956787 \n",
      "Iteration: 621. Loss: 1.8457939624786377 \n",
      "Iteration: 622. Loss: 1.8045326471328735 \n",
      "Iteration: 623. Loss: 1.805423617362976 \n",
      "Iteration: 624. Loss: 1.787562608718872 \n",
      "Iteration: 625. Loss: 1.868177056312561 \n",
      "Iteration: 626. Loss: 1.81184983253479 \n",
      "Iteration: 627. Loss: 1.808271050453186 \n",
      "Iteration: 628. Loss: 1.7097692489624023 \n",
      "Iteration: 629. Loss: 1.781874656677246 \n",
      "Iteration: 630. Loss: 1.7951472997665405 \n",
      "Iteration: 631. Loss: 1.807673692703247 \n",
      "Iteration: 632. Loss: 1.8027476072311401 \n",
      "Iteration: 633. Loss: 1.7680506706237793 \n",
      "Iteration: 634. Loss: 1.8401025533676147 \n",
      "Iteration: 635. Loss: 1.7871664762496948 \n",
      "Iteration: 636. Loss: 1.782117486000061 \n",
      "Iteration: 637. Loss: 1.8043574094772339 \n",
      "Iteration: 638. Loss: 1.7609072923660278 \n",
      "Iteration: 639. Loss: 1.7802716493606567 \n",
      "Iteration: 640. Loss: 1.7313814163208008 \n",
      "Iteration: 641. Loss: 1.730713963508606 \n",
      "Iteration: 642. Loss: 1.8222434520721436 \n",
      "Iteration: 643. Loss: 1.7483956813812256 \n",
      "Iteration: 644. Loss: 1.7786195278167725 \n",
      "Iteration: 645. Loss: 1.7978386878967285 \n",
      "Iteration: 646. Loss: 1.8305249214172363 \n",
      "Iteration: 647. Loss: 1.761536717414856 \n",
      "Iteration: 648. Loss: 1.8377608060836792 \n",
      "Iteration: 649. Loss: 1.7467209100723267 \n",
      "Iteration: 650. Loss: 1.769261121749878 \n",
      "Iteration: 651. Loss: 1.7886770963668823 \n",
      "Iteration: 652. Loss: 1.75209641456604 \n",
      "Iteration: 653. Loss: 1.7944579124450684 \n",
      "Iteration: 654. Loss: 1.7506611347198486 \n",
      "Iteration: 655. Loss: 1.8147225379943848 \n",
      "Iteration: 656. Loss: 1.804886817932129 \n",
      "Iteration: 657. Loss: 1.7720282077789307 \n",
      "Iteration: 658. Loss: 1.7142606973648071 \n",
      "Iteration: 659. Loss: 1.700769066810608 \n",
      "Iteration: 660. Loss: 1.7041027545928955 \n",
      "Iteration: 661. Loss: 1.7942546606063843 \n",
      "Iteration: 662. Loss: 1.7943569421768188 \n",
      "Iteration: 663. Loss: 1.7503705024719238 \n",
      "Iteration: 664. Loss: 1.7529895305633545 \n",
      "Iteration: 665. Loss: 1.8341885805130005 \n",
      "Iteration: 666. Loss: 1.829309105873108 \n",
      "Iteration: 667. Loss: 1.7988684177398682 \n",
      "Iteration: 668. Loss: 1.761070728302002 \n",
      "Iteration: 669. Loss: 1.7540544271469116 \n",
      "Iteration: 670. Loss: 1.772436499595642 \n",
      "Iteration: 671. Loss: 1.7139804363250732 \n",
      "Iteration: 672. Loss: 1.7901060581207275 \n",
      "Iteration: 673. Loss: 1.758623480796814 \n",
      "Iteration: 674. Loss: 1.7926361560821533 \n",
      "Iteration: 675. Loss: 1.7773982286453247 \n",
      "Iteration: 676. Loss: 1.7572382688522339 \n",
      "Iteration: 677. Loss: 1.7888914346694946 \n",
      "Iteration: 678. Loss: 1.7571556568145752 \n",
      "Iteration: 679. Loss: 1.7612271308898926 \n",
      "Iteration: 680. Loss: 1.7252304553985596 \n",
      "Iteration: 681. Loss: 1.755465030670166 \n",
      "Iteration: 682. Loss: 1.6934579610824585 \n",
      "Iteration: 683. Loss: 1.7795507907867432 \n",
      "Iteration: 684. Loss: 1.7677772045135498 \n",
      "Iteration: 685. Loss: 1.7745198011398315 \n",
      "Iteration: 686. Loss: 1.8233692646026611 \n",
      "Iteration: 687. Loss: 1.7760581970214844 \n",
      "Iteration: 688. Loss: 1.7039179801940918 \n",
      "Iteration: 689. Loss: 1.7499535083770752 \n",
      "Iteration: 690. Loss: 1.7335368394851685 \n",
      "Iteration: 691. Loss: 1.7649470567703247 \n",
      "Iteration: 692. Loss: 1.7281047105789185 \n",
      "Iteration: 693. Loss: 1.757047414779663 \n",
      "Iteration: 694. Loss: 1.7037431001663208 \n",
      "Iteration: 695. Loss: 1.8211435079574585 \n",
      "Iteration: 696. Loss: 1.7481533288955688 \n",
      "Iteration: 697. Loss: 1.7157814502716064 \n",
      "Iteration: 698. Loss: 1.7324801683425903 \n",
      "Iteration: 699. Loss: 1.7691491842269897 \n",
      "Iteration: 700. Loss: 1.7395482063293457 \n",
      "Iteration: 701. Loss: 1.775313377380371 \n",
      "Iteration: 702. Loss: 1.7530510425567627 \n",
      "Iteration: 703. Loss: 1.7522789239883423 \n",
      "Iteration: 704. Loss: 1.7565912008285522 \n",
      "Iteration: 705. Loss: 1.748543381690979 \n",
      "Iteration: 706. Loss: 1.7591105699539185 \n",
      "Iteration: 707. Loss: 1.7275768518447876 \n",
      "Iteration: 708. Loss: 1.7221180200576782 \n",
      "Iteration: 709. Loss: 1.7616894245147705 \n",
      "Iteration: 710. Loss: 1.784069538116455 \n",
      "Iteration: 711. Loss: 1.7481510639190674 \n",
      "Iteration: 712. Loss: 1.7992016077041626 \n",
      "Iteration: 713. Loss: 1.7844771146774292 \n",
      "Iteration: 714. Loss: 1.781501054763794 \n",
      "Iteration: 715. Loss: 1.7552589178085327 \n",
      "Iteration: 716. Loss: 1.778656005859375 \n",
      "Iteration: 717. Loss: 1.6929211616516113 \n",
      "Iteration: 718. Loss: 1.7272018194198608 \n",
      "Iteration: 719. Loss: 1.7226076126098633 \n",
      "Iteration: 720. Loss: 1.758095383644104 \n",
      "Iteration: 721. Loss: 1.7425860166549683 \n",
      "Iteration: 722. Loss: 1.7685139179229736 \n",
      "Iteration: 723. Loss: 1.774895429611206 \n",
      "Iteration: 724. Loss: 1.7643470764160156 \n",
      "Iteration: 725. Loss: 1.696179986000061 \n",
      "Iteration: 726. Loss: 1.6806739568710327 \n",
      "Iteration: 727. Loss: 1.7453416585922241 \n",
      "Iteration: 728. Loss: 1.6759334802627563 \n",
      "Iteration: 729. Loss: 1.6899060010910034 \n",
      "Iteration: 730. Loss: 1.7522060871124268 \n",
      "Iteration: 731. Loss: 1.7292463779449463 \n",
      "Iteration: 732. Loss: 1.7677102088928223 \n",
      "Iteration: 733. Loss: 1.6702972650527954 \n",
      "Iteration: 734. Loss: 1.8183146715164185 \n",
      "Iteration: 735. Loss: 1.702639102935791 \n",
      "Iteration: 736. Loss: 1.679295301437378 \n",
      "Iteration: 737. Loss: 1.7374440431594849 \n",
      "Iteration: 738. Loss: 1.6948200464248657 \n",
      "Iteration: 739. Loss: 1.7741541862487793 \n",
      "Iteration: 740. Loss: 1.7361388206481934 \n",
      "Iteration: 741. Loss: 1.6477125883102417 \n",
      "Iteration: 742. Loss: 1.8093806505203247 \n",
      "Iteration: 743. Loss: 1.753895878791809 \n",
      "Iteration: 744. Loss: 1.7392361164093018 \n",
      "Iteration: 745. Loss: 1.739755392074585 \n",
      "Iteration: 746. Loss: 1.7223517894744873 \n",
      "Iteration: 747. Loss: 1.7477706670761108 \n",
      "Iteration: 748. Loss: 1.6839338541030884 \n",
      "Iteration: 749. Loss: 1.7743022441864014 \n",
      "Iteration: 750. Loss: 1.6513018608093262 \n",
      "Iteration: 751. Loss: 1.7293319702148438 \n",
      "Iteration: 752. Loss: 1.7931708097457886 \n",
      "Iteration: 753. Loss: 1.7073992490768433 \n",
      "Iteration: 754. Loss: 1.7093833684921265 \n",
      "Iteration: 755. Loss: 1.7304023504257202 \n",
      "Iteration: 756. Loss: 1.704911470413208 \n",
      "Iteration: 757. Loss: 1.6504844427108765 \n",
      "Iteration: 758. Loss: 1.6974663734436035 \n",
      "Iteration: 759. Loss: 1.7082152366638184 \n",
      "Iteration: 760. Loss: 1.7183479070663452 \n",
      "Iteration: 761. Loss: 1.7213830947875977 \n",
      "Iteration: 762. Loss: 1.7482093572616577 \n",
      "Iteration: 763. Loss: 1.7418369054794312 \n",
      "Iteration: 764. Loss: 1.6964465379714966 \n",
      "Iteration: 765. Loss: 1.7126870155334473 \n",
      "Iteration: 766. Loss: 1.7472870349884033 \n",
      "Iteration: 767. Loss: 1.708427906036377 \n",
      "Iteration: 768. Loss: 1.6520376205444336 \n",
      "Iteration: 769. Loss: 1.6963316202163696 \n",
      "Iteration: 770. Loss: 1.6727912425994873 \n",
      "Iteration: 771. Loss: 1.6753621101379395 \n",
      "Iteration: 772. Loss: 1.691807508468628 \n",
      "Iteration: 773. Loss: 1.6761409044265747 \n",
      "Iteration: 774. Loss: 1.7208189964294434 \n",
      "Iteration: 775. Loss: 1.7440065145492554 \n",
      "Iteration: 776. Loss: 1.6922235488891602 \n",
      "Iteration: 777. Loss: 1.6894932985305786 \n",
      "Iteration: 778. Loss: 1.6978601217269897 \n",
      "Iteration: 779. Loss: 1.7041707038879395 \n",
      "Iteration: 780. Loss: 1.6970839500427246 \n",
      "Iteration: 781. Loss: 1.7466336488723755 \n",
      "Iteration: 782. Loss: 1.7406779527664185 \n",
      "Iteration: 783. Loss: 1.7022032737731934 \n",
      "Iteration: 784. Loss: 1.6554901599884033 \n",
      "Iteration: 785. Loss: 1.7439546585083008 \n",
      "Iteration: 786. Loss: 1.7112716436386108 \n",
      "Iteration: 787. Loss: 1.6697496175765991 \n",
      "Iteration: 788. Loss: 1.6406151056289673 \n",
      "Iteration: 789. Loss: 1.726169466972351 \n",
      "Iteration: 790. Loss: 1.6800721883773804 \n",
      "Iteration: 791. Loss: 1.6942814588546753 \n",
      "Iteration: 792. Loss: 1.7271418571472168 \n",
      "Iteration: 793. Loss: 1.6901545524597168 \n",
      "Iteration: 794. Loss: 1.6245169639587402 \n",
      "Iteration: 795. Loss: 1.6274657249450684 \n",
      "Iteration: 796. Loss: 1.7296624183654785 \n",
      "Iteration: 797. Loss: 1.637417197227478 \n",
      "Iteration: 798. Loss: 1.709680199623108 \n",
      "Iteration: 799. Loss: 1.6462461948394775 \n",
      "Iteration: 800. Loss: 1.6602970361709595 \n",
      "Iteration: 801. Loss: 1.722200632095337 \n",
      "Iteration: 802. Loss: 1.6756482124328613 \n",
      "Iteration: 803. Loss: 1.6969484090805054 \n",
      "Iteration: 804. Loss: 1.6892149448394775 \n",
      "Iteration: 805. Loss: 1.6810113191604614 \n",
      "Iteration: 806. Loss: 1.6642860174179077 \n",
      "Iteration: 807. Loss: 1.604955792427063 \n",
      "Iteration: 808. Loss: 1.6654552221298218 \n",
      "Iteration: 809. Loss: 1.6044539213180542 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 810. Loss: 1.6584992408752441 \n",
      "Iteration: 811. Loss: 1.6785943508148193 \n",
      "Iteration: 812. Loss: 1.6014307737350464 \n",
      "Iteration: 813. Loss: 1.7220584154129028 \n",
      "Iteration: 814. Loss: 1.7027521133422852 \n",
      "Iteration: 815. Loss: 1.6994227170944214 \n",
      "Iteration: 816. Loss: 1.6768766641616821 \n",
      "Iteration: 817. Loss: 1.685526728630066 \n",
      "Iteration: 818. Loss: 1.6341253519058228 \n",
      "Iteration: 819. Loss: 1.6339008808135986 \n",
      "Iteration: 820. Loss: 1.6416678428649902 \n",
      "Iteration: 821. Loss: 1.6435706615447998 \n",
      "Iteration: 822. Loss: 1.6516398191452026 \n",
      "Iteration: 823. Loss: 1.6974639892578125 \n",
      "Iteration: 824. Loss: 1.6388903856277466 \n",
      "Iteration: 825. Loss: 1.6408357620239258 \n",
      "Iteration: 826. Loss: 1.683703899383545 \n",
      "Iteration: 827. Loss: 1.6369634866714478 \n",
      "Iteration: 828. Loss: 1.6700830459594727 \n",
      "Iteration: 829. Loss: 1.632900595664978 \n",
      "Iteration: 830. Loss: 1.662790060043335 \n",
      "Iteration: 831. Loss: 1.6314294338226318 \n",
      "Iteration: 832. Loss: 1.6736239194869995 \n",
      "Iteration: 833. Loss: 1.5906097888946533 \n",
      "Iteration: 834. Loss: 1.599349856376648 \n",
      "Iteration: 835. Loss: 1.701905369758606 \n",
      "Iteration: 836. Loss: 1.6379870176315308 \n",
      "Iteration: 837. Loss: 1.66685152053833 \n",
      "Iteration: 838. Loss: 1.710647702217102 \n",
      "Iteration: 839. Loss: 1.6450648307800293 \n",
      "Iteration: 840. Loss: 1.663968801498413 \n",
      "Iteration: 841. Loss: 1.6532152891159058 \n",
      "Iteration: 842. Loss: 1.6662566661834717 \n",
      "Iteration: 843. Loss: 1.7051008939743042 \n",
      "Iteration: 844. Loss: 1.704247236251831 \n",
      "Iteration: 845. Loss: 1.662790060043335 \n",
      "Iteration: 846. Loss: 1.6709157228469849 \n",
      "Iteration: 847. Loss: 1.7702767848968506 \n",
      "Iteration: 848. Loss: 1.6206998825073242 \n",
      "Iteration: 849. Loss: 1.6498303413391113 \n",
      "Iteration: 850. Loss: 1.6443694829940796 \n",
      "Iteration: 851. Loss: 1.6315793991088867 \n",
      "Iteration: 852. Loss: 1.6184306144714355 \n",
      "Iteration: 853. Loss: 1.7182544469833374 \n",
      "Iteration: 854. Loss: 1.6253591775894165 \n",
      "Iteration: 855. Loss: 1.6461948156356812 \n",
      "Iteration: 856. Loss: 1.649418830871582 \n",
      "Iteration: 857. Loss: 1.6034654378890991 \n",
      "Iteration: 858. Loss: 1.6571152210235596 \n",
      "Iteration: 859. Loss: 1.6530729532241821 \n",
      "Iteration: 860. Loss: 1.6921287775039673 \n",
      "Iteration: 861. Loss: 1.5925202369689941 \n",
      "Iteration: 862. Loss: 1.6087586879730225 \n",
      "Iteration: 863. Loss: 1.6565754413604736 \n",
      "Iteration: 864. Loss: 1.597104549407959 \n",
      "Iteration: 865. Loss: 1.6837726831436157 \n",
      "Iteration: 866. Loss: 1.607266902923584 \n",
      "Iteration: 867. Loss: 1.6806045770645142 \n",
      "Iteration: 868. Loss: 1.6037969589233398 \n",
      "Iteration: 869. Loss: 1.6452046632766724 \n",
      "Iteration: 870. Loss: 1.6851927042007446 \n",
      "Iteration: 871. Loss: 1.624740481376648 \n",
      "Iteration: 872. Loss: 1.6344224214553833 \n",
      "Iteration: 873. Loss: 1.6173882484436035 \n",
      "Iteration: 874. Loss: 1.6322200298309326 \n",
      "Iteration: 875. Loss: 1.6875190734863281 \n",
      "Iteration: 876. Loss: 1.6490142345428467 \n",
      "Iteration: 877. Loss: 1.6915616989135742 \n",
      "Iteration: 878. Loss: 1.619603157043457 \n",
      "Iteration: 879. Loss: 1.584943413734436 \n",
      "Iteration: 880. Loss: 1.5864677429199219 \n",
      "Iteration: 881. Loss: 1.6725388765335083 \n",
      "Iteration: 882. Loss: 1.656667947769165 \n",
      "Iteration: 883. Loss: 1.5978846549987793 \n",
      "Iteration: 884. Loss: 1.6629152297973633 \n",
      "Iteration: 885. Loss: 1.6680047512054443 \n",
      "Iteration: 886. Loss: 1.5976868867874146 \n",
      "Iteration: 887. Loss: 1.6746242046356201 \n",
      "Iteration: 888. Loss: 1.6485270261764526 \n",
      "Iteration: 889. Loss: 1.601539134979248 \n",
      "Iteration: 890. Loss: 1.5754787921905518 \n",
      "Iteration: 891. Loss: 1.6289236545562744 \n",
      "Iteration: 892. Loss: 1.6511927843093872 \n",
      "Iteration: 893. Loss: 1.629399061203003 \n",
      "Iteration: 894. Loss: 1.6166812181472778 \n",
      "Iteration: 895. Loss: 1.6445324420928955 \n",
      "Iteration: 896. Loss: 1.6397610902786255 \n",
      "Iteration: 897. Loss: 1.6233941316604614 \n",
      "Iteration: 898. Loss: 1.6462364196777344 \n",
      "Iteration: 899. Loss: 1.56522798538208 \n",
      "Iteration: 900. Loss: 1.6084312200546265 \n",
      "Iteration: 901. Loss: 1.6619441509246826 \n",
      "Iteration: 902. Loss: 1.6179039478302002 \n",
      "Iteration: 903. Loss: 1.6687043905258179 \n",
      "Iteration: 904. Loss: 1.5840590000152588 \n",
      "Iteration: 905. Loss: 1.673994779586792 \n",
      "Iteration: 906. Loss: 1.6170157194137573 \n",
      "Iteration: 907. Loss: 1.6336976289749146 \n",
      "Iteration: 908. Loss: 1.5990835428237915 \n",
      "Iteration: 909. Loss: 1.6318103075027466 \n",
      "Iteration: 910. Loss: 1.5935903787612915 \n",
      "Iteration: 911. Loss: 1.5705695152282715 \n",
      "Iteration: 912. Loss: 1.6675132513046265 \n",
      "Iteration: 913. Loss: 1.5505071878433228 \n",
      "Iteration: 914. Loss: 1.5917054414749146 \n",
      "Iteration: 915. Loss: 1.6607530117034912 \n",
      "Iteration: 916. Loss: 1.6105849742889404 \n",
      "Iteration: 917. Loss: 1.6305149793624878 \n",
      "Iteration: 918. Loss: 1.6723217964172363 \n",
      "Iteration: 919. Loss: 1.6634361743927002 \n",
      "Iteration: 920. Loss: 1.546542763710022 \n",
      "Iteration: 921. Loss: 1.6723593473434448 \n",
      "Iteration: 922. Loss: 1.6488456726074219 \n",
      "Iteration: 923. Loss: 1.6549606323242188 \n",
      "Iteration: 924. Loss: 1.5889883041381836 \n",
      "Iteration: 925. Loss: 1.6513876914978027 \n",
      "Iteration: 926. Loss: 1.595386028289795 \n",
      "Iteration: 927. Loss: 1.589200496673584 \n",
      "Iteration: 928. Loss: 1.6209927797317505 \n",
      "Iteration: 929. Loss: 1.5418963432312012 \n",
      "Iteration: 930. Loss: 1.6379339694976807 \n",
      "Iteration: 931. Loss: 1.642836570739746 \n",
      "Iteration: 932. Loss: 1.6219162940979004 \n",
      "Iteration: 933. Loss: 1.6064633131027222 \n",
      "Iteration: 934. Loss: 1.6331720352172852 \n",
      "Iteration: 935. Loss: 1.5681698322296143 \n",
      "Iteration: 936. Loss: 1.6259273290634155 \n",
      "Iteration: 937. Loss: 1.5983662605285645 \n",
      "Iteration: 938. Loss: 1.6256303787231445 \n",
      "Iteration: 939. Loss: 1.646131157875061 \n",
      "Iteration: 940. Loss: 1.6619598865509033 \n",
      "Iteration: 941. Loss: 1.5277907848358154 \n",
      "Iteration: 942. Loss: 1.6022192239761353 \n",
      "Iteration: 943. Loss: 1.5665243864059448 \n",
      "Iteration: 944. Loss: 1.5956753492355347 \n",
      "Iteration: 945. Loss: 1.6064598560333252 \n",
      "Iteration: 946. Loss: 1.5454217195510864 \n",
      "Iteration: 947. Loss: 1.6317518949508667 \n",
      "Iteration: 948. Loss: 1.6703345775604248 \n",
      "Iteration: 949. Loss: 1.6657052040100098 \n",
      "Iteration: 950. Loss: 1.5465388298034668 \n",
      "Iteration: 951. Loss: 1.5447359085083008 \n",
      "Iteration: 952. Loss: 1.5653631687164307 \n",
      "Iteration: 953. Loss: 1.5917338132858276 \n",
      "Iteration: 954. Loss: 1.597589135169983 \n",
      "Iteration: 955. Loss: 1.5852583646774292 \n",
      "Iteration: 956. Loss: 1.588219404220581 \n",
      "Iteration: 957. Loss: 1.6018143892288208 \n",
      "Iteration: 958. Loss: 1.5968266725540161 \n",
      "Iteration: 959. Loss: 1.5401774644851685 \n",
      "Iteration: 960. Loss: 1.595199465751648 \n",
      "Iteration: 961. Loss: 1.5583276748657227 \n",
      "Iteration: 962. Loss: 1.6235660314559937 \n",
      "Iteration: 963. Loss: 1.5669583082199097 \n",
      "Iteration: 964. Loss: 1.581038236618042 \n",
      "Iteration: 965. Loss: 1.5138812065124512 \n",
      "Iteration: 966. Loss: 1.6292451620101929 \n",
      "Iteration: 967. Loss: 1.5174622535705566 \n",
      "Iteration: 968. Loss: 1.6163498163223267 \n",
      "Iteration: 969. Loss: 1.5942602157592773 \n",
      "Iteration: 970. Loss: 1.5699900388717651 \n",
      "Iteration: 971. Loss: 1.6037057638168335 \n",
      "Iteration: 972. Loss: 1.552904486656189 \n",
      "Iteration: 973. Loss: 1.5938066244125366 \n",
      "Iteration: 974. Loss: 1.593875527381897 \n",
      "Iteration: 975. Loss: 1.4946844577789307 \n",
      "Iteration: 976. Loss: 1.5701954364776611 \n",
      "Iteration: 977. Loss: 1.5983575582504272 \n",
      "Iteration: 978. Loss: 1.5879848003387451 \n",
      "Iteration: 979. Loss: 1.5780338048934937 \n",
      "Iteration: 980. Loss: 1.57393217086792 \n",
      "Iteration: 981. Loss: 1.6077842712402344 \n",
      "Iteration: 982. Loss: 1.602257490158081 \n",
      "Iteration: 983. Loss: 1.5933201313018799 \n",
      "Iteration: 984. Loss: 1.5723636150360107 \n",
      "Iteration: 985. Loss: 1.5749192237854004 \n",
      "Iteration: 986. Loss: 1.5947048664093018 \n",
      "Iteration: 987. Loss: 1.5685888528823853 \n",
      "Iteration: 988. Loss: 1.57171630859375 \n",
      "Iteration: 989. Loss: 1.624727487564087 \n",
      "Iteration: 990. Loss: 1.5479261875152588 \n",
      "Iteration: 991. Loss: 1.5916465520858765 \n",
      "Iteration: 992. Loss: 1.5094050168991089 \n",
      "Iteration: 993. Loss: 1.571094036102295 \n",
      "Iteration: 994. Loss: 1.5681006908416748 \n",
      "Iteration: 995. Loss: 1.5282752513885498 \n",
      "Iteration: 996. Loss: 1.5341055393218994 \n",
      "Iteration: 997. Loss: 1.5608690977096558 \n",
      "Iteration: 998. Loss: 1.5647170543670654 \n",
      "Iteration: 999. Loss: 1.617324948310852 \n",
      "Iteration: 1000. Loss: 1.5209025144577026 \n",
      "Iteration: 1001. Loss: 1.5701960325241089 \n",
      "Iteration: 1002. Loss: 1.5799453258514404 \n",
      "Iteration: 1003. Loss: 1.5409976243972778 \n",
      "Iteration: 1004. Loss: 1.5193405151367188 \n",
      "Iteration: 1005. Loss: 1.5701152086257935 \n",
      "Iteration: 1006. Loss: 1.5816662311553955 \n",
      "Iteration: 1007. Loss: 1.463008999824524 \n",
      "Iteration: 1008. Loss: 1.548323392868042 \n",
      "Iteration: 1009. Loss: 1.5655921697616577 \n",
      "Iteration: 1010. Loss: 1.5434961318969727 \n",
      "Iteration: 1011. Loss: 1.4661858081817627 \n",
      "Iteration: 1012. Loss: 1.6171070337295532 \n",
      "Iteration: 1013. Loss: 1.5901302099227905 \n",
      "Iteration: 1014. Loss: 1.548305630683899 \n",
      "Iteration: 1015. Loss: 1.551863431930542 \n",
      "Iteration: 1016. Loss: 1.5569206476211548 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1017. Loss: 1.5147088766098022 \n",
      "Iteration: 1018. Loss: 1.5302296876907349 \n",
      "Iteration: 1019. Loss: 1.5615954399108887 \n",
      "Iteration: 1020. Loss: 1.549422264099121 \n",
      "Iteration: 1021. Loss: 1.6023037433624268 \n",
      "Iteration: 1022. Loss: 1.5555044412612915 \n",
      "Iteration: 1023. Loss: 1.6157407760620117 \n",
      "Iteration: 1024. Loss: 1.651595115661621 \n",
      "Iteration: 1025. Loss: 1.568713665008545 \n",
      "Iteration: 1026. Loss: 1.5658260583877563 \n",
      "Iteration: 1027. Loss: 1.5133377313613892 \n",
      "Iteration: 1028. Loss: 1.5492099523544312 \n",
      "Iteration: 1029. Loss: 1.5286645889282227 \n",
      "Iteration: 1030. Loss: 1.6399937868118286 \n",
      "Iteration: 1031. Loss: 1.6428008079528809 \n",
      "Iteration: 1032. Loss: 1.5927093029022217 \n",
      "Iteration: 1033. Loss: 1.5830044746398926 \n",
      "Iteration: 1034. Loss: 1.568647027015686 \n",
      "Iteration: 1035. Loss: 1.5816150903701782 \n",
      "Iteration: 1036. Loss: 1.6032484769821167 \n",
      "Iteration: 1037. Loss: 1.5534486770629883 \n",
      "Iteration: 1038. Loss: 1.6334723234176636 \n",
      "Iteration: 1039. Loss: 1.5933506488800049 \n",
      "Iteration: 1040. Loss: 1.5621775388717651 \n",
      "Iteration: 1041. Loss: 1.5474728345870972 \n",
      "Iteration: 1042. Loss: 1.4913396835327148 \n",
      "Iteration: 1043. Loss: 1.5354247093200684 \n",
      "Iteration: 1044. Loss: 1.500311017036438 \n",
      "Iteration: 1045. Loss: 1.5577049255371094 \n",
      "Iteration: 1046. Loss: 1.5630875825881958 \n",
      "Iteration: 1047. Loss: 1.5048534870147705 \n",
      "Iteration: 1048. Loss: 1.5758507251739502 \n",
      "Iteration: 1049. Loss: 1.5827574729919434 \n",
      "Iteration: 1050. Loss: 1.5237243175506592 \n",
      "Iteration: 1051. Loss: 1.4841399192810059 \n",
      "Iteration: 1052. Loss: 1.5019413232803345 \n",
      "Iteration: 1053. Loss: 1.5268974304199219 \n",
      "Iteration: 1054. Loss: 1.5891735553741455 \n",
      "Iteration: 1055. Loss: 1.4983983039855957 \n",
      "Iteration: 1056. Loss: 1.5412083864212036 \n",
      "Iteration: 1057. Loss: 1.4615814685821533 \n",
      "Iteration: 1058. Loss: 1.5214821100234985 \n",
      "Iteration: 1059. Loss: 1.5559712648391724 \n",
      "Iteration: 1060. Loss: 1.577363133430481 \n",
      "Iteration: 1061. Loss: 1.4990860223770142 \n",
      "Iteration: 1062. Loss: 1.5966277122497559 \n",
      "Iteration: 1063. Loss: 1.52885103225708 \n",
      "Iteration: 1064. Loss: 1.518309473991394 \n",
      "Iteration: 1065. Loss: 1.5157853364944458 \n",
      "Iteration: 1066. Loss: 1.520473599433899 \n",
      "Iteration: 1067. Loss: 1.6276894807815552 \n",
      "Iteration: 1068. Loss: 1.609020709991455 \n",
      "Iteration: 1069. Loss: 1.5006617307662964 \n",
      "Iteration: 1070. Loss: 1.5269410610198975 \n",
      "Iteration: 1071. Loss: 1.6396446228027344 \n",
      "Iteration: 1072. Loss: 1.5654774904251099 \n",
      "Iteration: 1073. Loss: 1.5902632474899292 \n",
      "Iteration: 1074. Loss: 1.5661851167678833 \n",
      "Iteration: 1075. Loss: 1.562037706375122 \n",
      "Iteration: 1076. Loss: 1.5418411493301392 \n",
      "Iteration: 1077. Loss: 1.5077908039093018 \n",
      "Iteration: 1078. Loss: 1.5476243495941162 \n",
      "Iteration: 1079. Loss: 1.5113487243652344 \n",
      "Iteration: 1080. Loss: 1.5347033739089966 \n",
      "Iteration: 1081. Loss: 1.4984735250473022 \n",
      "Iteration: 1082. Loss: 1.516629695892334 \n",
      "Iteration: 1083. Loss: 1.5008316040039062 \n",
      "Iteration: 1084. Loss: 1.5378721952438354 \n",
      "Iteration: 1085. Loss: 1.5901294946670532 \n",
      "Iteration: 1086. Loss: 1.4801242351531982 \n",
      "Iteration: 1087. Loss: 1.5265384912490845 \n",
      "Iteration: 1088. Loss: 1.4929372072219849 \n",
      "Iteration: 1089. Loss: 1.5668895244598389 \n",
      "Iteration: 1090. Loss: 1.4985111951828003 \n",
      "Iteration: 1091. Loss: 1.4701495170593262 \n",
      "Iteration: 1092. Loss: 1.5582003593444824 \n",
      "Iteration: 1093. Loss: 1.5172635316848755 \n",
      "Iteration: 1094. Loss: 1.5297552347183228 \n",
      "Iteration: 1095. Loss: 1.570261836051941 \n",
      "Iteration: 1096. Loss: 1.4962795972824097 \n",
      "Iteration: 1097. Loss: 1.4968947172164917 \n",
      "Iteration: 1098. Loss: 1.4750316143035889 \n",
      "Iteration: 1099. Loss: 1.5278871059417725 \n",
      "Iteration: 1100. Loss: 1.519094705581665 \n",
      "Iteration: 1101. Loss: 1.5409141778945923 \n",
      "Iteration: 1102. Loss: 1.575240969657898 \n",
      "Iteration: 1103. Loss: 1.5713677406311035 \n",
      "Iteration: 1104. Loss: 1.5812618732452393 \n",
      "Iteration: 1105. Loss: 1.606858491897583 \n",
      "Iteration: 1106. Loss: 1.5314959287643433 \n",
      "Iteration: 1107. Loss: 1.480362892150879 \n",
      "Iteration: 1108. Loss: 1.5790308713912964 \n",
      "Iteration: 1109. Loss: 1.5357017517089844 \n",
      "Iteration: 1110. Loss: 1.4630818367004395 \n",
      "Iteration: 1111. Loss: 1.573244571685791 \n",
      "Iteration: 1112. Loss: 1.5135043859481812 \n",
      "Iteration: 1113. Loss: 1.5316455364227295 \n",
      "Iteration: 1114. Loss: 1.4988257884979248 \n",
      "Iteration: 1115. Loss: 1.5158796310424805 \n",
      "Iteration: 1116. Loss: 1.5417377948760986 \n",
      "Iteration: 1117. Loss: 1.4890857934951782 \n",
      "Iteration: 1118. Loss: 1.501432180404663 \n",
      "Iteration: 1119. Loss: 1.4817817211151123 \n",
      "Iteration: 1120. Loss: 1.5161596536636353 \n",
      "Iteration: 1121. Loss: 1.5239191055297852 \n",
      "Iteration: 1122. Loss: 1.512906551361084 \n",
      "Iteration: 1123. Loss: 1.5134140253067017 \n",
      "Iteration: 1124. Loss: 1.4882158041000366 \n",
      "Iteration: 1125. Loss: 1.5264391899108887 \n",
      "Iteration: 1126. Loss: 1.410702109336853 \n",
      "Iteration: 1127. Loss: 1.536486029624939 \n",
      "Iteration: 1128. Loss: 1.5041542053222656 \n",
      "Iteration: 1129. Loss: 1.5265637636184692 \n",
      "Iteration: 1130. Loss: 1.4871020317077637 \n",
      "Iteration: 1131. Loss: 1.4726043939590454 \n",
      "Iteration: 1132. Loss: 1.4354404211044312 \n",
      "Iteration: 1133. Loss: 1.5410176515579224 \n",
      "Iteration: 1134. Loss: 1.5726006031036377 \n",
      "Iteration: 1135. Loss: 1.5096098184585571 \n",
      "Iteration: 1136. Loss: 1.51185941696167 \n",
      "Iteration: 1137. Loss: 1.5001150369644165 \n",
      "Iteration: 1138. Loss: 1.4755659103393555 \n",
      "Iteration: 1139. Loss: 1.576934814453125 \n",
      "Iteration: 1140. Loss: 1.5094050168991089 \n",
      "Iteration: 1141. Loss: 1.4449551105499268 \n",
      "Iteration: 1142. Loss: 1.5679702758789062 \n",
      "Iteration: 1143. Loss: 1.544510006904602 \n",
      "Iteration: 1144. Loss: 1.4641337394714355 \n",
      "Iteration: 1145. Loss: 1.4693259000778198 \n",
      "Iteration: 1146. Loss: 1.5185896158218384 \n",
      "Iteration: 1147. Loss: 1.4142576456069946 \n",
      "Iteration: 1148. Loss: 1.5649546384811401 \n",
      "Iteration: 1149. Loss: 1.5297203063964844 \n",
      "Iteration: 1150. Loss: 1.4968034029006958 \n",
      "Iteration: 1151. Loss: 1.429538607597351 \n",
      "Iteration: 1152. Loss: 1.4114336967468262 \n",
      "Iteration: 1153. Loss: 1.5170565843582153 \n",
      "Iteration: 1154. Loss: 1.5841022729873657 \n",
      "Iteration: 1155. Loss: 1.5325686931610107 \n",
      "Iteration: 1156. Loss: 1.4338546991348267 \n",
      "Iteration: 1157. Loss: 1.4995646476745605 \n",
      "Iteration: 1158. Loss: 1.6304447650909424 \n",
      "Iteration: 1159. Loss: 1.5189316272735596 \n",
      "Iteration: 1160. Loss: 1.5172985792160034 \n",
      "Iteration: 1161. Loss: 1.5298980474472046 \n",
      "Iteration: 1162. Loss: 1.435298204421997 \n",
      "Iteration: 1163. Loss: 1.5750374794006348 \n",
      "Iteration: 1164. Loss: 1.4687243700027466 \n",
      "Iteration: 1165. Loss: 1.5794415473937988 \n",
      "Iteration: 1166. Loss: 1.498084306716919 \n",
      "Iteration: 1167. Loss: 1.4553760290145874 \n",
      "Iteration: 1168. Loss: 1.4578040838241577 \n",
      "Iteration: 1169. Loss: 1.4539402723312378 \n",
      "Iteration: 1170. Loss: 1.5040141344070435 \n",
      "Iteration: 1171. Loss: 1.4973546266555786 \n",
      "Iteration: 1172. Loss: 1.5965521335601807 \n",
      "Iteration: 1173. Loss: 1.4552000761032104 \n",
      "Iteration: 1174. Loss: 1.5153933763504028 \n",
      "Iteration: 1175. Loss: 1.4287375211715698 \n",
      "Iteration: 1176. Loss: 1.436426043510437 \n",
      "Iteration: 1177. Loss: 1.5094962120056152 \n",
      "Iteration: 1178. Loss: 1.3935446739196777 \n",
      "Iteration: 1179. Loss: 1.3948426246643066 \n",
      "Iteration: 1180. Loss: 1.4377000331878662 \n",
      "Iteration: 1181. Loss: 1.5009338855743408 \n",
      "Iteration: 1182. Loss: 1.4338030815124512 \n",
      "Iteration: 1183. Loss: 1.4314388036727905 \n",
      "Iteration: 1184. Loss: 1.4539310932159424 \n",
      "Iteration: 1185. Loss: 1.4836022853851318 \n",
      "Iteration: 1186. Loss: 1.4983793497085571 \n",
      "Iteration: 1187. Loss: 1.4695264101028442 \n",
      "Iteration: 1188. Loss: 1.4459564685821533 \n",
      "Iteration: 1189. Loss: 1.4396998882293701 \n",
      "Iteration: 1190. Loss: 1.5480661392211914 \n",
      "Iteration: 1191. Loss: 1.5092581510543823 \n",
      "Iteration: 1192. Loss: 1.4528864622116089 \n",
      "Iteration: 1193. Loss: 1.4541287422180176 \n",
      "Iteration: 1194. Loss: 1.4593380689620972 \n",
      "Iteration: 1195. Loss: 1.540238380432129 \n",
      "Iteration: 1196. Loss: 1.5394668579101562 \n",
      "Iteration: 1197. Loss: 1.4438529014587402 \n",
      "Iteration: 1198. Loss: 1.509466528892517 \n",
      "Iteration: 1199. Loss: 1.4802067279815674 \n",
      "Iteration: 1200. Loss: 1.4681469202041626 \n",
      "Iteration: 1201. Loss: 1.4953845739364624 \n",
      "Iteration: 1202. Loss: 1.5727219581604004 \n",
      "Iteration: 1203. Loss: 1.5282303094863892 \n",
      "Iteration: 1204. Loss: 1.4270869493484497 \n",
      "Iteration: 1205. Loss: 1.5060054063796997 \n",
      "Iteration: 1206. Loss: 1.506759524345398 \n",
      "Iteration: 1207. Loss: 1.466349482536316 \n",
      "Iteration: 1208. Loss: 1.476222038269043 \n",
      "Iteration: 1209. Loss: 1.5102537870407104 \n",
      "Iteration: 1210. Loss: 1.4705923795700073 \n",
      "Iteration: 1211. Loss: 1.4868922233581543 \n",
      "Iteration: 1212. Loss: 1.4601755142211914 \n",
      "Iteration: 1213. Loss: 1.4837037324905396 \n",
      "Iteration: 1214. Loss: 1.4883140325546265 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1215. Loss: 1.462920069694519 \n",
      "Iteration: 1216. Loss: 1.4911233186721802 \n",
      "Iteration: 1217. Loss: 1.4761461019515991 \n",
      "Iteration: 1218. Loss: 1.4387129545211792 \n",
      "Iteration: 1219. Loss: 1.4805041551589966 \n",
      "Iteration: 1220. Loss: 1.4792011976242065 \n",
      "Iteration: 1221. Loss: 1.463564395904541 \n",
      "Iteration: 1222. Loss: 1.4361586570739746 \n",
      "Iteration: 1223. Loss: 1.5066008567810059 \n",
      "Iteration: 1224. Loss: 1.4235749244689941 \n",
      "Iteration: 1225. Loss: 1.4634441137313843 \n",
      "Iteration: 1226. Loss: 1.4295424222946167 \n",
      "Iteration: 1227. Loss: 1.5244144201278687 \n",
      "Iteration: 1228. Loss: 1.4558697938919067 \n",
      "Iteration: 1229. Loss: 1.4725923538208008 \n",
      "Iteration: 1230. Loss: 1.5305255651474 \n",
      "Iteration: 1231. Loss: 1.4184868335723877 \n",
      "Iteration: 1232. Loss: 1.3702539205551147 \n",
      "Iteration: 1233. Loss: 1.452121376991272 \n",
      "Iteration: 1234. Loss: 1.5283567905426025 \n",
      "Iteration: 1235. Loss: 1.4570976495742798 \n",
      "Iteration: 1236. Loss: 1.488940715789795 \n",
      "Iteration: 1237. Loss: 1.5049911737442017 \n",
      "Iteration: 1238. Loss: 1.5220288038253784 \n",
      "Iteration: 1239. Loss: 1.4929718971252441 \n",
      "Iteration: 1240. Loss: 1.4650381803512573 \n",
      "Iteration: 1241. Loss: 1.4569060802459717 \n",
      "Iteration: 1242. Loss: 1.3668456077575684 \n",
      "Iteration: 1243. Loss: 1.4679863452911377 \n",
      "Iteration: 1244. Loss: 1.5034445524215698 \n",
      "Iteration: 1245. Loss: 1.3729480504989624 \n",
      "Iteration: 1246. Loss: 1.5224272012710571 \n",
      "Iteration: 1247. Loss: 1.46028733253479 \n",
      "Iteration: 1248. Loss: 1.43800950050354 \n",
      "Iteration: 1249. Loss: 1.4506518840789795 \n",
      "Iteration: 1250. Loss: 1.483396291732788 \n",
      "Iteration: 1251. Loss: 1.4772825241088867 \n",
      "Iteration: 1252. Loss: 1.566068410873413 \n",
      "Iteration: 1253. Loss: 1.4388920068740845 \n",
      "Iteration: 1254. Loss: 1.4854233264923096 \n",
      "Iteration: 1255. Loss: 1.4840985536575317 \n",
      "Iteration: 1256. Loss: 1.4888818264007568 \n",
      "Iteration: 1257. Loss: 1.4106636047363281 \n",
      "Iteration: 1258. Loss: 1.4673439264297485 \n",
      "Iteration: 1259. Loss: 1.3674360513687134 \n",
      "Iteration: 1260. Loss: 1.4185329675674438 \n",
      "Iteration: 1261. Loss: 1.3928793668746948 \n",
      "Iteration: 1262. Loss: 1.4494388103485107 \n",
      "Iteration: 1263. Loss: 1.4501265287399292 \n",
      "Iteration: 1264. Loss: 1.4759386777877808 \n",
      "Iteration: 1265. Loss: 1.4235490560531616 \n",
      "Iteration: 1266. Loss: 1.440887451171875 \n",
      "Iteration: 1267. Loss: 1.463344693183899 \n",
      "Iteration: 1268. Loss: 1.3860290050506592 \n",
      "Iteration: 1269. Loss: 1.4487496614456177 \n",
      "Iteration: 1270. Loss: 1.4382144212722778 \n",
      "Iteration: 1271. Loss: 1.4971191883087158 \n",
      "Iteration: 1272. Loss: 1.3672038316726685 \n",
      "Iteration: 1273. Loss: 1.4575592279434204 \n",
      "Iteration: 1274. Loss: 1.4779047966003418 \n",
      "Iteration: 1275. Loss: 1.4311622381210327 \n",
      "Iteration: 1276. Loss: 1.4327425956726074 \n",
      "Iteration: 1277. Loss: 1.4694589376449585 \n",
      "Iteration: 1278. Loss: 1.4302045106887817 \n",
      "Iteration: 1279. Loss: 1.4164831638336182 \n",
      "Iteration: 1280. Loss: 1.4482645988464355 \n",
      "Iteration: 1281. Loss: 1.3846515417099 \n",
      "Iteration: 1282. Loss: 1.408859372138977 \n",
      "Iteration: 1283. Loss: 1.407802700996399 \n",
      "Iteration: 1284. Loss: 1.3907314538955688 \n",
      "Iteration: 1285. Loss: 1.5126464366912842 \n",
      "Iteration: 1286. Loss: 1.4363586902618408 \n",
      "Iteration: 1287. Loss: 1.4248133897781372 \n",
      "Iteration: 1288. Loss: 1.4624390602111816 \n",
      "Iteration: 1289. Loss: 1.402341365814209 \n",
      "Iteration: 1290. Loss: 1.4504998922348022 \n",
      "Iteration: 1291. Loss: 1.4627130031585693 \n",
      "Iteration: 1292. Loss: 1.4325013160705566 \n",
      "Iteration: 1293. Loss: 1.4588772058486938 \n",
      "Iteration: 1294. Loss: 1.416977047920227 \n",
      "Iteration: 1295. Loss: 1.4371631145477295 \n",
      "Iteration: 1296. Loss: 1.499528408050537 \n",
      "Iteration: 1297. Loss: 1.33235502243042 \n",
      "Iteration: 1298. Loss: 1.3991999626159668 \n",
      "Iteration: 1299. Loss: 1.3891254663467407 \n",
      "Iteration: 1300. Loss: 1.430104374885559 \n",
      "Iteration: 1301. Loss: 1.4380656480789185 \n",
      "Iteration: 1302. Loss: 1.48486328125 \n",
      "Iteration: 1303. Loss: 1.4444918632507324 \n",
      "Iteration: 1304. Loss: 1.4687891006469727 \n",
      "Iteration: 1305. Loss: 1.4604142904281616 \n",
      "Iteration: 1306. Loss: 1.4349875450134277 \n",
      "Iteration: 1307. Loss: 1.4010872840881348 \n",
      "Iteration: 1308. Loss: 1.4494441747665405 \n",
      "Iteration: 1309. Loss: 1.4091132879257202 \n",
      "Iteration: 1310. Loss: 1.4469435214996338 \n",
      "Iteration: 1311. Loss: 1.4682669639587402 \n",
      "Iteration: 1312. Loss: 1.4211723804473877 \n",
      "Iteration: 1313. Loss: 1.4407789707183838 \n",
      "Iteration: 1314. Loss: 1.3396154642105103 \n",
      "Iteration: 1315. Loss: 1.4600616693496704 \n",
      "Iteration: 1316. Loss: 1.3654065132141113 \n",
      "Iteration: 1317. Loss: 1.4213306903839111 \n",
      "Iteration: 1318. Loss: 1.4725803136825562 \n",
      "Iteration: 1319. Loss: 1.3750478029251099 \n",
      "Iteration: 1320. Loss: 1.4179795980453491 \n",
      "Iteration: 1321. Loss: 1.441468358039856 \n",
      "Iteration: 1322. Loss: 1.482333779335022 \n",
      "Iteration: 1323. Loss: 1.4032864570617676 \n",
      "Iteration: 1324. Loss: 1.4041186571121216 \n",
      "Iteration: 1325. Loss: 1.4979978799819946 \n",
      "Iteration: 1326. Loss: 1.4466739892959595 \n",
      "Iteration: 1327. Loss: 1.3531630039215088 \n",
      "Iteration: 1328. Loss: 1.4532749652862549 \n",
      "Iteration: 1329. Loss: 1.409924030303955 \n",
      "Iteration: 1330. Loss: 1.3811589479446411 \n",
      "Iteration: 1331. Loss: 1.3962502479553223 \n",
      "Iteration: 1332. Loss: 1.4038974046707153 \n",
      "Iteration: 1333. Loss: 1.4248069524765015 \n",
      "Iteration: 1334. Loss: 1.368162989616394 \n",
      "Iteration: 1335. Loss: 1.4503540992736816 \n",
      "Iteration: 1336. Loss: 1.4469199180603027 \n",
      "Iteration: 1337. Loss: 1.478487491607666 \n",
      "Iteration: 1338. Loss: 1.467395305633545 \n",
      "Iteration: 1339. Loss: 1.3964189291000366 \n",
      "Iteration: 1340. Loss: 1.4719470739364624 \n",
      "Iteration: 1341. Loss: 1.5093154907226562 \n",
      "Iteration: 1342. Loss: 1.424077033996582 \n",
      "Iteration: 1343. Loss: 1.3767722845077515 \n",
      "Iteration: 1344. Loss: 1.455080509185791 \n",
      "Iteration: 1345. Loss: 1.4408044815063477 \n",
      "Iteration: 1346. Loss: 1.4412072896957397 \n",
      "Iteration: 1347. Loss: 1.4392508268356323 \n",
      "Iteration: 1348. Loss: 1.3850445747375488 \n",
      "Iteration: 1349. Loss: 1.482456088066101 \n",
      "Iteration: 1350. Loss: 1.3992791175842285 \n",
      "Iteration: 1351. Loss: 1.4632439613342285 \n",
      "Iteration: 1352. Loss: 1.4204192161560059 \n",
      "Iteration: 1353. Loss: 1.4631381034851074 \n",
      "Iteration: 1354. Loss: 1.4358553886413574 \n",
      "Iteration: 1355. Loss: 1.349082350730896 \n",
      "Iteration: 1356. Loss: 1.40567946434021 \n",
      "Iteration: 1357. Loss: 1.4277328252792358 \n",
      "Iteration: 1358. Loss: 1.4847800731658936 \n",
      "Iteration: 1359. Loss: 1.4190865755081177 \n",
      "Iteration: 1360. Loss: 1.4133496284484863 \n",
      "Iteration: 1361. Loss: 1.4241009950637817 \n",
      "Iteration: 1362. Loss: 1.3309390544891357 \n",
      "Iteration: 1363. Loss: 1.3684393167495728 \n",
      "Iteration: 1364. Loss: 1.418564796447754 \n",
      "Iteration: 1365. Loss: 1.434838056564331 \n",
      "Iteration: 1366. Loss: 1.45867919921875 \n",
      "Iteration: 1367. Loss: 1.3843995332717896 \n",
      "Iteration: 1368. Loss: 1.4242063760757446 \n",
      "Iteration: 1369. Loss: 1.392439603805542 \n",
      "Iteration: 1370. Loss: 1.3575439453125 \n",
      "Iteration: 1371. Loss: 1.4985090494155884 \n",
      "Iteration: 1372. Loss: 1.359424114227295 \n",
      "Iteration: 1373. Loss: 1.4248052835464478 \n",
      "Iteration: 1374. Loss: 1.4204961061477661 \n",
      "Iteration: 1375. Loss: 1.4295263290405273 \n",
      "Iteration: 1376. Loss: 1.4348841905593872 \n",
      "Iteration: 1377. Loss: 1.4205694198608398 \n",
      "Iteration: 1378. Loss: 1.3872344493865967 \n",
      "Iteration: 1379. Loss: 1.315720796585083 \n",
      "Iteration: 1380. Loss: 1.3898687362670898 \n",
      "Iteration: 1381. Loss: 1.3150932788848877 \n",
      "Iteration: 1382. Loss: 1.4533990621566772 \n",
      "Iteration: 1383. Loss: 1.3761377334594727 \n",
      "Iteration: 1384. Loss: 1.439054012298584 \n",
      "Iteration: 1385. Loss: 1.4110909700393677 \n",
      "Iteration: 1386. Loss: 1.4845408201217651 \n",
      "Iteration: 1387. Loss: 1.4768627882003784 \n",
      "Iteration: 1388. Loss: 1.39858078956604 \n",
      "Iteration: 1389. Loss: 1.3436082601547241 \n",
      "Iteration: 1390. Loss: 1.4166842699050903 \n",
      "Iteration: 1391. Loss: 1.4006141424179077 \n",
      "Iteration: 1392. Loss: 1.387622356414795 \n",
      "Iteration: 1393. Loss: 1.4137042760849 \n",
      "Iteration: 1394. Loss: 1.4225196838378906 \n",
      "Iteration: 1395. Loss: 1.4161887168884277 \n",
      "Iteration: 1396. Loss: 1.325240969657898 \n",
      "Iteration: 1397. Loss: 1.37449312210083 \n",
      "Iteration: 1398. Loss: 1.38924241065979 \n",
      "Iteration: 1399. Loss: 1.344525933265686 \n",
      "Iteration: 1400. Loss: 1.4793277978897095 \n",
      "Iteration: 1401. Loss: 1.3400886058807373 \n",
      "Iteration: 1402. Loss: 1.3865383863449097 \n",
      "Iteration: 1403. Loss: 1.3117927312850952 \n",
      "Iteration: 1404. Loss: 1.3422634601593018 \n",
      "Iteration: 1405. Loss: 1.4754198789596558 \n",
      "Iteration: 1406. Loss: 1.4147789478302002 \n",
      "Iteration: 1407. Loss: 1.4203670024871826 \n",
      "Iteration: 1408. Loss: 1.4443532228469849 \n",
      "Iteration: 1409. Loss: 1.440610647201538 \n",
      "Iteration: 1410. Loss: 1.4383063316345215 \n",
      "Iteration: 1411. Loss: 1.4569146633148193 \n",
      "Iteration: 1412. Loss: 1.3890358209609985 \n",
      "Iteration: 1413. Loss: 1.3684638738632202 \n",
      "Iteration: 1414. Loss: 1.3581912517547607 \n",
      "Iteration: 1415. Loss: 1.4685972929000854 \n",
      "Iteration: 1416. Loss: 1.2828059196472168 \n",
      "Iteration: 1417. Loss: 1.4515750408172607 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1418. Loss: 1.399145245552063 \n",
      "Iteration: 1419. Loss: 1.3357642889022827 \n",
      "Iteration: 1420. Loss: 1.425087571144104 \n",
      "Iteration: 1421. Loss: 1.3853983879089355 \n",
      "Iteration: 1422. Loss: 1.3741892576217651 \n",
      "Iteration: 1423. Loss: 1.3408055305480957 \n",
      "Iteration: 1424. Loss: 1.444465160369873 \n",
      "Iteration: 1425. Loss: 1.323887825012207 \n",
      "Iteration: 1426. Loss: 1.4499090909957886 \n",
      "Iteration: 1427. Loss: 1.383192539215088 \n",
      "Iteration: 1428. Loss: 1.4728273153305054 \n",
      "Iteration: 1429. Loss: 1.3748691082000732 \n",
      "Iteration: 1430. Loss: 1.4050416946411133 \n",
      "Iteration: 1431. Loss: 1.4178177118301392 \n",
      "Iteration: 1432. Loss: 1.366960883140564 \n",
      "Iteration: 1433. Loss: 1.3181239366531372 \n",
      "Iteration: 1434. Loss: 1.3893088102340698 \n",
      "Iteration: 1435. Loss: 1.3052406311035156 \n",
      "Iteration: 1436. Loss: 1.3469064235687256 \n",
      "Iteration: 1437. Loss: 1.4012941122055054 \n",
      "Iteration: 1438. Loss: 1.3868844509124756 \n",
      "Iteration: 1439. Loss: 1.3005876541137695 \n",
      "Iteration: 1440. Loss: 1.4715986251831055 \n",
      "Iteration: 1441. Loss: 1.3363871574401855 \n",
      "Iteration: 1442. Loss: 1.4397821426391602 \n",
      "Iteration: 1443. Loss: 1.297460675239563 \n",
      "Iteration: 1444. Loss: 1.4039559364318848 \n",
      "Iteration: 1445. Loss: 1.4647705554962158 \n",
      "Iteration: 1446. Loss: 1.4014997482299805 \n",
      "Iteration: 1447. Loss: 1.3505680561065674 \n",
      "Iteration: 1448. Loss: 1.4175117015838623 \n",
      "Iteration: 1449. Loss: 1.4528541564941406 \n",
      "Iteration: 1450. Loss: 1.3517001867294312 \n",
      "Iteration: 1451. Loss: 1.3917142152786255 \n",
      "Iteration: 1452. Loss: 1.430814266204834 \n",
      "Iteration: 1453. Loss: 1.4079158306121826 \n",
      "Iteration: 1454. Loss: 1.433320164680481 \n",
      "Iteration: 1455. Loss: 1.492335319519043 \n",
      "Iteration: 1456. Loss: 1.2802860736846924 \n",
      "Iteration: 1457. Loss: 1.298529863357544 \n",
      "Iteration: 1458. Loss: 1.3221867084503174 \n",
      "Iteration: 1459. Loss: 1.3637700080871582 \n",
      "Iteration: 1460. Loss: 1.3591034412384033 \n",
      "Iteration: 1461. Loss: 1.2969870567321777 \n",
      "Iteration: 1462. Loss: 1.3566720485687256 \n",
      "Iteration: 1463. Loss: 1.308709740638733 \n",
      "Iteration: 1464. Loss: 1.3265717029571533 \n",
      "Iteration: 1465. Loss: 1.3673467636108398 \n",
      "Iteration: 1466. Loss: 1.365806221961975 \n",
      "Iteration: 1467. Loss: 1.3932331800460815 \n",
      "Iteration: 1468. Loss: 1.3398349285125732 \n",
      "Iteration: 1469. Loss: 1.401557207107544 \n",
      "Iteration: 1470. Loss: 1.384953498840332 \n",
      "Iteration: 1471. Loss: 1.4151729345321655 \n",
      "Iteration: 1472. Loss: 1.3708451986312866 \n",
      "Iteration: 1473. Loss: 1.4202953577041626 \n",
      "Iteration: 1474. Loss: 1.3552310466766357 \n",
      "Iteration: 1475. Loss: 1.3372414112091064 \n",
      "Iteration: 1476. Loss: 1.3292485475540161 \n",
      "Iteration: 1477. Loss: 1.4043620824813843 \n",
      "Iteration: 1478. Loss: 1.3163514137268066 \n",
      "Iteration: 1479. Loss: 1.4500541687011719 \n",
      "Iteration: 1480. Loss: 1.3733770847320557 \n",
      "Iteration: 1481. Loss: 1.3768837451934814 \n",
      "Iteration: 1482. Loss: 1.4287800788879395 \n",
      "Iteration: 1483. Loss: 1.3515697717666626 \n",
      "Iteration: 1484. Loss: 1.4503675699234009 \n",
      "Iteration: 1485. Loss: 1.358859896659851 \n",
      "Iteration: 1486. Loss: 1.3753705024719238 \n",
      "Iteration: 1487. Loss: 1.3540208339691162 \n",
      "Iteration: 1488. Loss: 1.3389973640441895 \n",
      "Iteration: 1489. Loss: 1.373153567314148 \n",
      "Iteration: 1490. Loss: 1.3866633176803589 \n",
      "Iteration: 1491. Loss: 1.3418279886245728 \n",
      "Iteration: 1492. Loss: 1.3669782876968384 \n",
      "Iteration: 1493. Loss: 1.3840820789337158 \n",
      "Iteration: 1494. Loss: 1.3582125902175903 \n",
      "Iteration: 1495. Loss: 1.3683056831359863 \n",
      "Iteration: 1496. Loss: 1.2903629541397095 \n",
      "Iteration: 1497. Loss: 1.335728645324707 \n",
      "Iteration: 1498. Loss: 1.3839107751846313 \n",
      "Iteration: 1499. Loss: 1.2830890417099 \n",
      "Iteration: 1500. Loss: 1.3389155864715576 \n",
      "Iteration: 1501. Loss: 1.3990620374679565 \n",
      "Iteration: 1502. Loss: 1.320793867111206 \n",
      "Iteration: 1503. Loss: 1.3205283880233765 \n",
      "Iteration: 1504. Loss: 1.3010257482528687 \n",
      "Iteration: 1505. Loss: 1.3765513896942139 \n",
      "Iteration: 1506. Loss: 1.2985998392105103 \n",
      "Iteration: 1507. Loss: 1.3609070777893066 \n",
      "Iteration: 1508. Loss: 1.3312381505966187 \n",
      "Iteration: 1509. Loss: 1.4422478675842285 \n",
      "Iteration: 1510. Loss: 1.3921432495117188 \n",
      "Iteration: 1511. Loss: 1.377717137336731 \n",
      "Iteration: 1512. Loss: 1.3913534879684448 \n",
      "Iteration: 1513. Loss: 1.342496395111084 \n",
      "Iteration: 1514. Loss: 1.32853364944458 \n",
      "Iteration: 1515. Loss: 1.3461198806762695 \n",
      "Iteration: 1516. Loss: 1.3312616348266602 \n",
      "Iteration: 1517. Loss: 1.3532323837280273 \n",
      "Iteration: 1518. Loss: 1.3538342714309692 \n",
      "Iteration: 1519. Loss: 1.4487391710281372 \n",
      "Iteration: 1520. Loss: 1.4345953464508057 \n",
      "Iteration: 1521. Loss: 1.2812800407409668 \n",
      "Iteration: 1522. Loss: 1.3446117639541626 \n",
      "Iteration: 1523. Loss: 1.3477345705032349 \n",
      "Iteration: 1524. Loss: 1.3113188743591309 \n",
      "Iteration: 1525. Loss: 1.3567986488342285 \n",
      "Iteration: 1526. Loss: 1.3626452684402466 \n",
      "Iteration: 1527. Loss: 1.3269497156143188 \n",
      "Iteration: 1528. Loss: 1.353363037109375 \n",
      "Iteration: 1529. Loss: 1.251868486404419 \n",
      "Iteration: 1530. Loss: 1.2950657606124878 \n",
      "Iteration: 1531. Loss: 1.2653148174285889 \n",
      "Iteration: 1532. Loss: 1.2988920211791992 \n",
      "Iteration: 1533. Loss: 1.301139235496521 \n",
      "Iteration: 1534. Loss: 1.4372531175613403 \n",
      "Iteration: 1535. Loss: 1.337580919265747 \n",
      "Iteration: 1536. Loss: 1.3934725522994995 \n",
      "Iteration: 1537. Loss: 1.2204049825668335 \n",
      "Iteration: 1538. Loss: 1.3106486797332764 \n",
      "Iteration: 1539. Loss: 1.3509039878845215 \n",
      "Iteration: 1540. Loss: 1.411494493484497 \n",
      "Iteration: 1541. Loss: 1.36305570602417 \n",
      "Iteration: 1542. Loss: 1.3018568754196167 \n",
      "Iteration: 1543. Loss: 1.3935058116912842 \n",
      "Iteration: 1544. Loss: 1.343867540359497 \n",
      "Iteration: 1545. Loss: 1.28118097782135 \n",
      "Iteration: 1546. Loss: 1.3488550186157227 \n",
      "Iteration: 1547. Loss: 1.3197741508483887 \n",
      "Iteration: 1548. Loss: 1.3250333070755005 \n",
      "Iteration: 1549. Loss: 1.2998754978179932 \n",
      "Iteration: 1550. Loss: 1.3487967252731323 \n",
      "Iteration: 1551. Loss: 1.3133442401885986 \n",
      "Iteration: 1552. Loss: 1.3404440879821777 \n",
      "Iteration: 1553. Loss: 1.2782211303710938 \n",
      "Iteration: 1554. Loss: 1.337211012840271 \n",
      "Iteration: 1555. Loss: 1.2844111919403076 \n",
      "Iteration: 1556. Loss: 1.3884235620498657 \n",
      "Iteration: 1557. Loss: 1.4075759649276733 \n",
      "Iteration: 1558. Loss: 1.3279426097869873 \n",
      "Iteration: 1559. Loss: 1.2839033603668213 \n",
      "Iteration: 1560. Loss: 1.3304665088653564 \n",
      "Iteration: 1561. Loss: 1.3803093433380127 \n",
      "Iteration: 1562. Loss: 1.3469499349594116 \n",
      "Iteration: 1563. Loss: 1.4042850732803345 \n",
      "Iteration: 1564. Loss: 1.3679919242858887 \n",
      "Iteration: 1565. Loss: 1.387374997138977 \n",
      "Iteration: 1566. Loss: 1.332075834274292 \n",
      "Iteration: 1567. Loss: 1.3607831001281738 \n",
      "Iteration: 1568. Loss: 1.2941431999206543 \n",
      "Iteration: 1569. Loss: 1.3688569068908691 \n",
      "Iteration: 1570. Loss: 1.227664589881897 \n",
      "Iteration: 1571. Loss: 1.268700361251831 \n",
      "Iteration: 1572. Loss: 1.2576298713684082 \n",
      "Iteration: 1573. Loss: 1.3032500743865967 \n",
      "Iteration: 1574. Loss: 1.364617943763733 \n",
      "Iteration: 1575. Loss: 1.3931465148925781 \n",
      "Iteration: 1576. Loss: 1.3316901922225952 \n",
      "Iteration: 1577. Loss: 1.3883177042007446 \n",
      "Iteration: 1578. Loss: 1.320048213005066 \n",
      "Iteration: 1579. Loss: 1.3127810955047607 \n",
      "Iteration: 1580. Loss: 1.396030306816101 \n",
      "Iteration: 1581. Loss: 1.2604392766952515 \n",
      "Iteration: 1582. Loss: 1.3166543245315552 \n",
      "Iteration: 1583. Loss: 1.2795681953430176 \n",
      "Iteration: 1584. Loss: 1.2916849851608276 \n",
      "Iteration: 1585. Loss: 1.3300492763519287 \n",
      "Iteration: 1586. Loss: 1.3600735664367676 \n",
      "Iteration: 1587. Loss: 1.2437663078308105 \n",
      "Iteration: 1588. Loss: 1.3266929388046265 \n",
      "Iteration: 1589. Loss: 1.2904083728790283 \n",
      "Iteration: 1590. Loss: 1.357499122619629 \n",
      "Iteration: 1591. Loss: 1.3325289487838745 \n",
      "Iteration: 1592. Loss: 1.2913613319396973 \n",
      "Iteration: 1593. Loss: 1.3071409463882446 \n",
      "Iteration: 1594. Loss: 1.361771821975708 \n",
      "Iteration: 1595. Loss: 1.2284140586853027 \n",
      "Iteration: 1596. Loss: 1.3758355379104614 \n",
      "Iteration: 1597. Loss: 1.2953715324401855 \n",
      "Iteration: 1598. Loss: 1.427235722541809 \n",
      "Iteration: 1599. Loss: 1.3221800327301025 \n",
      "Iteration: 1600. Loss: 1.3439128398895264 \n",
      "Iteration: 1601. Loss: 1.341563105583191 \n",
      "Iteration: 1602. Loss: 1.2935991287231445 \n",
      "Iteration: 1603. Loss: 1.3256311416625977 \n",
      "Iteration: 1604. Loss: 1.3534541130065918 \n",
      "Iteration: 1605. Loss: 1.333126187324524 \n",
      "Iteration: 1606. Loss: 1.3603062629699707 \n",
      "Iteration: 1607. Loss: 1.3080263137817383 \n",
      "Iteration: 1608. Loss: 1.3989144563674927 \n",
      "Iteration: 1609. Loss: 1.3112339973449707 \n",
      "Iteration: 1610. Loss: 1.322861671447754 \n",
      "Iteration: 1611. Loss: 1.3058613538742065 \n",
      "Iteration: 1612. Loss: 1.2546701431274414 \n",
      "Iteration: 1613. Loss: 1.3049392700195312 \n",
      "Iteration: 1614. Loss: 1.2435357570648193 \n",
      "Iteration: 1615. Loss: 1.3808380365371704 \n",
      "Iteration: 1616. Loss: 1.3943710327148438 \n",
      "Iteration: 1617. Loss: 1.2451919317245483 \n",
      "Iteration: 1618. Loss: 1.282080888748169 \n",
      "Iteration: 1619. Loss: 1.2417445182800293 \n",
      "Iteration: 1620. Loss: 1.2966793775558472 \n",
      "Iteration: 1621. Loss: 1.1648751497268677 \n",
      "Iteration: 1622. Loss: 1.3131636381149292 \n",
      "Iteration: 1623. Loss: 1.338241457939148 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1624. Loss: 1.3227289915084839 \n",
      "Iteration: 1625. Loss: 1.2093472480773926 \n",
      "Iteration: 1626. Loss: 1.292819619178772 \n",
      "Iteration: 1627. Loss: 1.2661038637161255 \n",
      "Iteration: 1628. Loss: 1.408860445022583 \n",
      "Iteration: 1629. Loss: 1.2806288003921509 \n",
      "Iteration: 1630. Loss: 1.3748208284378052 \n",
      "Iteration: 1631. Loss: 1.3516029119491577 \n",
      "Iteration: 1632. Loss: 1.3294404745101929 \n",
      "Iteration: 1633. Loss: 1.346858263015747 \n",
      "Iteration: 1634. Loss: 1.2520806789398193 \n",
      "Iteration: 1635. Loss: 1.3145893812179565 \n",
      "Iteration: 1636. Loss: 1.2811964750289917 \n",
      "Iteration: 1637. Loss: 1.3048491477966309 \n",
      "Iteration: 1638. Loss: 1.2890537977218628 \n",
      "Iteration: 1639. Loss: 1.248274803161621 \n",
      "Iteration: 1640. Loss: 1.3043348789215088 \n",
      "Iteration: 1641. Loss: 1.2455004453659058 \n",
      "Iteration: 1642. Loss: 1.3773640394210815 \n",
      "Iteration: 1643. Loss: 1.251082181930542 \n",
      "Iteration: 1644. Loss: 1.3378978967666626 \n",
      "Iteration: 1645. Loss: 1.3870962858200073 \n",
      "Iteration: 1646. Loss: 1.371951937675476 \n",
      "Iteration: 1647. Loss: 1.3083854913711548 \n",
      "Iteration: 1648. Loss: 1.2939770221710205 \n",
      "Iteration: 1649. Loss: 1.3147878646850586 \n",
      "Iteration: 1650. Loss: 1.2477200031280518 \n",
      "Iteration: 1651. Loss: 1.3278347253799438 \n",
      "Iteration: 1652. Loss: 1.293662428855896 \n",
      "Iteration: 1653. Loss: 1.307356357574463 \n",
      "Iteration: 1654. Loss: 1.2906560897827148 \n",
      "Iteration: 1655. Loss: 1.2829675674438477 \n",
      "Iteration: 1656. Loss: 1.3013862371444702 \n",
      "Iteration: 1657. Loss: 1.290656328201294 \n",
      "Iteration: 1658. Loss: 1.3014857769012451 \n",
      "Iteration: 1659. Loss: 1.3448271751403809 \n",
      "Iteration: 1660. Loss: 1.3324259519577026 \n",
      "Iteration: 1661. Loss: 1.3067429065704346 \n",
      "Iteration: 1662. Loss: 1.3263635635375977 \n",
      "Iteration: 1663. Loss: 1.397201657295227 \n",
      "Iteration: 1664. Loss: 1.301541805267334 \n",
      "Iteration: 1665. Loss: 1.2422194480895996 \n",
      "Iteration: 1666. Loss: 1.29311203956604 \n",
      "Iteration: 1667. Loss: 1.2918885946273804 \n",
      "Iteration: 1668. Loss: 1.282997488975525 \n",
      "Iteration: 1669. Loss: 1.3055083751678467 \n",
      "Iteration: 1670. Loss: 1.2383713722229004 \n",
      "Iteration: 1671. Loss: 1.3310835361480713 \n",
      "Iteration: 1672. Loss: 1.2986302375793457 \n",
      "Iteration: 1673. Loss: 1.2484989166259766 \n",
      "Iteration: 1674. Loss: 1.3686596155166626 \n",
      "Iteration: 1675. Loss: 1.3703662157058716 \n",
      "Iteration: 1676. Loss: 1.4176026582717896 \n",
      "Iteration: 1677. Loss: 1.3493080139160156 \n",
      "Iteration: 1678. Loss: 1.2387217283248901 \n",
      "Iteration: 1679. Loss: 1.299304485321045 \n",
      "Iteration: 1680. Loss: 1.2499761581420898 \n",
      "Iteration: 1681. Loss: 1.261191725730896 \n",
      "Iteration: 1682. Loss: 1.2956554889678955 \n",
      "Iteration: 1683. Loss: 1.2765445709228516 \n",
      "Iteration: 1684. Loss: 1.2022926807403564 \n",
      "Iteration: 1685. Loss: 1.281645655632019 \n",
      "Iteration: 1686. Loss: 1.3260897397994995 \n",
      "Iteration: 1687. Loss: 1.329857349395752 \n",
      "Iteration: 1688. Loss: 1.319504737854004 \n",
      "Iteration: 1689. Loss: 1.2693488597869873 \n",
      "Iteration: 1690. Loss: 1.293366551399231 \n",
      "Iteration: 1691. Loss: 1.289723515510559 \n",
      "Iteration: 1692. Loss: 1.2943153381347656 \n",
      "Iteration: 1693. Loss: 1.286732792854309 \n",
      "Iteration: 1694. Loss: 1.356387734413147 \n",
      "Iteration: 1695. Loss: 1.2460834980010986 \n",
      "Iteration: 1696. Loss: 1.2858619689941406 \n",
      "Iteration: 1697. Loss: 1.2792528867721558 \n",
      "Iteration: 1698. Loss: 1.2554490566253662 \n",
      "Iteration: 1699. Loss: 1.3453463315963745 \n",
      "Iteration: 1700. Loss: 1.251490831375122 \n",
      "Iteration: 1701. Loss: 1.3204562664031982 \n",
      "Iteration: 1702. Loss: 1.2693567276000977 \n",
      "Iteration: 1703. Loss: 1.2881150245666504 \n",
      "Iteration: 1704. Loss: 1.26533842086792 \n",
      "Iteration: 1705. Loss: 1.221763253211975 \n",
      "Iteration: 1706. Loss: 1.3292298316955566 \n",
      "Iteration: 1707. Loss: 1.2612313032150269 \n",
      "Iteration: 1708. Loss: 1.3191026449203491 \n",
      "Iteration: 1709. Loss: 1.269118309020996 \n",
      "Iteration: 1710. Loss: 1.310947299003601 \n",
      "Iteration: 1711. Loss: 1.1360427141189575 \n",
      "Iteration: 1712. Loss: 1.3519439697265625 \n",
      "Iteration: 1713. Loss: 1.2858633995056152 \n",
      "Iteration: 1714. Loss: 1.3457517623901367 \n",
      "Iteration: 1715. Loss: 1.2881654500961304 \n",
      "Iteration: 1716. Loss: 1.2720210552215576 \n",
      "Iteration: 1717. Loss: 1.3069988489151 \n",
      "Iteration: 1718. Loss: 1.287527322769165 \n",
      "Iteration: 1719. Loss: 1.3839619159698486 \n",
      "Iteration: 1720. Loss: 1.2854853868484497 \n",
      "Iteration: 1721. Loss: 1.22747802734375 \n",
      "Iteration: 1722. Loss: 1.287078857421875 \n",
      "Iteration: 1723. Loss: 1.2863490581512451 \n",
      "Iteration: 1724. Loss: 1.2163134813308716 \n",
      "Iteration: 1725. Loss: 1.2111550569534302 \n",
      "Iteration: 1726. Loss: 1.3755302429199219 \n",
      "Iteration: 1727. Loss: 1.3022783994674683 \n",
      "Iteration: 1728. Loss: 1.2490862607955933 \n",
      "Iteration: 1729. Loss: 1.2385097742080688 \n",
      "Iteration: 1730. Loss: 1.3191276788711548 \n",
      "Iteration: 1731. Loss: 1.2269684076309204 \n",
      "Iteration: 1732. Loss: 1.189138412475586 \n",
      "Iteration: 1733. Loss: 1.2817354202270508 \n",
      "Iteration: 1734. Loss: 1.3639483451843262 \n",
      "Iteration: 1735. Loss: 1.243675708770752 \n",
      "Iteration: 1736. Loss: 1.1780974864959717 \n",
      "Iteration: 1737. Loss: 1.2047626972198486 \n",
      "Iteration: 1738. Loss: 1.2349059581756592 \n",
      "Iteration: 1739. Loss: 1.292934775352478 \n",
      "Iteration: 1740. Loss: 1.3355810642242432 \n",
      "Iteration: 1741. Loss: 1.314314365386963 \n",
      "Iteration: 1742. Loss: 1.2165967226028442 \n",
      "Iteration: 1743. Loss: 1.3721562623977661 \n",
      "Iteration: 1744. Loss: 1.3007959127426147 \n",
      "Iteration: 1745. Loss: 1.3134465217590332 \n",
      "Iteration: 1746. Loss: 1.2931149005889893 \n",
      "Iteration: 1747. Loss: 1.2319974899291992 \n",
      "Iteration: 1748. Loss: 1.2661086320877075 \n",
      "Iteration: 1749. Loss: 1.2178874015808105 \n",
      "Iteration: 1750. Loss: 1.3227615356445312 \n",
      "Iteration: 1751. Loss: 1.1731523275375366 \n",
      "Iteration: 1752. Loss: 1.2854537963867188 \n",
      "Iteration: 1753. Loss: 1.282002568244934 \n",
      "Iteration: 1754. Loss: 1.1417862176895142 \n",
      "Iteration: 1755. Loss: 1.1486455202102661 \n",
      "Iteration: 1756. Loss: 1.2499053478240967 \n",
      "Iteration: 1757. Loss: 1.2035940885543823 \n",
      "Iteration: 1758. Loss: 1.3015583753585815 \n",
      "Iteration: 1759. Loss: 1.3084911108016968 \n",
      "Iteration: 1760. Loss: 1.2836483716964722 \n",
      "Iteration: 1761. Loss: 1.2339380979537964 \n",
      "Iteration: 1762. Loss: 1.2232656478881836 \n",
      "Iteration: 1763. Loss: 1.2068918943405151 \n",
      "Iteration: 1764. Loss: 1.2872495651245117 \n",
      "Iteration: 1765. Loss: 1.2413618564605713 \n",
      "Iteration: 1766. Loss: 1.260361909866333 \n",
      "Iteration: 1767. Loss: 1.3451040983200073 \n",
      "Iteration: 1768. Loss: 1.2127137184143066 \n",
      "Iteration: 1769. Loss: 1.3020089864730835 \n",
      "Iteration: 1770. Loss: 1.2093191146850586 \n",
      "Iteration: 1771. Loss: 1.273094892501831 \n",
      "Iteration: 1772. Loss: 1.2357803583145142 \n",
      "Iteration: 1773. Loss: 1.255858302116394 \n",
      "Iteration: 1774. Loss: 1.2024949789047241 \n",
      "Iteration: 1775. Loss: 1.2941293716430664 \n",
      "Iteration: 1776. Loss: 1.1896233558654785 \n",
      "Iteration: 1777. Loss: 1.259650707244873 \n",
      "Iteration: 1778. Loss: 1.325241208076477 \n",
      "Iteration: 1779. Loss: 1.2369563579559326 \n",
      "Iteration: 1780. Loss: 1.3062188625335693 \n",
      "Iteration: 1781. Loss: 1.229549765586853 \n",
      "Iteration: 1782. Loss: 1.2666839361190796 \n",
      "Iteration: 1783. Loss: 1.227187156677246 \n",
      "Iteration: 1784. Loss: 1.2991999387741089 \n",
      "Iteration: 1785. Loss: 1.2385624647140503 \n",
      "Iteration: 1786. Loss: 1.2508620023727417 \n",
      "Iteration: 1787. Loss: 1.2234795093536377 \n",
      "Iteration: 1788. Loss: 1.2187587022781372 \n",
      "Iteration: 1789. Loss: 1.3297981023788452 \n",
      "Iteration: 1790. Loss: 1.1917598247528076 \n",
      "Iteration: 1791. Loss: 1.2654117345809937 \n",
      "Iteration: 1792. Loss: 1.2483811378479004 \n",
      "Iteration: 1793. Loss: 1.2201380729675293 \n",
      "Iteration: 1794. Loss: 1.2060339450836182 \n",
      "Iteration: 1795. Loss: 1.3078982830047607 \n",
      "Iteration: 1796. Loss: 1.3345081806182861 \n",
      "Iteration: 1797. Loss: 1.272567868232727 \n",
      "Iteration: 1798. Loss: 1.2680916786193848 \n",
      "Iteration: 1799. Loss: 1.2334463596343994 \n",
      "Iteration: 1800. Loss: 1.2522482872009277 \n",
      "Iteration: 1801. Loss: 1.2955007553100586 \n",
      "Iteration: 1802. Loss: 1.20613431930542 \n",
      "Iteration: 1803. Loss: 1.3175420761108398 \n",
      "Iteration: 1804. Loss: 1.3092461824417114 \n",
      "Iteration: 1805. Loss: 1.2744919061660767 \n",
      "Iteration: 1806. Loss: 1.2313282489776611 \n",
      "Iteration: 1807. Loss: 1.1828974485397339 \n",
      "Iteration: 1808. Loss: 1.2730895280838013 \n",
      "Iteration: 1809. Loss: 1.2406737804412842 \n",
      "Iteration: 1810. Loss: 1.236707329750061 \n",
      "Iteration: 1811. Loss: 1.1941996812820435 \n",
      "Iteration: 1812. Loss: 1.2773511409759521 \n",
      "Iteration: 1813. Loss: 1.3540652990341187 \n",
      "Iteration: 1814. Loss: 1.2658162117004395 \n",
      "Iteration: 1815. Loss: 1.2760387659072876 \n",
      "Iteration: 1816. Loss: 1.2473423480987549 \n",
      "Iteration: 1817. Loss: 1.282768726348877 \n",
      "Iteration: 1818. Loss: 1.262213945388794 \n",
      "Iteration: 1819. Loss: 1.3022974729537964 \n",
      "Iteration: 1820. Loss: 1.3204143047332764 \n",
      "Iteration: 1821. Loss: 1.2815117835998535 \n",
      "Iteration: 1822. Loss: 1.2302746772766113 \n",
      "Iteration: 1823. Loss: 1.2385388612747192 \n",
      "Iteration: 1824. Loss: 1.3157745599746704 \n",
      "Iteration: 1825. Loss: 1.2272573709487915 \n",
      "Iteration: 1826. Loss: 1.2157130241394043 \n",
      "Iteration: 1827. Loss: 1.1980311870574951 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1828. Loss: 1.2524306774139404 \n",
      "Iteration: 1829. Loss: 1.2422159910202026 \n",
      "Iteration: 1830. Loss: 1.2693103551864624 \n",
      "Iteration: 1831. Loss: 1.3128451108932495 \n",
      "Iteration: 1832. Loss: 1.2536742687225342 \n",
      "Iteration: 1833. Loss: 1.2579855918884277 \n",
      "Iteration: 1834. Loss: 1.2929803133010864 \n",
      "Iteration: 1835. Loss: 1.2123297452926636 \n",
      "Iteration: 1836. Loss: 1.3101061582565308 \n",
      "Iteration: 1837. Loss: 1.3310376405715942 \n",
      "Iteration: 1838. Loss: 1.264449119567871 \n",
      "Iteration: 1839. Loss: 1.2198569774627686 \n",
      "Iteration: 1840. Loss: 1.2668826580047607 \n",
      "Iteration: 1841. Loss: 1.2672417163848877 \n",
      "Iteration: 1842. Loss: 1.215388536453247 \n",
      "Iteration: 1843. Loss: 1.3116111755371094 \n",
      "Iteration: 1844. Loss: 1.1839631795883179 \n",
      "Iteration: 1845. Loss: 1.2808945178985596 \n",
      "Iteration: 1846. Loss: 1.3501049280166626 \n",
      "Iteration: 1847. Loss: 1.2057099342346191 \n",
      "Iteration: 1848. Loss: 1.2054160833358765 \n",
      "Iteration: 1849. Loss: 1.1412183046340942 \n",
      "Iteration: 1850. Loss: 1.2404794692993164 \n",
      "Iteration: 1851. Loss: 1.2894903421401978 \n",
      "Iteration: 1852. Loss: 1.1320949792861938 \n",
      "Iteration: 1853. Loss: 1.2413305044174194 \n",
      "Iteration: 1854. Loss: 1.2330543994903564 \n",
      "Iteration: 1855. Loss: 1.281957745552063 \n",
      "Iteration: 1856. Loss: 1.2014703750610352 \n",
      "Iteration: 1857. Loss: 1.2350488901138306 \n",
      "Iteration: 1858. Loss: 1.2692139148712158 \n",
      "Iteration: 1859. Loss: 1.2372506856918335 \n",
      "Iteration: 1860. Loss: 1.268689513206482 \n",
      "Iteration: 1861. Loss: 1.2524843215942383 \n",
      "Iteration: 1862. Loss: 1.2575688362121582 \n",
      "Iteration: 1863. Loss: 1.3269940614700317 \n",
      "Iteration: 1864. Loss: 1.3199968338012695 \n",
      "Iteration: 1865. Loss: 1.2112747430801392 \n",
      "Iteration: 1866. Loss: 1.3083364963531494 \n",
      "Iteration: 1867. Loss: 1.2165106534957886 \n",
      "Iteration: 1868. Loss: 1.1240731477737427 \n",
      "Iteration: 1869. Loss: 1.2503056526184082 \n",
      "Iteration: 1870. Loss: 1.18181574344635 \n",
      "Iteration: 1871. Loss: 1.2598098516464233 \n",
      "Iteration: 1872. Loss: 1.1669360399246216 \n",
      "Iteration: 1873. Loss: 1.213729739189148 \n",
      "Iteration: 1874. Loss: 1.180048942565918 \n",
      "Iteration: 1875. Loss: 1.222259283065796 \n",
      "Iteration: 1876. Loss: 1.33683443069458 \n",
      "Iteration: 1877. Loss: 1.2458559274673462 \n",
      "Iteration: 1878. Loss: 1.1904953718185425 \n",
      "Iteration: 1879. Loss: 1.3338357210159302 \n",
      "Iteration: 1880. Loss: 1.2383956909179688 \n",
      "Iteration: 1881. Loss: 1.3112597465515137 \n",
      "Iteration: 1882. Loss: 1.248300313949585 \n",
      "Iteration: 1883. Loss: 1.2832814455032349 \n",
      "Iteration: 1884. Loss: 1.1705349683761597 \n",
      "Iteration: 1885. Loss: 1.2180269956588745 \n",
      "Iteration: 1886. Loss: 1.3226349353790283 \n",
      "Iteration: 1887. Loss: 1.3299429416656494 \n",
      "Iteration: 1888. Loss: 1.2218683958053589 \n",
      "Iteration: 1889. Loss: 1.2811036109924316 \n",
      "Iteration: 1890. Loss: 1.2126599550247192 \n",
      "Iteration: 1891. Loss: 1.2443184852600098 \n",
      "Iteration: 1892. Loss: 1.1757867336273193 \n",
      "Iteration: 1893. Loss: 1.2534328699111938 \n",
      "Iteration: 1894. Loss: 1.1394240856170654 \n",
      "Iteration: 1895. Loss: 1.271355390548706 \n",
      "Iteration: 1896. Loss: 1.2478327751159668 \n",
      "Iteration: 1897. Loss: 1.2200895547866821 \n",
      "Iteration: 1898. Loss: 1.2686234712600708 \n",
      "Iteration: 1899. Loss: 1.2157237529754639 \n",
      "Iteration: 1900. Loss: 1.3057833909988403 \n",
      "Iteration: 1901. Loss: 1.2753918170928955 \n",
      "Iteration: 1902. Loss: 1.1997755765914917 \n",
      "Iteration: 1903. Loss: 1.2404342889785767 \n",
      "Iteration: 1904. Loss: 1.31390380859375 \n",
      "Iteration: 1905. Loss: 1.2158278226852417 \n",
      "Iteration: 1906. Loss: 1.1751081943511963 \n",
      "Iteration: 1907. Loss: 1.2498250007629395 \n",
      "Iteration: 1908. Loss: 1.2332326173782349 \n",
      "Iteration: 1909. Loss: 1.3440184593200684 \n",
      "Iteration: 1910. Loss: 1.2414636611938477 \n",
      "Iteration: 1911. Loss: 1.2605072259902954 \n",
      "Iteration: 1912. Loss: 1.1787610054016113 \n",
      "Iteration: 1913. Loss: 1.2669193744659424 \n",
      "Iteration: 1914. Loss: 1.2138482332229614 \n",
      "Iteration: 1915. Loss: 1.2936413288116455 \n",
      "Iteration: 1916. Loss: 1.2635607719421387 \n",
      "Iteration: 1917. Loss: 1.2029861211776733 \n",
      "Iteration: 1918. Loss: 1.2363922595977783 \n",
      "Iteration: 1919. Loss: 1.1064350605010986 \n",
      "Iteration: 1920. Loss: 1.2175021171569824 \n",
      "Iteration: 1921. Loss: 1.2758762836456299 \n",
      "Iteration: 1922. Loss: 1.2108319997787476 \n",
      "Iteration: 1923. Loss: 1.214976191520691 \n",
      "Iteration: 1924. Loss: 1.190861701965332 \n",
      "Iteration: 1925. Loss: 1.2272578477859497 \n",
      "Iteration: 1926. Loss: 1.1117453575134277 \n",
      "Iteration: 1927. Loss: 1.2815512418746948 \n",
      "Iteration: 1928. Loss: 1.278062343597412 \n",
      "Iteration: 1929. Loss: 1.15028715133667 \n",
      "Iteration: 1930. Loss: 1.2027438879013062 \n",
      "Iteration: 1931. Loss: 1.2431302070617676 \n",
      "Iteration: 1932. Loss: 1.1744791269302368 \n",
      "Iteration: 1933. Loss: 1.1923048496246338 \n",
      "Iteration: 1934. Loss: 1.1812314987182617 \n",
      "Iteration: 1935. Loss: 1.242682933807373 \n",
      "Iteration: 1936. Loss: 1.2679059505462646 \n",
      "Iteration: 1937. Loss: 1.2576996088027954 \n",
      "Iteration: 1938. Loss: 1.2038965225219727 \n",
      "Iteration: 1939. Loss: 1.3188188076019287 \n",
      "Iteration: 1940. Loss: 1.2938728332519531 \n",
      "Iteration: 1941. Loss: 1.1581412553787231 \n",
      "Iteration: 1942. Loss: 1.2871290445327759 \n",
      "Iteration: 1943. Loss: 1.1775628328323364 \n",
      "Iteration: 1944. Loss: 1.2075424194335938 \n",
      "Iteration: 1945. Loss: 1.1619724035263062 \n",
      "Iteration: 1946. Loss: 1.2822624444961548 \n",
      "Iteration: 1947. Loss: 1.233852505683899 \n",
      "Iteration: 1948. Loss: 1.2132079601287842 \n",
      "Iteration: 1949. Loss: 1.2488937377929688 \n",
      "Iteration: 1950. Loss: 1.2373261451721191 \n",
      "Iteration: 1951. Loss: 1.1713224649429321 \n",
      "Iteration: 1952. Loss: 1.2320493459701538 \n",
      "Iteration: 1953. Loss: 1.205306887626648 \n",
      "Iteration: 1954. Loss: 1.1407806873321533 \n",
      "Iteration: 1955. Loss: 1.1861191987991333 \n",
      "Iteration: 1956. Loss: 1.1744002103805542 \n",
      "Iteration: 1957. Loss: 1.2837698459625244 \n",
      "Iteration: 1958. Loss: 1.2386640310287476 \n",
      "Iteration: 1959. Loss: 1.1612465381622314 \n",
      "Iteration: 1960. Loss: 1.2322132587432861 \n",
      "Iteration: 1961. Loss: 1.2813409566879272 \n",
      "Iteration: 1962. Loss: 1.1748367547988892 \n",
      "Iteration: 1963. Loss: 1.2312618494033813 \n",
      "Iteration: 1964. Loss: 1.186218023300171 \n",
      "Iteration: 1965. Loss: 1.2669414281845093 \n",
      "Iteration: 1966. Loss: 1.2462235689163208 \n",
      "Iteration: 1967. Loss: 1.2495601177215576 \n",
      "Iteration: 1968. Loss: 1.1717181205749512 \n",
      "Iteration: 1969. Loss: 1.215915560722351 \n",
      "Iteration: 1970. Loss: 1.282070517539978 \n",
      "Iteration: 1971. Loss: 1.2105215787887573 \n",
      "Iteration: 1972. Loss: 1.2872259616851807 \n",
      "Iteration: 1973. Loss: 1.0921093225479126 \n",
      "Iteration: 1974. Loss: 1.1951398849487305 \n",
      "Iteration: 1975. Loss: 1.1889077425003052 \n",
      "Iteration: 1976. Loss: 1.2027400732040405 \n",
      "Iteration: 1977. Loss: 1.2328858375549316 \n",
      "Iteration: 1978. Loss: 1.1799101829528809 \n",
      "Iteration: 1979. Loss: 1.1897907257080078 \n",
      "Iteration: 1980. Loss: 1.1611783504486084 \n",
      "Iteration: 1981. Loss: 1.2205051183700562 \n",
      "Iteration: 1982. Loss: 1.182077169418335 \n",
      "Iteration: 1983. Loss: 1.2719388008117676 \n",
      "Iteration: 1984. Loss: 1.1189582347869873 \n",
      "Iteration: 1985. Loss: 1.1553678512573242 \n",
      "Iteration: 1986. Loss: 1.3216772079467773 \n",
      "Iteration: 1987. Loss: 1.156425952911377 \n",
      "Iteration: 1988. Loss: 1.2510130405426025 \n",
      "Iteration: 1989. Loss: 1.2165310382843018 \n",
      "Iteration: 1990. Loss: 1.1517795324325562 \n",
      "Iteration: 1991. Loss: 1.2442682981491089 \n",
      "Iteration: 1992. Loss: 1.260026216506958 \n",
      "Iteration: 1993. Loss: 1.1917294263839722 \n",
      "Iteration: 1994. Loss: 1.2565492391586304 \n",
      "Iteration: 1995. Loss: 1.2082293033599854 \n",
      "Iteration: 1996. Loss: 1.2314378023147583 \n",
      "Iteration: 1997. Loss: 1.2084133625030518 \n",
      "Iteration: 1998. Loss: 1.2339199781417847 \n",
      "Iteration: 1999. Loss: 1.1529223918914795 \n",
      "Iteration: 2000. Loss: 1.2062571048736572 \n",
      "Iteration: 2001. Loss: 1.218685507774353 \n",
      "Iteration: 2002. Loss: 1.179947018623352 \n",
      "Iteration: 2003. Loss: 1.1805386543273926 \n",
      "Iteration: 2004. Loss: 1.1647683382034302 \n",
      "Iteration: 2005. Loss: 1.1972270011901855 \n",
      "Iteration: 2006. Loss: 1.233391523361206 \n",
      "Iteration: 2007. Loss: 1.117597222328186 \n",
      "Iteration: 2008. Loss: 1.208940029144287 \n",
      "Iteration: 2009. Loss: 1.204099178314209 \n",
      "Iteration: 2010. Loss: 1.241328239440918 \n",
      "Iteration: 2011. Loss: 1.2975120544433594 \n",
      "Iteration: 2012. Loss: 1.2028933763504028 \n",
      "Iteration: 2013. Loss: 1.137665867805481 \n",
      "Iteration: 2014. Loss: 1.1700013875961304 \n",
      "Iteration: 2015. Loss: 1.211479663848877 \n",
      "Iteration: 2016. Loss: 1.1459661722183228 \n",
      "Iteration: 2017. Loss: 1.1831209659576416 \n",
      "Iteration: 2018. Loss: 1.1135759353637695 \n",
      "Iteration: 2019. Loss: 1.22748601436615 \n",
      "Iteration: 2020. Loss: 1.2560069561004639 \n",
      "Iteration: 2021. Loss: 1.1520302295684814 \n",
      "Iteration: 2022. Loss: 1.1692492961883545 \n",
      "Iteration: 2023. Loss: 1.139626383781433 \n",
      "Iteration: 2024. Loss: 1.2481012344360352 \n",
      "Iteration: 2025. Loss: 1.180739164352417 \n",
      "Iteration: 2026. Loss: 1.1825518608093262 \n",
      "Iteration: 2027. Loss: 1.2243635654449463 \n",
      "Iteration: 2028. Loss: 1.161021113395691 \n",
      "Iteration: 2029. Loss: 1.1766834259033203 \n",
      "Iteration: 2030. Loss: 1.1894720792770386 \n",
      "Iteration: 2031. Loss: 1.236493706703186 \n",
      "Iteration: 2032. Loss: 1.2549412250518799 \n",
      "Iteration: 2033. Loss: 1.181207299232483 \n",
      "Iteration: 2034. Loss: 1.1817576885223389 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2035. Loss: 1.198026418685913 \n",
      "Iteration: 2036. Loss: 1.1979652643203735 \n",
      "Iteration: 2037. Loss: 1.2286102771759033 \n",
      "Iteration: 2038. Loss: 1.1799448728561401 \n",
      "Iteration: 2039. Loss: 1.1761200428009033 \n",
      "Iteration: 2040. Loss: 1.0999112129211426 \n",
      "Iteration: 2041. Loss: 1.211683988571167 \n",
      "Iteration: 2042. Loss: 1.2235589027404785 \n",
      "Iteration: 2043. Loss: 1.2330214977264404 \n",
      "Iteration: 2044. Loss: 1.3107436895370483 \n",
      "Iteration: 2045. Loss: 1.2324563264846802 \n",
      "Iteration: 2046. Loss: 1.198380708694458 \n",
      "Iteration: 2047. Loss: 1.139765739440918 \n",
      "Iteration: 2048. Loss: 1.2181628942489624 \n",
      "Iteration: 2049. Loss: 1.2020821571350098 \n",
      "Iteration: 2050. Loss: 1.2091774940490723 \n",
      "Iteration: 2051. Loss: 1.1157667636871338 \n",
      "Iteration: 2052. Loss: 1.142899990081787 \n",
      "Iteration: 2053. Loss: 1.2406458854675293 \n",
      "Iteration: 2054. Loss: 1.1504733562469482 \n",
      "Iteration: 2055. Loss: 1.2019256353378296 \n",
      "Iteration: 2056. Loss: 1.2568306922912598 \n",
      "Iteration: 2057. Loss: 1.2106636762619019 \n",
      "Iteration: 2058. Loss: 1.0565205812454224 \n",
      "Iteration: 2059. Loss: 1.1128674745559692 \n",
      "Iteration: 2060. Loss: 1.1688963174819946 \n",
      "Iteration: 2061. Loss: 1.1668106317520142 \n",
      "Iteration: 2062. Loss: 1.252213716506958 \n",
      "Iteration: 2063. Loss: 1.17825186252594 \n",
      "Iteration: 2064. Loss: 1.3049365282058716 \n",
      "Iteration: 2065. Loss: 1.1248440742492676 \n",
      "Iteration: 2066. Loss: 1.178098201751709 \n",
      "Iteration: 2067. Loss: 1.2114167213439941 \n",
      "Iteration: 2068. Loss: 1.0637362003326416 \n",
      "Iteration: 2069. Loss: 1.218492031097412 \n",
      "Iteration: 2070. Loss: 1.1638892889022827 \n",
      "Iteration: 2071. Loss: 1.193921685218811 \n",
      "Iteration: 2072. Loss: 1.1781699657440186 \n",
      "Iteration: 2073. Loss: 1.158549189567566 \n",
      "Iteration: 2074. Loss: 1.162979245185852 \n",
      "Iteration: 2075. Loss: 1.192110538482666 \n",
      "Iteration: 2076. Loss: 1.182690143585205 \n",
      "Iteration: 2077. Loss: 1.1136728525161743 \n",
      "Iteration: 2078. Loss: 1.0366699695587158 \n",
      "Iteration: 2079. Loss: 1.2031821012496948 \n",
      "Iteration: 2080. Loss: 1.2391612529754639 \n",
      "Iteration: 2081. Loss: 1.15695059299469 \n",
      "Iteration: 2082. Loss: 1.1099053621292114 \n",
      "Iteration: 2083. Loss: 1.1199402809143066 \n",
      "Iteration: 2084. Loss: 1.1847573518753052 \n",
      "Iteration: 2085. Loss: 1.1733344793319702 \n",
      "Iteration: 2086. Loss: 1.120416283607483 \n",
      "Iteration: 2087. Loss: 1.0617069005966187 \n",
      "Iteration: 2088. Loss: 1.1227682828903198 \n",
      "Iteration: 2089. Loss: 1.164963722229004 \n",
      "Iteration: 2090. Loss: 1.068270206451416 \n",
      "Iteration: 2091. Loss: 1.1892876625061035 \n",
      "Iteration: 2092. Loss: 1.200857400894165 \n",
      "Iteration: 2093. Loss: 1.2396739721298218 \n",
      "Iteration: 2094. Loss: 1.187244176864624 \n",
      "Iteration: 2095. Loss: 1.1171573400497437 \n",
      "Iteration: 2096. Loss: 1.1994775533676147 \n",
      "Iteration: 2097. Loss: 1.2321715354919434 \n",
      "Iteration: 2098. Loss: 1.187100887298584 \n",
      "Iteration: 2099. Loss: 1.1263517141342163 \n",
      "Iteration: 2100. Loss: 1.1938046216964722 \n",
      "Iteration: 2101. Loss: 1.1010656356811523 \n",
      "Iteration: 2102. Loss: 1.1175117492675781 \n",
      "Iteration: 2103. Loss: 1.158215880393982 \n",
      "Iteration: 2104. Loss: 1.2288588285446167 \n",
      "Iteration: 2105. Loss: 1.2581204175949097 \n",
      "Iteration: 2106. Loss: 1.1879112720489502 \n",
      "Iteration: 2107. Loss: 1.2084201574325562 \n",
      "Iteration: 2108. Loss: 1.1400948762893677 \n",
      "Iteration: 2109. Loss: 1.1284221410751343 \n",
      "Iteration: 2110. Loss: 1.2384521961212158 \n",
      "Iteration: 2111. Loss: 1.2272228002548218 \n",
      "Iteration: 2112. Loss: 1.2031347751617432 \n",
      "Iteration: 2113. Loss: 1.2196481227874756 \n",
      "Iteration: 2114. Loss: 1.2799594402313232 \n",
      "Iteration: 2115. Loss: 1.0422438383102417 \n",
      "Iteration: 2116. Loss: 1.2152894735336304 \n",
      "Iteration: 2117. Loss: 1.291168212890625 \n",
      "Iteration: 2118. Loss: 1.1363259553909302 \n",
      "Iteration: 2119. Loss: 1.1915591955184937 \n",
      "Iteration: 2120. Loss: 1.1702957153320312 \n",
      "Iteration: 2121. Loss: 1.0584511756896973 \n",
      "Iteration: 2122. Loss: 1.0764758586883545 \n",
      "Iteration: 2123. Loss: 1.2845089435577393 \n",
      "Iteration: 2124. Loss: 1.1244442462921143 \n",
      "Iteration: 2125. Loss: 1.1787811517715454 \n",
      "Iteration: 2126. Loss: 1.167626142501831 \n",
      "Iteration: 2127. Loss: 1.0664831399917603 \n",
      "Iteration: 2128. Loss: 1.1895194053649902 \n",
      "Iteration: 2129. Loss: 1.2323437929153442 \n",
      "Iteration: 2130. Loss: 1.1515558958053589 \n",
      "Iteration: 2131. Loss: 1.2014580965042114 \n",
      "Iteration: 2132. Loss: 1.0970964431762695 \n",
      "Iteration: 2133. Loss: 1.2236182689666748 \n",
      "Iteration: 2134. Loss: 1.2107512950897217 \n",
      "Iteration: 2135. Loss: 1.1729408502578735 \n",
      "Iteration: 2136. Loss: 1.228318214416504 \n",
      "Iteration: 2137. Loss: 1.1203066110610962 \n",
      "Iteration: 2138. Loss: 1.1580395698547363 \n",
      "Iteration: 2139. Loss: 1.1525377035140991 \n",
      "Iteration: 2140. Loss: 1.1631247997283936 \n",
      "Iteration: 2141. Loss: 1.254836916923523 \n",
      "Iteration: 2142. Loss: 1.1840338706970215 \n",
      "Iteration: 2143. Loss: 1.0705260038375854 \n",
      "Iteration: 2144. Loss: 1.0804753303527832 \n",
      "Iteration: 2145. Loss: 1.210168480873108 \n",
      "Iteration: 2146. Loss: 1.159569501876831 \n",
      "Iteration: 2147. Loss: 1.1365058422088623 \n",
      "Iteration: 2148. Loss: 1.1024171113967896 \n",
      "Iteration: 2149. Loss: 1.1685967445373535 \n",
      "Iteration: 2150. Loss: 1.0733743906021118 \n",
      "Iteration: 2151. Loss: 1.2397459745407104 \n",
      "Iteration: 2152. Loss: 1.2313765287399292 \n",
      "Iteration: 2153. Loss: 1.170861840248108 \n",
      "Iteration: 2154. Loss: 1.174403429031372 \n",
      "Iteration: 2155. Loss: 1.1779366731643677 \n",
      "Iteration: 2156. Loss: 1.1167546510696411 \n",
      "Iteration: 2157. Loss: 1.1847060918807983 \n",
      "Iteration: 2158. Loss: 1.1730865240097046 \n",
      "Iteration: 2159. Loss: 1.232184648513794 \n",
      "Iteration: 2160. Loss: 1.170634388923645 \n",
      "Iteration: 2161. Loss: 1.1714919805526733 \n",
      "Iteration: 2162. Loss: 1.1269451379776 \n",
      "Iteration: 2163. Loss: 1.060662031173706 \n",
      "Iteration: 2164. Loss: 1.1737340688705444 \n",
      "Iteration: 2165. Loss: 1.1432064771652222 \n",
      "Iteration: 2166. Loss: 1.135352373123169 \n",
      "Iteration: 2167. Loss: 1.1359976530075073 \n",
      "Iteration: 2168. Loss: 1.1892940998077393 \n",
      "Iteration: 2169. Loss: 1.2461442947387695 \n",
      "Iteration: 2170. Loss: 1.107597827911377 \n",
      "Iteration: 2171. Loss: 1.1812790632247925 \n",
      "Iteration: 2172. Loss: 1.1732136011123657 \n",
      "Iteration: 2173. Loss: 1.1210849285125732 \n",
      "Iteration: 2174. Loss: 1.1789606809616089 \n",
      "Iteration: 2175. Loss: 1.0385427474975586 \n",
      "Iteration: 2176. Loss: 1.1024338006973267 \n",
      "Iteration: 2177. Loss: 1.0825905799865723 \n",
      "Iteration: 2178. Loss: 1.2540441751480103 \n",
      "Iteration: 2179. Loss: 1.2234901189804077 \n",
      "Iteration: 2180. Loss: 1.153437614440918 \n",
      "Iteration: 2181. Loss: 1.2096718549728394 \n",
      "Iteration: 2182. Loss: 1.2107402086257935 \n",
      "Iteration: 2183. Loss: 1.0546339750289917 \n",
      "Iteration: 2184. Loss: 1.1921570301055908 \n",
      "Iteration: 2185. Loss: 1.129166603088379 \n",
      "Iteration: 2186. Loss: 1.052977204322815 \n",
      "Iteration: 2187. Loss: 1.239376187324524 \n",
      "Iteration: 2188. Loss: 1.1316804885864258 \n",
      "Iteration: 2189. Loss: 1.1411460638046265 \n",
      "Iteration: 2190. Loss: 1.1134867668151855 \n",
      "Iteration: 2191. Loss: 1.1909018754959106 \n",
      "Iteration: 2192. Loss: 1.20478093624115 \n",
      "Iteration: 2193. Loss: 1.1689157485961914 \n",
      "Iteration: 2194. Loss: 1.1373172998428345 \n",
      "Iteration: 2195. Loss: 1.0778443813323975 \n",
      "Iteration: 2196. Loss: 1.0873013734817505 \n",
      "Iteration: 2197. Loss: 1.2150627374649048 \n",
      "Iteration: 2198. Loss: 1.2165919542312622 \n",
      "Iteration: 2199. Loss: 1.1979387998580933 \n",
      "Iteration: 2200. Loss: 1.1343498229980469 \n",
      "Iteration: 2201. Loss: 1.1380221843719482 \n",
      "Iteration: 2202. Loss: 1.2814679145812988 \n",
      "Iteration: 2203. Loss: 1.1018881797790527 \n",
      "Iteration: 2204. Loss: 1.0942904949188232 \n",
      "Iteration: 2205. Loss: 1.1896066665649414 \n",
      "Iteration: 2206. Loss: 1.1701871156692505 \n",
      "Iteration: 2207. Loss: 1.23488187789917 \n",
      "Iteration: 2208. Loss: 1.2345306873321533 \n",
      "Iteration: 2209. Loss: 1.1849347352981567 \n",
      "Iteration: 2210. Loss: 1.119606375694275 \n",
      "Iteration: 2211. Loss: 1.1057178974151611 \n",
      "Iteration: 2212. Loss: 1.120966911315918 \n",
      "Iteration: 2213. Loss: 1.069008231163025 \n",
      "Iteration: 2214. Loss: 1.1040809154510498 \n",
      "Iteration: 2215. Loss: 1.2368135452270508 \n",
      "Iteration: 2216. Loss: 1.192294955253601 \n",
      "Iteration: 2217. Loss: 1.2517756223678589 \n",
      "Iteration: 2218. Loss: 1.25441575050354 \n",
      "Iteration: 2219. Loss: 1.2073520421981812 \n",
      "Iteration: 2220. Loss: 1.17203950881958 \n",
      "Iteration: 2221. Loss: 1.1346399784088135 \n",
      "Iteration: 2222. Loss: 1.1664241552352905 \n",
      "Iteration: 2223. Loss: 1.2744842767715454 \n",
      "Iteration: 2224. Loss: 1.1849327087402344 \n",
      "Iteration: 2225. Loss: 1.1622084379196167 \n",
      "Iteration: 2226. Loss: 1.258304476737976 \n",
      "Iteration: 2227. Loss: 1.1174372434616089 \n",
      "Iteration: 2228. Loss: 1.2125883102416992 \n",
      "Iteration: 2229. Loss: 1.167383074760437 \n",
      "Iteration: 2230. Loss: 1.1561120748519897 \n",
      "Iteration: 2231. Loss: 1.2241637706756592 \n",
      "Iteration: 2232. Loss: 1.1274343729019165 \n",
      "Iteration: 2233. Loss: 1.1800161600112915 \n",
      "Iteration: 2234. Loss: 1.1770882606506348 \n",
      "Iteration: 2235. Loss: 1.1398935317993164 \n",
      "Iteration: 2236. Loss: 1.0897780656814575 \n",
      "Iteration: 2237. Loss: 1.2062853574752808 \n",
      "Iteration: 2238. Loss: 1.133885145187378 \n",
      "Iteration: 2239. Loss: 1.1567462682724 \n",
      "Iteration: 2240. Loss: 1.0936009883880615 \n",
      "Iteration: 2241. Loss: 1.1287188529968262 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2242. Loss: 1.122222900390625 \n",
      "Iteration: 2243. Loss: 1.1172996759414673 \n",
      "Iteration: 2244. Loss: 1.0484312772750854 \n",
      "Iteration: 2245. Loss: 1.1143211126327515 \n",
      "Iteration: 2246. Loss: 1.1445711851119995 \n",
      "Iteration: 2247. Loss: 1.1407508850097656 \n",
      "Iteration: 2248. Loss: 1.0730369091033936 \n",
      "Iteration: 2249. Loss: 1.1655486822128296 \n",
      "Iteration: 2250. Loss: 1.1855578422546387 \n",
      "Iteration: 2251. Loss: 1.1247870922088623 \n",
      "Iteration: 2252. Loss: 1.1429412364959717 \n",
      "Iteration: 2253. Loss: 1.0887362957000732 \n",
      "Iteration: 2254. Loss: 1.080228328704834 \n",
      "Iteration: 2255. Loss: 1.1226515769958496 \n",
      "Iteration: 2256. Loss: 1.1071655750274658 \n",
      "Iteration: 2257. Loss: 1.1055172681808472 \n",
      "Iteration: 2258. Loss: 1.1104328632354736 \n",
      "Iteration: 2259. Loss: 1.1528836488723755 \n",
      "Iteration: 2260. Loss: 1.0896950960159302 \n",
      "Iteration: 2261. Loss: 1.1465601921081543 \n",
      "Iteration: 2262. Loss: 1.200461506843567 \n",
      "Iteration: 2263. Loss: 1.1170835494995117 \n",
      "Iteration: 2264. Loss: 1.1716982126235962 \n",
      "Iteration: 2265. Loss: 1.2060537338256836 \n",
      "Iteration: 2266. Loss: 1.1509928703308105 \n",
      "Iteration: 2267. Loss: 1.196895718574524 \n",
      "Iteration: 2268. Loss: 1.1737018823623657 \n",
      "Iteration: 2269. Loss: 1.124022364616394 \n",
      "Iteration: 2270. Loss: 0.9656307697296143 \n",
      "Iteration: 2271. Loss: 1.0813732147216797 \n",
      "Iteration: 2272. Loss: 1.0744045972824097 \n",
      "Iteration: 2273. Loss: 1.0952173471450806 \n",
      "Iteration: 2274. Loss: 1.2248955965042114 \n",
      "Iteration: 2275. Loss: 1.117708444595337 \n",
      "Iteration: 2276. Loss: 1.101688265800476 \n",
      "Iteration: 2277. Loss: 1.0992786884307861 \n",
      "Iteration: 2278. Loss: 1.1257842779159546 \n",
      "Iteration: 2279. Loss: 1.22846519947052 \n",
      "Iteration: 2280. Loss: 1.065228819847107 \n",
      "Iteration: 2281. Loss: 1.2032406330108643 \n",
      "Iteration: 2282. Loss: 1.2104804515838623 \n",
      "Iteration: 2283. Loss: 1.0946309566497803 \n",
      "Iteration: 2284. Loss: 1.1692135334014893 \n",
      "Iteration: 2285. Loss: 1.1249581575393677 \n",
      "Iteration: 2286. Loss: 1.0849294662475586 \n",
      "Iteration: 2287. Loss: 1.1184287071228027 \n",
      "Iteration: 2288. Loss: 1.105422019958496 \n",
      "Iteration: 2289. Loss: 1.1557068824768066 \n",
      "Iteration: 2290. Loss: 1.1061644554138184 \n",
      "Iteration: 2291. Loss: 1.2078758478164673 \n",
      "Iteration: 2292. Loss: 1.1282131671905518 \n",
      "Iteration: 2293. Loss: 1.1716493368148804 \n",
      "Iteration: 2294. Loss: 1.085374116897583 \n",
      "Iteration: 2295. Loss: 1.1206306219100952 \n",
      "Iteration: 2296. Loss: 1.0682655572891235 \n",
      "Iteration: 2297. Loss: 1.1654713153839111 \n",
      "Iteration: 2298. Loss: 1.140446424484253 \n",
      "Iteration: 2299. Loss: 1.1336827278137207 \n",
      "Iteration: 2300. Loss: 1.102526068687439 \n",
      "Iteration: 2301. Loss: 1.1758004426956177 \n",
      "Iteration: 2302. Loss: 1.0963035821914673 \n",
      "Iteration: 2303. Loss: 1.0389931201934814 \n",
      "Iteration: 2304. Loss: 1.100740671157837 \n",
      "Iteration: 2305. Loss: 1.0970277786254883 \n",
      "Iteration: 2306. Loss: 1.1694103479385376 \n",
      "Iteration: 2307. Loss: 1.161007046699524 \n",
      "Iteration: 2308. Loss: 1.0524784326553345 \n",
      "Iteration: 2309. Loss: 1.106940746307373 \n",
      "Iteration: 2310. Loss: 1.155144453048706 \n",
      "Iteration: 2311. Loss: 1.1235957145690918 \n",
      "Iteration: 2312. Loss: 1.0619392395019531 \n",
      "Iteration: 2313. Loss: 1.127640724182129 \n",
      "Iteration: 2314. Loss: 0.9976307153701782 \n",
      "Iteration: 2315. Loss: 1.130929946899414 \n",
      "Iteration: 2316. Loss: 1.1237527132034302 \n",
      "Iteration: 2317. Loss: 1.1002562046051025 \n",
      "Iteration: 2318. Loss: 1.1387076377868652 \n",
      "Iteration: 2319. Loss: 1.076635479927063 \n",
      "Iteration: 2320. Loss: 1.049748182296753 \n",
      "Iteration: 2321. Loss: 1.1066488027572632 \n",
      "Iteration: 2322. Loss: 1.0773049592971802 \n",
      "Iteration: 2323. Loss: 1.0541325807571411 \n",
      "Iteration: 2324. Loss: 1.1646212339401245 \n",
      "Iteration: 2325. Loss: 1.2001200914382935 \n",
      "Iteration: 2326. Loss: 1.1351914405822754 \n",
      "Iteration: 2327. Loss: 1.1184971332550049 \n",
      "Iteration: 2328. Loss: 1.153627634048462 \n",
      "Iteration: 2329. Loss: 1.052815556526184 \n",
      "Iteration: 2330. Loss: 1.1846873760223389 \n",
      "Iteration: 2331. Loss: 1.182112216949463 \n",
      "Iteration: 2332. Loss: 1.1132402420043945 \n",
      "Iteration: 2333. Loss: 1.0860404968261719 \n",
      "Iteration: 2334. Loss: 1.1491340398788452 \n",
      "Iteration: 2335. Loss: 1.163731336593628 \n",
      "Iteration: 2336. Loss: 1.0828044414520264 \n",
      "Iteration: 2337. Loss: 1.1498596668243408 \n",
      "Iteration: 2338. Loss: 1.102776050567627 \n",
      "Iteration: 2339. Loss: 1.0954242944717407 \n",
      "Iteration: 2340. Loss: 1.085010290145874 \n",
      "Iteration: 2341. Loss: 1.086413860321045 \n",
      "Iteration: 2342. Loss: 1.1170161962509155 \n",
      "Iteration: 2343. Loss: 1.2084457874298096 \n",
      "Iteration: 2344. Loss: 1.0443675518035889 \n",
      "Iteration: 2345. Loss: 1.148474097251892 \n",
      "Iteration: 2346. Loss: 1.2354086637496948 \n",
      "Iteration: 2347. Loss: 1.0648837089538574 \n",
      "Iteration: 2348. Loss: 1.2117019891738892 \n",
      "Iteration: 2349. Loss: 1.118263602256775 \n",
      "Iteration: 2350. Loss: 1.1361483335494995 \n",
      "Iteration: 2351. Loss: 1.111090064048767 \n",
      "Iteration: 2352. Loss: 1.1071112155914307 \n",
      "Iteration: 2353. Loss: 1.0813453197479248 \n",
      "Iteration: 2354. Loss: 1.068372368812561 \n",
      "Iteration: 2355. Loss: 1.1539592742919922 \n",
      "Iteration: 2356. Loss: 1.078890323638916 \n",
      "Iteration: 2357. Loss: 1.2324732542037964 \n",
      "Iteration: 2358. Loss: 1.1160063743591309 \n",
      "Iteration: 2359. Loss: 1.0737837553024292 \n",
      "Iteration: 2360. Loss: 1.0488213300704956 \n",
      "Iteration: 2361. Loss: 1.1305893659591675 \n",
      "Iteration: 2362. Loss: 1.1291385889053345 \n",
      "Iteration: 2363. Loss: 1.1825660467147827 \n",
      "Iteration: 2364. Loss: 1.1187797784805298 \n",
      "Iteration: 2365. Loss: 1.113451600074768 \n",
      "Iteration: 2366. Loss: 1.0708767175674438 \n",
      "Iteration: 2367. Loss: 1.101132869720459 \n",
      "Iteration: 2368. Loss: 1.2293733358383179 \n",
      "Iteration: 2369. Loss: 1.1500418186187744 \n",
      "Iteration: 2370. Loss: 1.0475072860717773 \n",
      "Iteration: 2371. Loss: 1.0725224018096924 \n",
      "Iteration: 2372. Loss: 1.1231786012649536 \n",
      "Iteration: 2373. Loss: 1.1960723400115967 \n",
      "Iteration: 2374. Loss: 1.1420347690582275 \n",
      "Iteration: 2375. Loss: 1.0498672723770142 \n",
      "Iteration: 2376. Loss: 1.1382983922958374 \n",
      "Iteration: 2377. Loss: 1.1835949420928955 \n",
      "Iteration: 2378. Loss: 1.1304278373718262 \n",
      "Iteration: 2379. Loss: 1.0359963178634644 \n",
      "Iteration: 2380. Loss: 1.1153842210769653 \n",
      "Iteration: 2381. Loss: 1.181022047996521 \n",
      "Iteration: 2382. Loss: 1.0856763124465942 \n",
      "Iteration: 2383. Loss: 1.0947978496551514 \n",
      "Iteration: 2384. Loss: 1.1577556133270264 \n",
      "Iteration: 2385. Loss: 1.1243445873260498 \n",
      "Iteration: 2386. Loss: 1.0632270574569702 \n",
      "Iteration: 2387. Loss: 1.138443112373352 \n",
      "Iteration: 2388. Loss: 1.1058365106582642 \n",
      "Iteration: 2389. Loss: 1.1025688648223877 \n",
      "Iteration: 2390. Loss: 1.1302964687347412 \n",
      "Iteration: 2391. Loss: 1.094543695449829 \n",
      "Iteration: 2392. Loss: 1.2546943426132202 \n",
      "Iteration: 2393. Loss: 1.0713962316513062 \n",
      "Iteration: 2394. Loss: 1.1619397401809692 \n",
      "Iteration: 2395. Loss: 1.1008291244506836 \n",
      "Iteration: 2396. Loss: 1.0480858087539673 \n",
      "Iteration: 2397. Loss: 1.0491971969604492 \n",
      "Iteration: 2398. Loss: 1.1301425695419312 \n",
      "Iteration: 2399. Loss: 1.1405093669891357 \n",
      "Iteration: 2400. Loss: 1.1114044189453125 \n",
      "Iteration: 2401. Loss: 1.128633975982666 \n",
      "Iteration: 2402. Loss: 1.0381135940551758 \n",
      "Iteration: 2403. Loss: 1.099793553352356 \n",
      "Iteration: 2404. Loss: 1.1362802982330322 \n",
      "Iteration: 2405. Loss: 1.129788875579834 \n",
      "Iteration: 2406. Loss: 1.0254089832305908 \n",
      "Iteration: 2407. Loss: 1.1018211841583252 \n",
      "Iteration: 2408. Loss: 1.0300425291061401 \n",
      "Iteration: 2409. Loss: 1.1250157356262207 \n",
      "Iteration: 2410. Loss: 1.0840634107589722 \n",
      "Iteration: 2411. Loss: 1.1363052129745483 \n",
      "Iteration: 2412. Loss: 1.082014799118042 \n",
      "Iteration: 2413. Loss: 1.1315908432006836 \n",
      "Iteration: 2414. Loss: 1.2126615047454834 \n",
      "Iteration: 2415. Loss: 1.0784257650375366 \n",
      "Iteration: 2416. Loss: 1.1413575410842896 \n",
      "Iteration: 2417. Loss: 1.014931082725525 \n",
      "Iteration: 2418. Loss: 1.1382373571395874 \n",
      "Iteration: 2419. Loss: 1.1721299886703491 \n",
      "Iteration: 2420. Loss: 1.1489901542663574 \n",
      "Iteration: 2421. Loss: 1.1375569105148315 \n",
      "Iteration: 2422. Loss: 1.0729897022247314 \n",
      "Iteration: 2423. Loss: 1.1301919221878052 \n",
      "Iteration: 2424. Loss: 1.0633220672607422 \n",
      "Iteration: 2425. Loss: 1.1142598390579224 \n",
      "Iteration: 2426. Loss: 1.075994610786438 \n",
      "Iteration: 2427. Loss: 1.1169151067733765 \n",
      "Iteration: 2428. Loss: 1.0486366748809814 \n",
      "Iteration: 2429. Loss: 0.9909636974334717 \n",
      "Iteration: 2430. Loss: 1.157753348350525 \n",
      "Iteration: 2431. Loss: 1.070833683013916 \n",
      "Iteration: 2432. Loss: 1.1389286518096924 \n",
      "Iteration: 2433. Loss: 1.1613476276397705 \n",
      "Iteration: 2434. Loss: 1.1270776987075806 \n",
      "Iteration: 2435. Loss: 1.1221668720245361 \n",
      "Iteration: 2436. Loss: 1.1026849746704102 \n",
      "Iteration: 2437. Loss: 1.1016873121261597 \n",
      "Iteration: 2438. Loss: 1.084630012512207 \n",
      "Iteration: 2439. Loss: 1.159011960029602 \n",
      "Iteration: 2440. Loss: 1.083875298500061 \n",
      "Iteration: 2441. Loss: 1.1322382688522339 \n",
      "Iteration: 2442. Loss: 1.0989288091659546 \n",
      "Iteration: 2443. Loss: 1.1626399755477905 \n",
      "Iteration: 2444. Loss: 1.0949592590332031 \n",
      "Iteration: 2445. Loss: 1.0265157222747803 \n",
      "Iteration: 2446. Loss: 1.0104951858520508 \n",
      "Iteration: 2447. Loss: 1.1973880529403687 \n",
      "Iteration: 2448. Loss: 1.1020127534866333 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2449. Loss: 1.0953961610794067 \n",
      "Iteration: 2450. Loss: 1.015606164932251 \n",
      "Iteration: 2451. Loss: 1.2286826372146606 \n",
      "Iteration: 2452. Loss: 1.0998859405517578 \n",
      "Iteration: 2453. Loss: 1.0957274436950684 \n",
      "Iteration: 2454. Loss: 1.1876075267791748 \n",
      "Iteration: 2455. Loss: 1.0317060947418213 \n",
      "Iteration: 2456. Loss: 1.1469619274139404 \n",
      "Iteration: 2457. Loss: 1.1839783191680908 \n",
      "Iteration: 2458. Loss: 1.1069536209106445 \n",
      "Iteration: 2459. Loss: 1.0665838718414307 \n",
      "Iteration: 2460. Loss: 1.1472147703170776 \n",
      "Iteration: 2461. Loss: 1.0921075344085693 \n",
      "Iteration: 2462. Loss: 1.1462842226028442 \n",
      "Iteration: 2463. Loss: 1.1712846755981445 \n",
      "Iteration: 2464. Loss: 1.2491493225097656 \n",
      "Iteration: 2465. Loss: 1.141188383102417 \n",
      "Iteration: 2466. Loss: 1.0468268394470215 \n",
      "Iteration: 2467. Loss: 1.0579148530960083 \n",
      "Iteration: 2468. Loss: 1.0496197938919067 \n",
      "Iteration: 2469. Loss: 1.0863491296768188 \n",
      "Iteration: 2470. Loss: 1.0771093368530273 \n",
      "Iteration: 2471. Loss: 1.120985984802246 \n",
      "Iteration: 2472. Loss: 1.0904463529586792 \n",
      "Iteration: 2473. Loss: 1.0849684476852417 \n",
      "Iteration: 2474. Loss: 1.162665605545044 \n",
      "Iteration: 2475. Loss: 1.1197535991668701 \n",
      "Iteration: 2476. Loss: 0.9569626450538635 \n",
      "Iteration: 2477. Loss: 1.0525527000427246 \n",
      "Iteration: 2478. Loss: 1.1631953716278076 \n",
      "Iteration: 2479. Loss: 1.0589687824249268 \n",
      "Iteration: 2480. Loss: 1.025415301322937 \n",
      "Iteration: 2481. Loss: 1.140082597732544 \n",
      "Iteration: 2482. Loss: 1.041458010673523 \n",
      "Iteration: 2483. Loss: 1.2045753002166748 \n",
      "Iteration: 2484. Loss: 1.0095018148422241 \n",
      "Iteration: 2485. Loss: 1.0980831384658813 \n",
      "Iteration: 2486. Loss: 1.0911247730255127 \n",
      "Iteration: 2487. Loss: 1.086754322052002 \n",
      "Iteration: 2488. Loss: 1.1678239107131958 \n",
      "Iteration: 2489. Loss: 1.1304783821105957 \n",
      "Iteration: 2490. Loss: 1.1741126775741577 \n",
      "Iteration: 2491. Loss: 1.075973391532898 \n",
      "Iteration: 2492. Loss: 1.0970515012741089 \n",
      "Iteration: 2493. Loss: 0.9919785857200623 \n",
      "Iteration: 2494. Loss: 1.0906243324279785 \n",
      "Iteration: 2495. Loss: 1.0412631034851074 \n",
      "Iteration: 2496. Loss: 1.142045021057129 \n",
      "Iteration: 2497. Loss: 1.1296334266662598 \n",
      "Iteration: 2498. Loss: 1.0899734497070312 \n",
      "Iteration: 2499. Loss: 1.1016814708709717 \n",
      "Iteration: 2500. Loss: 1.0841349363327026 \n",
      "Iteration: 2501. Loss: 1.17854905128479 \n",
      "Iteration: 2502. Loss: 1.0661649703979492 \n",
      "Iteration: 2503. Loss: 1.1602728366851807 \n",
      "Iteration: 2504. Loss: 1.059434175491333 \n",
      "Iteration: 2505. Loss: 0.9869648218154907 \n",
      "Iteration: 2506. Loss: 1.079928994178772 \n",
      "Iteration: 2507. Loss: 1.160857915878296 \n",
      "Iteration: 2508. Loss: 1.0350803136825562 \n",
      "Iteration: 2509. Loss: 1.0977189540863037 \n",
      "Iteration: 2510. Loss: 1.054201602935791 \n",
      "Iteration: 2511. Loss: 1.1388652324676514 \n",
      "Iteration: 2512. Loss: 1.0765464305877686 \n",
      "Iteration: 2513. Loss: 0.959064781665802 \n",
      "Iteration: 2514. Loss: 1.184646725654602 \n",
      "Iteration: 2515. Loss: 1.0777381658554077 \n",
      "Iteration: 2516. Loss: 1.0661753416061401 \n",
      "Iteration: 2517. Loss: 1.0963473320007324 \n",
      "Iteration: 2518. Loss: 1.132593035697937 \n",
      "Iteration: 2519. Loss: 1.0427800416946411 \n",
      "Iteration: 2520. Loss: 1.029091477394104 \n",
      "Iteration: 2521. Loss: 1.0864113569259644 \n",
      "Iteration: 2522. Loss: 1.0235918760299683 \n",
      "Iteration: 2523. Loss: 1.1113731861114502 \n",
      "Iteration: 2524. Loss: 1.0716220140457153 \n",
      "Iteration: 2525. Loss: 1.0573198795318604 \n",
      "Iteration: 2526. Loss: 1.0197967290878296 \n",
      "Iteration: 2527. Loss: 1.1380815505981445 \n",
      "Iteration: 2528. Loss: 1.0862541198730469 \n",
      "Iteration: 2529. Loss: 1.1265069246292114 \n",
      "Iteration: 2530. Loss: 1.0616720914840698 \n",
      "Iteration: 2531. Loss: 1.0865997076034546 \n",
      "Iteration: 2532. Loss: 0.9916998147964478 \n",
      "Iteration: 2533. Loss: 1.032516360282898 \n",
      "Iteration: 2534. Loss: 1.1011639833450317 \n",
      "Iteration: 2535. Loss: 1.0999071598052979 \n",
      "Iteration: 2536. Loss: 1.0719506740570068 \n",
      "Iteration: 2537. Loss: 1.0506222248077393 \n",
      "Iteration: 2538. Loss: 1.1499654054641724 \n",
      "Iteration: 2539. Loss: 1.032335877418518 \n",
      "Iteration: 2540. Loss: 0.9619728326797485 \n",
      "Iteration: 2541. Loss: 1.1334046125411987 \n",
      "Iteration: 2542. Loss: 1.0656083822250366 \n",
      "Iteration: 2543. Loss: 0.9674056172370911 \n",
      "Iteration: 2544. Loss: 1.1064563989639282 \n",
      "Iteration: 2545. Loss: 1.0040326118469238 \n",
      "Iteration: 2546. Loss: 1.0748122930526733 \n",
      "Iteration: 2547. Loss: 1.10284423828125 \n",
      "Iteration: 2548. Loss: 1.1319998502731323 \n",
      "Iteration: 2549. Loss: 1.0917476415634155 \n",
      "Iteration: 2550. Loss: 1.0707907676696777 \n",
      "Iteration: 2551. Loss: 1.0883678197860718 \n",
      "Iteration: 2552. Loss: 1.1122558116912842 \n",
      "Iteration: 2553. Loss: 1.025059700012207 \n",
      "Iteration: 2554. Loss: 1.0268172025680542 \n",
      "Iteration: 2555. Loss: 1.0313774347305298 \n",
      "Iteration: 2556. Loss: 1.0405621528625488 \n",
      "Iteration: 2557. Loss: 1.049190640449524 \n",
      "Iteration: 2558. Loss: 1.1085361242294312 \n",
      "Iteration: 2559. Loss: 1.1585947275161743 \n",
      "Iteration: 2560. Loss: 1.1305508613586426 \n",
      "Iteration: 2561. Loss: 1.0660903453826904 \n",
      "Iteration: 2562. Loss: 1.0452953577041626 \n",
      "Iteration: 2563. Loss: 1.096560001373291 \n",
      "Iteration: 2564. Loss: 1.1323513984680176 \n",
      "Iteration: 2565. Loss: 1.090611219406128 \n",
      "Iteration: 2566. Loss: 1.1105458736419678 \n",
      "Iteration: 2567. Loss: 1.0302705764770508 \n",
      "Iteration: 2568. Loss: 1.0465528964996338 \n",
      "Iteration: 2569. Loss: 1.0627551078796387 \n",
      "Iteration: 2570. Loss: 1.1626945734024048 \n",
      "Iteration: 2571. Loss: 1.1196537017822266 \n",
      "Iteration: 2572. Loss: 1.0774067640304565 \n",
      "Iteration: 2573. Loss: 1.083103060722351 \n",
      "Iteration: 2574. Loss: 1.0808229446411133 \n",
      "Iteration: 2575. Loss: 1.137113094329834 \n",
      "Iteration: 2576. Loss: 1.05973482131958 \n",
      "Iteration: 2577. Loss: 1.1287639141082764 \n",
      "Iteration: 2578. Loss: 0.9850859642028809 \n",
      "Iteration: 2579. Loss: 1.0361624956130981 \n",
      "Iteration: 2580. Loss: 1.139162302017212 \n",
      "Iteration: 2581. Loss: 1.0555366277694702 \n",
      "Iteration: 2582. Loss: 0.9594867825508118 \n",
      "Iteration: 2583. Loss: 1.0035603046417236 \n",
      "Iteration: 2584. Loss: 1.0613023042678833 \n",
      "Iteration: 2585. Loss: 1.1307365894317627 \n",
      "Iteration: 2586. Loss: 1.113296627998352 \n",
      "Iteration: 2587. Loss: 1.0371580123901367 \n",
      "Iteration: 2588. Loss: 1.1381813287734985 \n",
      "Iteration: 2589. Loss: 0.9837967753410339 \n",
      "Iteration: 2590. Loss: 1.128955602645874 \n",
      "Iteration: 2591. Loss: 0.9782153367996216 \n",
      "Iteration: 2592. Loss: 1.125036597251892 \n",
      "Iteration: 2593. Loss: 1.1346445083618164 \n",
      "Iteration: 2594. Loss: 1.044640064239502 \n",
      "Iteration: 2595. Loss: 0.9881888628005981 \n",
      "Iteration: 2596. Loss: 1.0749701261520386 \n",
      "Iteration: 2597. Loss: 1.1031873226165771 \n",
      "Iteration: 2598. Loss: 1.029295802116394 \n",
      "Iteration: 2599. Loss: 1.0547497272491455 \n",
      "Iteration: 2600. Loss: 0.9996740818023682 \n",
      "Iteration: 2601. Loss: 1.093335747718811 \n",
      "Iteration: 2602. Loss: 1.149251103401184 \n",
      "Iteration: 2603. Loss: 0.991895854473114 \n",
      "Iteration: 2604. Loss: 1.0414565801620483 \n",
      "Iteration: 2605. Loss: 0.9340333342552185 \n",
      "Iteration: 2606. Loss: 1.1239628791809082 \n",
      "Iteration: 2607. Loss: 1.0445666313171387 \n",
      "Iteration: 2608. Loss: 1.0969330072402954 \n",
      "Iteration: 2609. Loss: 1.0595611333847046 \n",
      "Iteration: 2610. Loss: 0.9534937143325806 \n",
      "Iteration: 2611. Loss: 1.0235767364501953 \n",
      "Iteration: 2612. Loss: 1.1648989915847778 \n",
      "Iteration: 2613. Loss: 1.0916115045547485 \n",
      "Iteration: 2614. Loss: 1.0347299575805664 \n",
      "Iteration: 2615. Loss: 1.0045217275619507 \n",
      "Iteration: 2616. Loss: 1.1557295322418213 \n",
      "Iteration: 2617. Loss: 1.0914286375045776 \n",
      "Iteration: 2618. Loss: 1.0783921480178833 \n",
      "Iteration: 2619. Loss: 1.0559314489364624 \n",
      "Iteration: 2620. Loss: 1.1486634016036987 \n",
      "Iteration: 2621. Loss: 1.0916836261749268 \n",
      "Iteration: 2622. Loss: 1.0470086336135864 \n",
      "Iteration: 2623. Loss: 1.1225476264953613 \n",
      "Iteration: 2624. Loss: 1.0419971942901611 \n",
      "Iteration: 2625. Loss: 1.172029733657837 \n",
      "Iteration: 2626. Loss: 1.0762364864349365 \n",
      "Iteration: 2627. Loss: 1.1114881038665771 \n",
      "Iteration: 2628. Loss: 1.0526829957962036 \n",
      "Iteration: 2629. Loss: 1.0290254354476929 \n",
      "Iteration: 2630. Loss: 1.05405855178833 \n",
      "Iteration: 2631. Loss: 1.133621335029602 \n",
      "Iteration: 2632. Loss: 1.0345207452774048 \n",
      "Iteration: 2633. Loss: 1.069890022277832 \n",
      "Iteration: 2634. Loss: 1.097313404083252 \n",
      "Iteration: 2635. Loss: 0.9689882397651672 \n",
      "Iteration: 2636. Loss: 0.9932687878608704 \n",
      "Iteration: 2637. Loss: 1.1270642280578613 \n",
      "Iteration: 2638. Loss: 1.0944855213165283 \n",
      "Iteration: 2639. Loss: 1.0016086101531982 \n",
      "Iteration: 2640. Loss: 1.0546437501907349 \n",
      "Iteration: 2641. Loss: 1.0632824897766113 \n",
      "Iteration: 2642. Loss: 1.1254558563232422 \n",
      "Iteration: 2643. Loss: 1.088566780090332 \n",
      "Iteration: 2644. Loss: 1.1127705574035645 \n",
      "Iteration: 2645. Loss: 1.0528690814971924 \n",
      "Iteration: 2646. Loss: 1.1298823356628418 \n",
      "Iteration: 2647. Loss: 1.1501994132995605 \n",
      "Iteration: 2648. Loss: 1.1312922239303589 \n",
      "Iteration: 2649. Loss: 0.9441667199134827 \n",
      "Iteration: 2650. Loss: 1.1145803928375244 \n",
      "Iteration: 2651. Loss: 1.0009512901306152 \n",
      "Iteration: 2652. Loss: 1.0381426811218262 \n",
      "Iteration: 2653. Loss: 1.079940676689148 \n",
      "Iteration: 2654. Loss: 0.9926719665527344 \n",
      "Iteration: 2655. Loss: 1.1262803077697754 \n",
      "Iteration: 2656. Loss: 1.0407880544662476 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2657. Loss: 1.0046517848968506 \n",
      "Iteration: 2658. Loss: 1.1154448986053467 \n",
      "Iteration: 2659. Loss: 1.032764196395874 \n",
      "Iteration: 2660. Loss: 1.0675519704818726 \n",
      "Iteration: 2661. Loss: 1.0796701908111572 \n",
      "Iteration: 2662. Loss: 1.0499353408813477 \n",
      "Iteration: 2663. Loss: 0.9677568078041077 \n",
      "Iteration: 2664. Loss: 1.036036729812622 \n",
      "Iteration: 2665. Loss: 1.0559988021850586 \n",
      "Iteration: 2666. Loss: 1.093860149383545 \n",
      "Iteration: 2667. Loss: 1.0448294878005981 \n",
      "Iteration: 2668. Loss: 1.1001588106155396 \n",
      "Iteration: 2669. Loss: 1.0530884265899658 \n",
      "Iteration: 2670. Loss: 1.011570930480957 \n",
      "Iteration: 2671. Loss: 1.0691083669662476 \n",
      "Iteration: 2672. Loss: 1.110663652420044 \n",
      "Iteration: 2673. Loss: 1.0387343168258667 \n",
      "Iteration: 2674. Loss: 1.108190655708313 \n",
      "Iteration: 2675. Loss: 0.9801725149154663 \n",
      "Iteration: 2676. Loss: 1.1351929903030396 \n",
      "Iteration: 2677. Loss: 0.9972841143608093 \n",
      "Iteration: 2678. Loss: 0.9413983225822449 \n",
      "Iteration: 2679. Loss: 1.117661952972412 \n",
      "Iteration: 2680. Loss: 1.015756368637085 \n",
      "Iteration: 2681. Loss: 1.020017147064209 \n",
      "Iteration: 2682. Loss: 1.0772818326950073 \n",
      "Iteration: 2683. Loss: 1.0448282957077026 \n",
      "Iteration: 2684. Loss: 1.0768183469772339 \n",
      "Iteration: 2685. Loss: 1.1486152410507202 \n",
      "Iteration: 2686. Loss: 1.0128624439239502 \n",
      "Iteration: 2687. Loss: 1.0475332736968994 \n",
      "Iteration: 2688. Loss: 0.960519552230835 \n",
      "Iteration: 2689. Loss: 0.9878765940666199 \n",
      "Iteration: 2690. Loss: 1.015590786933899 \n",
      "Iteration: 2691. Loss: 1.136573314666748 \n",
      "Iteration: 2692. Loss: 1.1716967821121216 \n",
      "Iteration: 2693. Loss: 1.0925747156143188 \n",
      "Iteration: 2694. Loss: 1.1454466581344604 \n",
      "Iteration: 2695. Loss: 1.083000659942627 \n",
      "Iteration: 2696. Loss: 1.1134564876556396 \n",
      "Iteration: 2697. Loss: 1.0291253328323364 \n",
      "Iteration: 2698. Loss: 1.073966145515442 \n",
      "Iteration: 2699. Loss: 1.020127296447754 \n",
      "Iteration: 2700. Loss: 1.0934078693389893 \n",
      "Iteration: 2701. Loss: 1.0916659832000732 \n",
      "Iteration: 2702. Loss: 1.081270694732666 \n",
      "Iteration: 2703. Loss: 1.0465502738952637 \n",
      "Iteration: 2704. Loss: 1.1192647218704224 \n",
      "Iteration: 2705. Loss: 1.010697364807129 \n",
      "Iteration: 2706. Loss: 1.0796681642532349 \n",
      "Iteration: 2707. Loss: 1.0219643115997314 \n",
      "Iteration: 2708. Loss: 1.0592573881149292 \n",
      "Iteration: 2709. Loss: 1.0120025873184204 \n",
      "Iteration: 2710. Loss: 1.0649120807647705 \n",
      "Iteration: 2711. Loss: 0.9786415100097656 \n",
      "Iteration: 2712. Loss: 0.9511756896972656 \n",
      "Iteration: 2713. Loss: 1.0666654109954834 \n",
      "Iteration: 2714. Loss: 1.0611772537231445 \n",
      "Iteration: 2715. Loss: 1.198919415473938 \n",
      "Iteration: 2716. Loss: 1.1001086235046387 \n",
      "Iteration: 2717. Loss: 1.0181752443313599 \n",
      "Iteration: 2718. Loss: 1.1254199743270874 \n",
      "Iteration: 2719. Loss: 1.1521798372268677 \n",
      "Iteration: 2720. Loss: 1.1421449184417725 \n",
      "Iteration: 2721. Loss: 1.0453535318374634 \n",
      "Iteration: 2722. Loss: 1.130950689315796 \n",
      "Iteration: 2723. Loss: 1.0394072532653809 \n",
      "Iteration: 2724. Loss: 1.0350593328475952 \n",
      "Iteration: 2725. Loss: 1.0402299165725708 \n",
      "Iteration: 2726. Loss: 1.1633293628692627 \n",
      "Iteration: 2727. Loss: 1.0326393842697144 \n",
      "Iteration: 2728. Loss: 1.0491893291473389 \n",
      "Iteration: 2729. Loss: 1.0028592348098755 \n",
      "Iteration: 2730. Loss: 1.0375840663909912 \n",
      "Iteration: 2731. Loss: 1.0278254747390747 \n",
      "Iteration: 2732. Loss: 0.9877853393554688 \n",
      "Iteration: 2733. Loss: 1.0037997961044312 \n",
      "Iteration: 2734. Loss: 1.100865364074707 \n",
      "Iteration: 2735. Loss: 1.0797488689422607 \n",
      "Iteration: 2736. Loss: 1.035506010055542 \n",
      "Iteration: 2737. Loss: 1.1317368745803833 \n",
      "Iteration: 2738. Loss: 1.0068721771240234 \n",
      "Iteration: 2739. Loss: 0.9303476214408875 \n",
      "Iteration: 2740. Loss: 1.0197261571884155 \n",
      "Iteration: 2741. Loss: 1.0859994888305664 \n",
      "Iteration: 2742. Loss: 1.161278247833252 \n",
      "Iteration: 2743. Loss: 1.0333081483840942 \n",
      "Iteration: 2744. Loss: 1.1092097759246826 \n",
      "Iteration: 2745. Loss: 1.0645451545715332 \n",
      "Iteration: 2746. Loss: 1.035277247428894 \n",
      "Iteration: 2747. Loss: 1.1057562828063965 \n",
      "Iteration: 2748. Loss: 1.1123181581497192 \n",
      "Iteration: 2749. Loss: 1.1858066320419312 \n",
      "Iteration: 2750. Loss: 1.0075713396072388 \n",
      "Iteration: 2751. Loss: 1.0725282430648804 \n",
      "Iteration: 2752. Loss: 1.1143542528152466 \n",
      "Iteration: 2753. Loss: 1.0372264385223389 \n",
      "Iteration: 2754. Loss: 1.0036942958831787 \n",
      "Iteration: 2755. Loss: 1.0326452255249023 \n",
      "Iteration: 2756. Loss: 1.0698816776275635 \n",
      "Iteration: 2757. Loss: 1.0680022239685059 \n",
      "Iteration: 2758. Loss: 0.9873392581939697 \n",
      "Iteration: 2759. Loss: 1.043725848197937 \n",
      "Iteration: 2760. Loss: 1.0396275520324707 \n",
      "Iteration: 2761. Loss: 0.9544732570648193 \n",
      "Iteration: 2762. Loss: 1.1520529985427856 \n",
      "Iteration: 2763. Loss: 1.023455023765564 \n",
      "Iteration: 2764. Loss: 1.0791298151016235 \n",
      "Iteration: 2765. Loss: 1.0171637535095215 \n",
      "Iteration: 2766. Loss: 0.8915547132492065 \n",
      "Iteration: 2767. Loss: 1.0570144653320312 \n",
      "Iteration: 2768. Loss: 1.0035580396652222 \n",
      "Iteration: 2769. Loss: 1.0370746850967407 \n",
      "Iteration: 2770. Loss: 0.9854557514190674 \n",
      "Iteration: 2771. Loss: 1.1059013605117798 \n",
      "Iteration: 2772. Loss: 0.9314923882484436 \n",
      "Iteration: 2773. Loss: 1.0113093852996826 \n",
      "Iteration: 2774. Loss: 0.9837918281555176 \n",
      "Iteration: 2775. Loss: 1.0754592418670654 \n",
      "Iteration: 2776. Loss: 1.1388906240463257 \n",
      "Iteration: 2777. Loss: 1.073542594909668 \n",
      "Iteration: 2778. Loss: 0.9787650108337402 \n",
      "Iteration: 2779. Loss: 0.9309807419776917 \n",
      "Iteration: 2780. Loss: 1.0319021940231323 \n",
      "Iteration: 2781. Loss: 1.063327431678772 \n",
      "Iteration: 2782. Loss: 1.0014967918395996 \n",
      "Iteration: 2783. Loss: 0.9530104994773865 \n",
      "Iteration: 2784. Loss: 1.0911855697631836 \n",
      "Iteration: 2785. Loss: 1.0492732524871826 \n",
      "Iteration: 2786. Loss: 1.0242695808410645 \n",
      "Iteration: 2787. Loss: 1.0503593683242798 \n",
      "Iteration: 2788. Loss: 1.0274386405944824 \n",
      "Iteration: 2789. Loss: 1.0122699737548828 \n",
      "Iteration: 2790. Loss: 0.9801120162010193 \n",
      "Iteration: 2791. Loss: 0.9973229765892029 \n",
      "Iteration: 2792. Loss: 1.0148552656173706 \n",
      "Iteration: 2793. Loss: 1.0305089950561523 \n",
      "Iteration: 2794. Loss: 0.9573051929473877 \n",
      "Iteration: 2795. Loss: 0.9965264797210693 \n",
      "Iteration: 2796. Loss: 1.185044288635254 \n",
      "Iteration: 2797. Loss: 1.0924534797668457 \n",
      "Iteration: 2798. Loss: 1.0309605598449707 \n",
      "Iteration: 2799. Loss: 1.0962563753128052 \n",
      "Iteration: 2800. Loss: 1.0535355806350708 \n",
      "Iteration: 2801. Loss: 1.134099006652832 \n",
      "Iteration: 2802. Loss: 1.0021560192108154 \n",
      "Iteration: 2803. Loss: 1.0953335762023926 \n",
      "Iteration: 2804. Loss: 1.0580819845199585 \n",
      "Iteration: 2805. Loss: 1.065872311592102 \n",
      "Iteration: 2806. Loss: 1.0309163331985474 \n",
      "Iteration: 2807. Loss: 1.1435472965240479 \n",
      "Iteration: 2808. Loss: 1.0060968399047852 \n",
      "Iteration: 2809. Loss: 1.068203330039978 \n",
      "Iteration: 2810. Loss: 1.020293951034546 \n",
      "Iteration: 2811. Loss: 0.9653293490409851 \n",
      "Iteration: 2812. Loss: 1.0116868019104004 \n",
      "Iteration: 2813. Loss: 0.9838411211967468 \n",
      "Iteration: 2814. Loss: 0.9878000020980835 \n",
      "Iteration: 2815. Loss: 0.9986199736595154 \n",
      "Iteration: 2816. Loss: 0.9615568518638611 \n",
      "Iteration: 2817. Loss: 0.9816914200782776 \n",
      "Iteration: 2818. Loss: 0.9481543898582458 \n",
      "Iteration: 2819. Loss: 1.0598134994506836 \n",
      "Iteration: 2820. Loss: 1.1211868524551392 \n",
      "Iteration: 2821. Loss: 1.1124041080474854 \n",
      "Iteration: 2822. Loss: 1.1117244958877563 \n",
      "Iteration: 2823. Loss: 1.0265501737594604 \n",
      "Iteration: 2824. Loss: 1.0219407081604004 \n",
      "Iteration: 2825. Loss: 1.1106066703796387 \n",
      "Iteration: 2826. Loss: 1.0093365907669067 \n",
      "Iteration: 2827. Loss: 0.9841049909591675 \n",
      "Iteration: 2828. Loss: 1.0145299434661865 \n",
      "Iteration: 2829. Loss: 0.9946504831314087 \n",
      "Iteration: 2830. Loss: 1.086596965789795 \n",
      "Iteration: 2831. Loss: 0.9924392104148865 \n",
      "Iteration: 2832. Loss: 0.9123404622077942 \n",
      "Iteration: 2833. Loss: 1.0287790298461914 \n",
      "Iteration: 2834. Loss: 1.1321642398834229 \n",
      "Iteration: 2835. Loss: 0.9201550483703613 \n",
      "Iteration: 2836. Loss: 0.9460076689720154 \n",
      "Iteration: 2837. Loss: 1.0205146074295044 \n",
      "Iteration: 2838. Loss: 1.0154168605804443 \n",
      "Iteration: 2839. Loss: 0.9470188617706299 \n",
      "Iteration: 2840. Loss: 1.0657678842544556 \n",
      "Iteration: 2841. Loss: 0.993663489818573 \n",
      "Iteration: 2842. Loss: 0.9790596961975098 \n",
      "Iteration: 2843. Loss: 0.9722170233726501 \n",
      "Iteration: 2844. Loss: 0.9944747686386108 \n",
      "Iteration: 2845. Loss: 0.9560271501541138 \n",
      "Iteration: 2846. Loss: 1.0203771591186523 \n",
      "Iteration: 2847. Loss: 1.0301858186721802 \n",
      "Iteration: 2848. Loss: 1.0308809280395508 \n",
      "Iteration: 2849. Loss: 1.0765413045883179 \n",
      "Iteration: 2850. Loss: 1.0838497877120972 \n",
      "Iteration: 2851. Loss: 0.9852962493896484 \n",
      "Iteration: 2852. Loss: 1.0309041738510132 \n",
      "Iteration: 2853. Loss: 1.0366132259368896 \n",
      "Iteration: 2854. Loss: 1.0482466220855713 \n",
      "Iteration: 2855. Loss: 1.0528857707977295 \n",
      "Iteration: 2856. Loss: 1.081946849822998 \n",
      "Iteration: 2857. Loss: 1.1536073684692383 \n",
      "Iteration: 2858. Loss: 0.892529308795929 \n",
      "Iteration: 2859. Loss: 1.1059298515319824 \n",
      "Iteration: 2860. Loss: 0.9921131730079651 \n",
      "Iteration: 2861. Loss: 0.9952598810195923 \n",
      "Iteration: 2862. Loss: 0.9585618376731873 \n",
      "Iteration: 2863. Loss: 1.1278027296066284 \n",
      "Iteration: 2864. Loss: 1.0377439260482788 \n",
      "Iteration: 2865. Loss: 0.9452572464942932 \n",
      "Iteration: 2866. Loss: 1.098467469215393 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2867. Loss: 1.018535852432251 \n",
      "Iteration: 2868. Loss: 0.9768741726875305 \n",
      "Iteration: 2869. Loss: 0.9953187704086304 \n",
      "Iteration: 2870. Loss: 1.0744074583053589 \n",
      "Iteration: 2871. Loss: 0.9982537031173706 \n",
      "Iteration: 2872. Loss: 1.1182847023010254 \n",
      "Iteration: 2873. Loss: 0.929505467414856 \n",
      "Iteration: 2874. Loss: 1.0191559791564941 \n",
      "Iteration: 2875. Loss: 1.0615346431732178 \n",
      "Iteration: 2876. Loss: 1.0139923095703125 \n",
      "Iteration: 2877. Loss: 0.9454027414321899 \n",
      "Iteration: 2878. Loss: 1.019714117050171 \n",
      "Iteration: 2879. Loss: 1.0519076585769653 \n",
      "Iteration: 2880. Loss: 1.0389844179153442 \n",
      "Iteration: 2881. Loss: 1.0430972576141357 \n",
      "Iteration: 2882. Loss: 1.0250087976455688 \n",
      "Iteration: 2883. Loss: 1.0901639461517334 \n",
      "Iteration: 2884. Loss: 1.079471230506897 \n",
      "Iteration: 2885. Loss: 1.0302051305770874 \n",
      "Iteration: 2886. Loss: 1.0721583366394043 \n",
      "Iteration: 2887. Loss: 1.0419774055480957 \n",
      "Iteration: 2888. Loss: 1.0200164318084717 \n",
      "Iteration: 2889. Loss: 1.0435616970062256 \n",
      "Iteration: 2890. Loss: 1.0621052980422974 \n",
      "Iteration: 2891. Loss: 0.9730761647224426 \n",
      "Iteration: 2892. Loss: 0.947634220123291 \n",
      "Iteration: 2893. Loss: 0.9715835452079773 \n",
      "Iteration: 2894. Loss: 0.9425528049468994 \n",
      "Iteration: 2895. Loss: 1.0449326038360596 \n",
      "Iteration: 2896. Loss: 0.908113420009613 \n",
      "Iteration: 2897. Loss: 1.0238714218139648 \n",
      "Iteration: 2898. Loss: 1.030397653579712 \n",
      "Iteration: 2899. Loss: 1.0322424173355103 \n",
      "Iteration: 2900. Loss: 0.9765949845314026 \n",
      "Iteration: 2901. Loss: 0.967505693435669 \n",
      "Iteration: 2902. Loss: 1.0092711448669434 \n",
      "Iteration: 2903. Loss: 1.000270128250122 \n",
      "Iteration: 2904. Loss: 0.9535550475120544 \n",
      "Iteration: 2905. Loss: 1.0219244956970215 \n",
      "Iteration: 2906. Loss: 0.9863961935043335 \n",
      "Iteration: 2907. Loss: 1.071281909942627 \n",
      "Iteration: 2908. Loss: 1.0218664407730103 \n",
      "Iteration: 2909. Loss: 1.026428461074829 \n",
      "Iteration: 2910. Loss: 1.045807957649231 \n",
      "Iteration: 2911. Loss: 1.0647475719451904 \n",
      "Iteration: 2912. Loss: 1.116977572441101 \n",
      "Iteration: 2913. Loss: 1.0898597240447998 \n",
      "Iteration: 2914. Loss: 0.9523864984512329 \n",
      "Iteration: 2915. Loss: 0.9575207233428955 \n",
      "Iteration: 2916. Loss: 0.972929835319519 \n",
      "Iteration: 2917. Loss: 1.0517524480819702 \n",
      "Iteration: 2918. Loss: 1.0748370885849 \n",
      "Iteration: 2919. Loss: 1.0798048973083496 \n",
      "Iteration: 2920. Loss: 1.0330158472061157 \n",
      "Iteration: 2921. Loss: 1.0788432359695435 \n",
      "Iteration: 2922. Loss: 1.0119973421096802 \n",
      "Iteration: 2923. Loss: 1.041731595993042 \n",
      "Iteration: 2924. Loss: 1.1026724576950073 \n",
      "Iteration: 2925. Loss: 1.1165467500686646 \n",
      "Iteration: 2926. Loss: 0.9353672862052917 \n",
      "Iteration: 2927. Loss: 0.9553834795951843 \n",
      "Iteration: 2928. Loss: 1.0104719400405884 \n",
      "Iteration: 2929. Loss: 0.9985939264297485 \n",
      "Iteration: 2930. Loss: 0.9475954174995422 \n",
      "Iteration: 2931. Loss: 0.9449112415313721 \n",
      "Iteration: 2932. Loss: 1.0783305168151855 \n",
      "Iteration: 2933. Loss: 1.0533037185668945 \n",
      "Iteration: 2934. Loss: 1.0064986944198608 \n",
      "Iteration: 2935. Loss: 1.1115590333938599 \n",
      "Iteration: 2936. Loss: 0.9955299496650696 \n",
      "Iteration: 2937. Loss: 1.0101090669631958 \n",
      "Iteration: 2938. Loss: 1.0293325185775757 \n",
      "Iteration: 2939. Loss: 0.9531429409980774 \n",
      "Iteration: 2940. Loss: 0.9454052448272705 \n",
      "Iteration: 2941. Loss: 0.977591872215271 \n",
      "Iteration: 2942. Loss: 1.058822751045227 \n",
      "Iteration: 2943. Loss: 1.0780668258666992 \n",
      "Iteration: 2944. Loss: 0.9745650291442871 \n",
      "Iteration: 2945. Loss: 1.0471373796463013 \n",
      "Iteration: 2946. Loss: 1.0393048524856567 \n",
      "Iteration: 2947. Loss: 1.018599271774292 \n",
      "Iteration: 2948. Loss: 1.120647668838501 \n",
      "Iteration: 2949. Loss: 1.0398313999176025 \n",
      "Iteration: 2950. Loss: 1.0984506607055664 \n",
      "Iteration: 2951. Loss: 0.949794590473175 \n",
      "Iteration: 2952. Loss: 0.9776561856269836 \n",
      "Iteration: 2953. Loss: 1.0753213167190552 \n",
      "Iteration: 2954. Loss: 1.025678277015686 \n",
      "Iteration: 2955. Loss: 1.1395165920257568 \n",
      "Iteration: 2956. Loss: 1.070671558380127 \n",
      "Iteration: 2957. Loss: 1.026742935180664 \n",
      "Iteration: 2958. Loss: 1.0173087120056152 \n",
      "Iteration: 2959. Loss: 1.0836681127548218 \n",
      "Iteration: 2960. Loss: 0.9859209656715393 \n",
      "Iteration: 2961. Loss: 1.0506806373596191 \n",
      "Iteration: 2962. Loss: 0.9106494188308716 \n",
      "Iteration: 2963. Loss: 1.0557544231414795 \n",
      "Iteration: 2964. Loss: 1.1101043224334717 \n",
      "Iteration: 2965. Loss: 1.1244239807128906 \n",
      "Iteration: 2966. Loss: 0.986274003982544 \n",
      "Iteration: 2967. Loss: 0.992400050163269 \n",
      "Iteration: 2968. Loss: 0.9401621222496033 \n",
      "Iteration: 2969. Loss: 0.8994058966636658 \n",
      "Iteration: 2970. Loss: 1.0749624967575073 \n",
      "Iteration: 2971. Loss: 1.0291827917099 \n",
      "Iteration: 2972. Loss: 0.9797780513763428 \n",
      "Iteration: 2973. Loss: 1.0694782733917236 \n",
      "Iteration: 2974. Loss: 1.0288856029510498 \n",
      "Iteration: 2975. Loss: 1.0112544298171997 \n",
      "Iteration: 2976. Loss: 0.9271937012672424 \n",
      "Iteration: 2977. Loss: 0.9234910011291504 \n",
      "Iteration: 2978. Loss: 0.9328386187553406 \n",
      "Iteration: 2979. Loss: 1.0057984590530396 \n",
      "Iteration: 2980. Loss: 0.9801619052886963 \n",
      "Iteration: 2981. Loss: 1.0919994115829468 \n",
      "Iteration: 2982. Loss: 1.0580769777297974 \n",
      "Iteration: 2983. Loss: 1.0022567510604858 \n",
      "Iteration: 2984. Loss: 1.1076092720031738 \n",
      "Iteration: 2985. Loss: 0.9405989646911621 \n",
      "Iteration: 2986. Loss: 0.9906747341156006 \n",
      "Iteration: 2987. Loss: 0.9280948638916016 \n",
      "Iteration: 2988. Loss: 0.926435112953186 \n",
      "Iteration: 2989. Loss: 0.9452592730522156 \n",
      "Iteration: 2990. Loss: 1.0082311630249023 \n",
      "Iteration: 2991. Loss: 0.9464024901390076 \n",
      "Iteration: 2992. Loss: 1.0488320589065552 \n",
      "Iteration: 2993. Loss: 0.9849180579185486 \n",
      "Iteration: 2994. Loss: 0.933131992816925 \n",
      "Iteration: 2995. Loss: 1.0103423595428467 \n",
      "Iteration: 2996. Loss: 0.9283572435379028 \n",
      "Iteration: 2997. Loss: 1.0303237438201904 \n",
      "Iteration: 2998. Loss: 1.0905207395553589 \n",
      "Iteration: 2999. Loss: 0.9644833207130432 \n",
      "Iteration: 3000. Loss: 1.0300294160842896 \n"
     ]
    }
   ],
   "source": [
    "iter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        \n",
    "    \n",
    "        images = Variable(images.view(-1, 28*28))\n",
    "        labels = Variable(labels)\n",
    "        \n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass to get output/logits\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        iter += 1\n",
    "        \n",
    "        print('Iteration: {}. Loss: {} '.format(iter, loss.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%Iteration: 3000. Loss: 1.0300294160842896. Accuracy: 82.63\n"
     ]
    }
   ],
   "source": [
    "if iter %500 == 0:\n",
    "            # Calculate Accuracy         \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through test dataset\n",
    "            for images, labels in test_loader:\n",
    "               \n",
    "                images = Variable(images.view(-1, 28*28))\n",
    "                \n",
    "                # Forward pass only to get logits/output\n",
    "                outputs = model(images)\n",
    "                \n",
    "                # Get predictions from the maximum value\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                \n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "                \n",
    "                correct += (predicted.cpu() == labels.cpu()).sum()\n",
    "            \n",
    "            accuracy = 100 * correct / total\n",
    "        \n",
    "            # Print Loss\n",
    "            print('%Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.data[0], accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
