{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dataset = dsets.MNIST(root='./data', \n",
    "                            train=True, \n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='./data', \n",
    "                           train=False, \n",
    "                           transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "n_iters = 3000\n",
    "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a model class \n",
    "# input->linear function->non linear function(sigmoid)->linear function->Softmax->CrossEntropy\n",
    "class FeedFowardNeuralNetwork(nn.Module): \n",
    "    def __init__(self,input_dim,hidden_dim,output_dim):\n",
    "        super(FeedFowardNeuralNetwork,self).__init__()\n",
    "        self.fc1=nn.Linear(input_dim,hidden_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.fc2=nn.Linear(hidden_dim,output_dim)\n",
    "        \n",
    "    def forward(self,x): \n",
    "        out=self.fc1(x)\n",
    "        out=self.sigmoid(out)\n",
    "        out=self.fc2(out)\n",
    "        return out \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the model \n",
    "input_dim=28*28 \n",
    "hidden_dim=100\n",
    "output_dim=10 \n",
    "model=FeedFowardNeuralNetwork(input_dim,hidden_dim,output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# instantiate the loss \n",
    "criterion=nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instatiate the optimize \n",
    "learning_rate=0.1\n",
    "optimizer=torch.optim.SGD(model.parameters(),lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "torch.Size([100, 784])\n",
      "torch.Size([100])\n",
      "torch.Size([10, 100])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "print(len(list(model.parameters())))\n",
    "print(list(model.parameters())[0].size())\n",
    "print(list(model.parameters())[1].size())\n",
    "print(list(model.parameters())[2].size())\n",
    "print(list(model.parameters())[3].size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1. Loss: 2.3307626247406006\n",
      "Iteration: 2. Loss: 2.335205078125\n",
      "Iteration: 3. Loss: 2.3012731075286865\n",
      "Iteration: 4. Loss: 2.3016693592071533\n",
      "Iteration: 5. Loss: 2.3026785850524902\n",
      "Iteration: 6. Loss: 2.2862207889556885\n",
      "Iteration: 7. Loss: 2.2832369804382324\n",
      "Iteration: 8. Loss: 2.26493239402771\n",
      "Iteration: 9. Loss: 2.3040342330932617\n",
      "Iteration: 10. Loss: 2.2666122913360596\n",
      "Iteration: 11. Loss: 2.2747910022735596\n",
      "Iteration: 12. Loss: 2.2995901107788086\n",
      "Iteration: 13. Loss: 2.2954704761505127\n",
      "Iteration: 14. Loss: 2.272814989089966\n",
      "Iteration: 15. Loss: 2.294840097427368\n",
      "Iteration: 16. Loss: 2.270069122314453\n",
      "Iteration: 17. Loss: 2.271491527557373\n",
      "Iteration: 18. Loss: 2.244065284729004\n",
      "Iteration: 19. Loss: 2.274722099304199\n",
      "Iteration: 20. Loss: 2.2521162033081055\n",
      "Iteration: 21. Loss: 2.2776525020599365\n",
      "Iteration: 22. Loss: 2.237713098526001\n",
      "Iteration: 23. Loss: 2.234771966934204\n",
      "Iteration: 24. Loss: 2.2693986892700195\n",
      "Iteration: 25. Loss: 2.2691965103149414\n",
      "Iteration: 26. Loss: 2.2357935905456543\n",
      "Iteration: 27. Loss: 2.247850179672241\n",
      "Iteration: 28. Loss: 2.232212543487549\n",
      "Iteration: 29. Loss: 2.2242658138275146\n",
      "Iteration: 30. Loss: 2.213124990463257\n",
      "Iteration: 31. Loss: 2.21734356880188\n",
      "Iteration: 32. Loss: 2.1992082595825195\n",
      "Iteration: 33. Loss: 2.2198472023010254\n",
      "Iteration: 34. Loss: 2.2176434993743896\n",
      "Iteration: 35. Loss: 2.2236881256103516\n",
      "Iteration: 36. Loss: 2.2195496559143066\n",
      "Iteration: 37. Loss: 2.231551170349121\n",
      "Iteration: 38. Loss: 2.190432548522949\n",
      "Iteration: 39. Loss: 2.183445930480957\n",
      "Iteration: 40. Loss: 2.1785786151885986\n",
      "Iteration: 41. Loss: 2.1750290393829346\n",
      "Iteration: 42. Loss: 2.2211034297943115\n",
      "Iteration: 43. Loss: 2.183112144470215\n",
      "Iteration: 44. Loss: 2.1663777828216553\n",
      "Iteration: 45. Loss: 2.1952064037323\n",
      "Iteration: 46. Loss: 2.2022266387939453\n",
      "Iteration: 47. Loss: 2.1845765113830566\n",
      "Iteration: 48. Loss: 2.162855625152588\n",
      "Iteration: 49. Loss: 2.1692588329315186\n",
      "Iteration: 50. Loss: 2.180945873260498\n",
      "Iteration: 51. Loss: 2.149019241333008\n",
      "Iteration: 52. Loss: 2.1754300594329834\n",
      "Iteration: 53. Loss: 2.165755271911621\n",
      "Iteration: 54. Loss: 2.1572906970977783\n",
      "Iteration: 55. Loss: 2.1456568241119385\n",
      "Iteration: 56. Loss: 2.153543472290039\n",
      "Iteration: 57. Loss: 2.124579906463623\n",
      "Iteration: 58. Loss: 2.138775110244751\n",
      "Iteration: 59. Loss: 2.116079330444336\n",
      "Iteration: 60. Loss: 2.125885009765625\n",
      "Iteration: 61. Loss: 2.1221518516540527\n",
      "Iteration: 62. Loss: 2.152348279953003\n",
      "Iteration: 63. Loss: 2.09948992729187\n",
      "Iteration: 64. Loss: 2.0745198726654053\n",
      "Iteration: 65. Loss: 2.1094226837158203\n",
      "Iteration: 66. Loss: 2.110710859298706\n",
      "Iteration: 67. Loss: 2.112678289413452\n",
      "Iteration: 68. Loss: 2.091017246246338\n",
      "Iteration: 69. Loss: 2.0696091651916504\n",
      "Iteration: 70. Loss: 2.0927720069885254\n",
      "Iteration: 71. Loss: 2.1105921268463135\n",
      "Iteration: 72. Loss: 2.0661418437957764\n",
      "Iteration: 73. Loss: 2.0656816959381104\n",
      "Iteration: 74. Loss: 2.0686538219451904\n",
      "Iteration: 75. Loss: 2.0776431560516357\n",
      "Iteration: 76. Loss: 2.0619699954986572\n",
      "Iteration: 77. Loss: 2.0168235301971436\n",
      "Iteration: 78. Loss: 2.0561509132385254\n",
      "Iteration: 79. Loss: 2.04421329498291\n",
      "Iteration: 80. Loss: 2.0288448333740234\n",
      "Iteration: 81. Loss: 2.009394407272339\n",
      "Iteration: 82. Loss: 2.0357446670532227\n",
      "Iteration: 83. Loss: 2.037379026412964\n",
      "Iteration: 84. Loss: 2.040689706802368\n",
      "Iteration: 85. Loss: 2.0008301734924316\n",
      "Iteration: 86. Loss: 2.0096943378448486\n",
      "Iteration: 87. Loss: 2.0201618671417236\n",
      "Iteration: 88. Loss: 1.959489345550537\n",
      "Iteration: 89. Loss: 2.008113145828247\n",
      "Iteration: 90. Loss: 1.9413951635360718\n",
      "Iteration: 91. Loss: 1.964521050453186\n",
      "Iteration: 92. Loss: 1.9402586221694946\n",
      "Iteration: 93. Loss: 1.9880859851837158\n",
      "Iteration: 94. Loss: 1.9487899541854858\n",
      "Iteration: 95. Loss: 1.9982914924621582\n",
      "Iteration: 96. Loss: 1.9489742517471313\n",
      "Iteration: 97. Loss: 1.9361824989318848\n",
      "Iteration: 98. Loss: 1.950899362564087\n",
      "Iteration: 99. Loss: 1.9257971048355103\n",
      "Iteration: 100. Loss: 1.884050726890564\n",
      "Iteration: 101. Loss: 1.930893898010254\n",
      "Iteration: 102. Loss: 1.8853496313095093\n",
      "Iteration: 103. Loss: 1.9202107191085815\n",
      "Iteration: 104. Loss: 1.863209843635559\n",
      "Iteration: 105. Loss: 1.8886659145355225\n",
      "Iteration: 106. Loss: 1.8817262649536133\n",
      "Iteration: 107. Loss: 1.8504770994186401\n",
      "Iteration: 108. Loss: 1.8617538213729858\n",
      "Iteration: 109. Loss: 1.8135071992874146\n",
      "Iteration: 110. Loss: 1.9051862955093384\n",
      "Iteration: 111. Loss: 1.8816035985946655\n",
      "Iteration: 112. Loss: 1.8226433992385864\n",
      "Iteration: 113. Loss: 1.8728022575378418\n",
      "Iteration: 114. Loss: 1.8221709728240967\n",
      "Iteration: 115. Loss: 1.8451610803604126\n",
      "Iteration: 116. Loss: 1.8794136047363281\n",
      "Iteration: 117. Loss: 1.824375867843628\n",
      "Iteration: 118. Loss: 1.7624930143356323\n",
      "Iteration: 119. Loss: 1.8329066038131714\n",
      "Iteration: 120. Loss: 1.8034543991088867\n",
      "Iteration: 121. Loss: 1.7742245197296143\n",
      "Iteration: 122. Loss: 1.803623080253601\n",
      "Iteration: 123. Loss: 1.7677539587020874\n",
      "Iteration: 124. Loss: 1.7294459342956543\n",
      "Iteration: 125. Loss: 1.7759501934051514\n",
      "Iteration: 126. Loss: 1.7775371074676514\n",
      "Iteration: 127. Loss: 1.7889738082885742\n",
      "Iteration: 128. Loss: 1.7755695581436157\n",
      "Iteration: 129. Loss: 1.7458136081695557\n",
      "Iteration: 130. Loss: 1.7340855598449707\n",
      "Iteration: 131. Loss: 1.693190097808838\n",
      "Iteration: 132. Loss: 1.6779241561889648\n",
      "Iteration: 133. Loss: 1.6627711057662964\n",
      "Iteration: 134. Loss: 1.7373236417770386\n",
      "Iteration: 135. Loss: 1.6996469497680664\n",
      "Iteration: 136. Loss: 1.7284470796585083\n",
      "Iteration: 137. Loss: 1.6174014806747437\n",
      "Iteration: 138. Loss: 1.6151134967803955\n",
      "Iteration: 139. Loss: 1.6558539867401123\n",
      "Iteration: 140. Loss: 1.6807599067687988\n",
      "Iteration: 141. Loss: 1.6041014194488525\n",
      "Iteration: 142. Loss: 1.6423448324203491\n",
      "Iteration: 143. Loss: 1.6872949600219727\n",
      "Iteration: 144. Loss: 1.6099073886871338\n",
      "Iteration: 145. Loss: 1.6068675518035889\n",
      "Iteration: 146. Loss: 1.5651828050613403\n",
      "Iteration: 147. Loss: 1.5964529514312744\n",
      "Iteration: 148. Loss: 1.5827571153640747\n",
      "Iteration: 149. Loss: 1.584977388381958\n",
      "Iteration: 150. Loss: 1.6136916875839233\n",
      "Iteration: 151. Loss: 1.5658307075500488\n",
      "Iteration: 152. Loss: 1.5755298137664795\n",
      "Iteration: 153. Loss: 1.6102925539016724\n",
      "Iteration: 154. Loss: 1.4588810205459595\n",
      "Iteration: 155. Loss: 1.5881565809249878\n",
      "Iteration: 156. Loss: 1.6127738952636719\n",
      "Iteration: 157. Loss: 1.5780508518218994\n",
      "Iteration: 158. Loss: 1.6245276927947998\n",
      "Iteration: 159. Loss: 1.5119491815567017\n",
      "Iteration: 160. Loss: 1.4662128686904907\n",
      "Iteration: 161. Loss: 1.4675825834274292\n",
      "Iteration: 162. Loss: 1.5179378986358643\n",
      "Iteration: 163. Loss: 1.4500421285629272\n",
      "Iteration: 164. Loss: 1.5046439170837402\n",
      "Iteration: 165. Loss: 1.486659288406372\n",
      "Iteration: 166. Loss: 1.5706241130828857\n",
      "Iteration: 167. Loss: 1.4977880716323853\n",
      "Iteration: 168. Loss: 1.4809669256210327\n",
      "Iteration: 169. Loss: 1.439301609992981\n",
      "Iteration: 170. Loss: 1.4274836778640747\n",
      "Iteration: 171. Loss: 1.4983513355255127\n",
      "Iteration: 172. Loss: 1.437408447265625\n",
      "Iteration: 173. Loss: 1.3736228942871094\n",
      "Iteration: 174. Loss: 1.4665136337280273\n",
      "Iteration: 175. Loss: 1.4492506980895996\n",
      "Iteration: 176. Loss: 1.4300868511199951\n",
      "Iteration: 177. Loss: 1.4563237428665161\n",
      "Iteration: 178. Loss: 1.4957754611968994\n",
      "Iteration: 179. Loss: 1.4069852828979492\n",
      "Iteration: 180. Loss: 1.3872920274734497\n",
      "Iteration: 181. Loss: 1.3744083642959595\n",
      "Iteration: 182. Loss: 1.4100914001464844\n",
      "Iteration: 183. Loss: 1.3092516660690308\n",
      "Iteration: 184. Loss: 1.3980646133422852\n",
      "Iteration: 185. Loss: 1.3977432250976562\n",
      "Iteration: 186. Loss: 1.3243709802627563\n",
      "Iteration: 187. Loss: 1.3591388463974\n",
      "Iteration: 188. Loss: 1.3253943920135498\n",
      "Iteration: 189. Loss: 1.329397439956665\n",
      "Iteration: 190. Loss: 1.371683955192566\n",
      "Iteration: 191. Loss: 1.214072823524475\n",
      "Iteration: 192. Loss: 1.3533456325531006\n",
      "Iteration: 193. Loss: 1.3571457862854004\n",
      "Iteration: 194. Loss: 1.2669485807418823\n",
      "Iteration: 195. Loss: 1.3503156900405884\n",
      "Iteration: 196. Loss: 1.2296737432479858\n",
      "Iteration: 197. Loss: 1.3011198043823242\n",
      "Iteration: 198. Loss: 1.2583343982696533\n",
      "Iteration: 199. Loss: 1.2569957971572876\n",
      "Iteration: 200. Loss: 1.3561021089553833\n",
      "Iteration: 201. Loss: 1.1743162870407104\n",
      "Iteration: 202. Loss: 1.2512495517730713\n",
      "Iteration: 203. Loss: 1.214897871017456\n",
      "Iteration: 204. Loss: 1.2966362237930298\n",
      "Iteration: 205. Loss: 1.2813035249710083\n",
      "Iteration: 206. Loss: 1.2122610807418823\n",
      "Iteration: 207. Loss: 1.2840065956115723\n",
      "Iteration: 208. Loss: 1.2675212621688843\n",
      "Iteration: 209. Loss: 1.2460933923721313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 210. Loss: 1.20193612575531\n",
      "Iteration: 211. Loss: 1.203674554824829\n",
      "Iteration: 212. Loss: 1.2569990158081055\n",
      "Iteration: 213. Loss: 1.099810242652893\n",
      "Iteration: 214. Loss: 1.2523466348648071\n",
      "Iteration: 215. Loss: 1.221348524093628\n",
      "Iteration: 216. Loss: 1.1950064897537231\n",
      "Iteration: 217. Loss: 1.1988598108291626\n",
      "Iteration: 218. Loss: 1.2570981979370117\n",
      "Iteration: 219. Loss: 1.1025618314743042\n",
      "Iteration: 220. Loss: 1.2380335330963135\n",
      "Iteration: 221. Loss: 1.184187412261963\n",
      "Iteration: 222. Loss: 1.2080261707305908\n",
      "Iteration: 223. Loss: 1.1744797229766846\n",
      "Iteration: 224. Loss: 1.1390950679779053\n",
      "Iteration: 225. Loss: 1.1416269540786743\n",
      "Iteration: 226. Loss: 1.1592235565185547\n",
      "Iteration: 227. Loss: 1.1263138055801392\n",
      "Iteration: 228. Loss: 1.0963387489318848\n",
      "Iteration: 229. Loss: 1.095086932182312\n",
      "Iteration: 230. Loss: 1.0711536407470703\n",
      "Iteration: 231. Loss: 1.1585756540298462\n",
      "Iteration: 232. Loss: 1.1927951574325562\n",
      "Iteration: 233. Loss: 1.170780062675476\n",
      "Iteration: 234. Loss: 0.9732439517974854\n",
      "Iteration: 235. Loss: 1.0737282037734985\n",
      "Iteration: 236. Loss: 1.1500968933105469\n",
      "Iteration: 237. Loss: 1.1519362926483154\n",
      "Iteration: 238. Loss: 1.017788052558899\n",
      "Iteration: 239. Loss: 1.0928841829299927\n",
      "Iteration: 240. Loss: 1.0663440227508545\n",
      "Iteration: 241. Loss: 1.069533348083496\n",
      "Iteration: 242. Loss: 1.0485296249389648\n",
      "Iteration: 243. Loss: 1.128028154373169\n",
      "Iteration: 244. Loss: 1.0218394994735718\n",
      "Iteration: 245. Loss: 1.124182939529419\n",
      "Iteration: 246. Loss: 1.1123179197311401\n",
      "Iteration: 247. Loss: 1.023638367652893\n",
      "Iteration: 248. Loss: 1.0276697874069214\n",
      "Iteration: 249. Loss: 1.0596647262573242\n",
      "Iteration: 250. Loss: 1.09852933883667\n",
      "Iteration: 251. Loss: 1.0606486797332764\n",
      "Iteration: 252. Loss: 1.1102443933486938\n",
      "Iteration: 253. Loss: 1.06892728805542\n",
      "Iteration: 254. Loss: 1.0174968242645264\n",
      "Iteration: 255. Loss: 1.0105633735656738\n",
      "Iteration: 256. Loss: 1.0338592529296875\n",
      "Iteration: 257. Loss: 1.0285344123840332\n",
      "Iteration: 258. Loss: 1.1685707569122314\n",
      "Iteration: 259. Loss: 1.034087061882019\n",
      "Iteration: 260. Loss: 0.9130884408950806\n",
      "Iteration: 261. Loss: 1.0186964273452759\n",
      "Iteration: 262. Loss: 1.095240831375122\n",
      "Iteration: 263. Loss: 0.9863824248313904\n",
      "Iteration: 264. Loss: 1.054462194442749\n",
      "Iteration: 265. Loss: 0.9286967515945435\n",
      "Iteration: 266. Loss: 0.9434654712677002\n",
      "Iteration: 267. Loss: 1.0001342296600342\n",
      "Iteration: 268. Loss: 1.119221806526184\n",
      "Iteration: 269. Loss: 0.953413188457489\n",
      "Iteration: 270. Loss: 0.9911326766014099\n",
      "Iteration: 271. Loss: 0.9814172983169556\n",
      "Iteration: 272. Loss: 0.9231041073799133\n",
      "Iteration: 273. Loss: 0.9771860241889954\n",
      "Iteration: 274. Loss: 1.0929447412490845\n",
      "Iteration: 275. Loss: 1.0467334985733032\n",
      "Iteration: 276. Loss: 0.9643250107765198\n",
      "Iteration: 277. Loss: 0.9905717372894287\n",
      "Iteration: 278. Loss: 0.9404100775718689\n",
      "Iteration: 279. Loss: 1.0005425214767456\n",
      "Iteration: 280. Loss: 0.9686769247055054\n",
      "Iteration: 281. Loss: 1.0389764308929443\n",
      "Iteration: 282. Loss: 1.0706120729446411\n",
      "Iteration: 283. Loss: 0.9640724658966064\n",
      "Iteration: 284. Loss: 0.9879910945892334\n",
      "Iteration: 285. Loss: 0.9613154530525208\n",
      "Iteration: 286. Loss: 0.9536453485488892\n",
      "Iteration: 287. Loss: 0.9853084683418274\n",
      "Iteration: 288. Loss: 0.8671093583106995\n",
      "Iteration: 289. Loss: 0.9525482654571533\n",
      "Iteration: 290. Loss: 1.0339866876602173\n",
      "Iteration: 291. Loss: 0.8260154128074646\n",
      "Iteration: 292. Loss: 0.8268550038337708\n",
      "Iteration: 293. Loss: 1.0995573997497559\n",
      "Iteration: 294. Loss: 0.9390779733657837\n",
      "Iteration: 295. Loss: 0.9000359177589417\n",
      "Iteration: 296. Loss: 0.8928283452987671\n",
      "Iteration: 297. Loss: 1.012341856956482\n",
      "Iteration: 298. Loss: 0.9300171136856079\n",
      "Iteration: 299. Loss: 1.0098685026168823\n",
      "Iteration: 300. Loss: 0.9006696343421936\n",
      "Iteration: 301. Loss: 0.9401169419288635\n",
      "Iteration: 302. Loss: 0.9167685508728027\n",
      "Iteration: 303. Loss: 0.7992268204689026\n",
      "Iteration: 304. Loss: 0.9133569598197937\n",
      "Iteration: 305. Loss: 0.8680962324142456\n",
      "Iteration: 306. Loss: 0.9161703586578369\n",
      "Iteration: 307. Loss: 0.9881130456924438\n",
      "Iteration: 308. Loss: 0.8886467218399048\n",
      "Iteration: 309. Loss: 0.9020014405250549\n",
      "Iteration: 310. Loss: 0.8782406449317932\n",
      "Iteration: 311. Loss: 0.8397646546363831\n",
      "Iteration: 312. Loss: 0.9512985348701477\n",
      "Iteration: 313. Loss: 0.8065840005874634\n",
      "Iteration: 314. Loss: 0.9371093511581421\n",
      "Iteration: 315. Loss: 0.8124352097511292\n",
      "Iteration: 316. Loss: 0.8501548171043396\n",
      "Iteration: 317. Loss: 0.8359753489494324\n",
      "Iteration: 318. Loss: 0.8508110642433167\n",
      "Iteration: 319. Loss: 0.9263578057289124\n",
      "Iteration: 320. Loss: 0.7581269145011902\n",
      "Iteration: 321. Loss: 0.8747002482414246\n",
      "Iteration: 322. Loss: 0.8840029239654541\n",
      "Iteration: 323. Loss: 0.8475861549377441\n",
      "Iteration: 324. Loss: 0.7879186272621155\n",
      "Iteration: 325. Loss: 0.8825441002845764\n",
      "Iteration: 326. Loss: 0.7589394450187683\n",
      "Iteration: 327. Loss: 0.864276647567749\n",
      "Iteration: 328. Loss: 0.9121253490447998\n",
      "Iteration: 329. Loss: 0.8180226683616638\n",
      "Iteration: 330. Loss: 0.90333092212677\n",
      "Iteration: 331. Loss: 0.9699219465255737\n",
      "Iteration: 332. Loss: 0.8732593059539795\n",
      "Iteration: 333. Loss: 0.8525927662849426\n",
      "Iteration: 334. Loss: 0.9193899035453796\n",
      "Iteration: 335. Loss: 0.8145096302032471\n",
      "Iteration: 336. Loss: 0.8711881041526794\n",
      "Iteration: 337. Loss: 0.8394097089767456\n",
      "Iteration: 338. Loss: 0.8203756213188171\n",
      "Iteration: 339. Loss: 0.8914834856987\n",
      "Iteration: 340. Loss: 0.905698835849762\n",
      "Iteration: 341. Loss: 0.8784380555152893\n",
      "Iteration: 342. Loss: 0.7189822196960449\n",
      "Iteration: 343. Loss: 0.8161203861236572\n",
      "Iteration: 344. Loss: 0.8470808267593384\n",
      "Iteration: 345. Loss: 0.7714866399765015\n",
      "Iteration: 346. Loss: 0.9116793870925903\n",
      "Iteration: 347. Loss: 0.8103169798851013\n",
      "Iteration: 348. Loss: 0.975853681564331\n",
      "Iteration: 349. Loss: 0.7224769592285156\n",
      "Iteration: 350. Loss: 0.8043277859687805\n",
      "Iteration: 351. Loss: 0.7775535583496094\n",
      "Iteration: 352. Loss: 0.8122545480728149\n",
      "Iteration: 353. Loss: 0.7217593193054199\n",
      "Iteration: 354. Loss: 0.8365902900695801\n",
      "Iteration: 355. Loss: 0.7269268035888672\n",
      "Iteration: 356. Loss: 0.77227783203125\n",
      "Iteration: 357. Loss: 0.7207039594650269\n",
      "Iteration: 358. Loss: 0.8235769867897034\n",
      "Iteration: 359. Loss: 0.8751348257064819\n",
      "Iteration: 360. Loss: 0.795741856098175\n",
      "Iteration: 361. Loss: 0.8287445902824402\n",
      "Iteration: 362. Loss: 0.6875007152557373\n",
      "Iteration: 363. Loss: 0.7167873978614807\n",
      "Iteration: 364. Loss: 0.7208592295646667\n",
      "Iteration: 365. Loss: 0.7155691385269165\n",
      "Iteration: 366. Loss: 0.7689465284347534\n",
      "Iteration: 367. Loss: 0.7279825806617737\n",
      "Iteration: 368. Loss: 0.7528335452079773\n",
      "Iteration: 369. Loss: 0.7789726853370667\n",
      "Iteration: 370. Loss: 0.750095009803772\n",
      "Iteration: 371. Loss: 0.732296884059906\n",
      "Iteration: 372. Loss: 0.8413625359535217\n",
      "Iteration: 373. Loss: 0.7308881282806396\n",
      "Iteration: 374. Loss: 0.7255884408950806\n",
      "Iteration: 375. Loss: 0.7367444634437561\n",
      "Iteration: 376. Loss: 0.8186565637588501\n",
      "Iteration: 377. Loss: 0.8118600249290466\n",
      "Iteration: 378. Loss: 0.7237350940704346\n",
      "Iteration: 379. Loss: 0.8219069838523865\n",
      "Iteration: 380. Loss: 0.8748188018798828\n",
      "Iteration: 381. Loss: 0.7294372320175171\n",
      "Iteration: 382. Loss: 0.7039160132408142\n",
      "Iteration: 383. Loss: 0.7929747700691223\n",
      "Iteration: 384. Loss: 0.8704022765159607\n",
      "Iteration: 385. Loss: 0.7282534837722778\n",
      "Iteration: 386. Loss: 0.8481038808822632\n",
      "Iteration: 387. Loss: 0.7981399297714233\n",
      "Iteration: 388. Loss: 0.7463175058364868\n",
      "Iteration: 389. Loss: 0.7215284705162048\n",
      "Iteration: 390. Loss: 0.8164836168289185\n",
      "Iteration: 391. Loss: 0.7171983122825623\n",
      "Iteration: 392. Loss: 0.6287987232208252\n",
      "Iteration: 393. Loss: 0.7052313089370728\n",
      "Iteration: 394. Loss: 0.7569481730461121\n",
      "Iteration: 395. Loss: 0.7247515916824341\n",
      "Iteration: 396. Loss: 0.6554566025733948\n",
      "Iteration: 397. Loss: 0.6777561902999878\n",
      "Iteration: 398. Loss: 0.7598364353179932\n",
      "Iteration: 399. Loss: 0.7481315732002258\n",
      "Iteration: 400. Loss: 0.7553431987762451\n",
      "Iteration: 401. Loss: 0.6903937458992004\n",
      "Iteration: 402. Loss: 0.7118486762046814\n",
      "Iteration: 403. Loss: 0.6404391527175903\n",
      "Iteration: 404. Loss: 0.753261923789978\n",
      "Iteration: 405. Loss: 0.6533175706863403\n",
      "Iteration: 406. Loss: 0.7702971696853638\n",
      "Iteration: 407. Loss: 0.7726571559906006\n",
      "Iteration: 408. Loss: 0.8205705881118774\n",
      "Iteration: 409. Loss: 0.7155786156654358\n",
      "Iteration: 410. Loss: 0.6459714770317078\n",
      "Iteration: 411. Loss: 0.7598363757133484\n",
      "Iteration: 412. Loss: 0.7415323853492737\n",
      "Iteration: 413. Loss: 0.6552169919013977\n",
      "Iteration: 414. Loss: 0.7161560654640198\n",
      "Iteration: 415. Loss: 0.7218474745750427\n",
      "Iteration: 416. Loss: 0.8063288927078247\n",
      "Iteration: 417. Loss: 0.6600286364555359\n",
      "Iteration: 418. Loss: 0.6796594858169556\n",
      "Iteration: 419. Loss: 0.7396131753921509\n",
      "Iteration: 420. Loss: 0.7679404616355896\n",
      "Iteration: 421. Loss: 0.6571012139320374\n",
      "Iteration: 422. Loss: 0.776803195476532\n",
      "Iteration: 423. Loss: 0.7618909478187561\n",
      "Iteration: 424. Loss: 0.8148148059844971\n",
      "Iteration: 425. Loss: 0.7949457764625549\n",
      "Iteration: 426. Loss: 0.5912219285964966\n",
      "Iteration: 427. Loss: 0.6515906453132629\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 428. Loss: 0.7049258947372437\n",
      "Iteration: 429. Loss: 0.7024204134941101\n",
      "Iteration: 430. Loss: 0.6261444091796875\n",
      "Iteration: 431. Loss: 0.6208955645561218\n",
      "Iteration: 432. Loss: 0.6084201335906982\n",
      "Iteration: 433. Loss: 0.5814751982688904\n",
      "Iteration: 434. Loss: 0.5955579280853271\n",
      "Iteration: 435. Loss: 0.6804344654083252\n",
      "Iteration: 436. Loss: 0.6457852721214294\n",
      "Iteration: 437. Loss: 0.6622045636177063\n",
      "Iteration: 438. Loss: 0.6767109632492065\n",
      "Iteration: 439. Loss: 0.7834210991859436\n",
      "Iteration: 440. Loss: 0.6800373196601868\n",
      "Iteration: 441. Loss: 0.6234684586524963\n",
      "Iteration: 442. Loss: 0.6672210693359375\n",
      "Iteration: 443. Loss: 0.7156423330307007\n",
      "Iteration: 444. Loss: 0.7207456231117249\n",
      "Iteration: 445. Loss: 0.6166000962257385\n",
      "Iteration: 446. Loss: 0.6938987970352173\n",
      "Iteration: 447. Loss: 0.6930767893791199\n",
      "Iteration: 448. Loss: 0.6167694330215454\n",
      "Iteration: 449. Loss: 0.5799959897994995\n",
      "Iteration: 450. Loss: 0.6748138666152954\n",
      "Iteration: 451. Loss: 0.45767584443092346\n",
      "Iteration: 452. Loss: 0.5141258835792542\n",
      "Iteration: 453. Loss: 0.5837509036064148\n",
      "Iteration: 454. Loss: 0.686458945274353\n",
      "Iteration: 455. Loss: 0.712639570236206\n",
      "Iteration: 456. Loss: 0.6076158881187439\n",
      "Iteration: 457. Loss: 0.706723690032959\n",
      "Iteration: 458. Loss: 0.6125082969665527\n",
      "Iteration: 459. Loss: 0.7307775020599365\n",
      "Iteration: 460. Loss: 0.7001453638076782\n",
      "Iteration: 461. Loss: 0.765258252620697\n",
      "Iteration: 462. Loss: 0.5934545397758484\n",
      "Iteration: 463. Loss: 0.6257786750793457\n",
      "Iteration: 464. Loss: 0.7254270315170288\n",
      "Iteration: 465. Loss: 0.7574859857559204\n",
      "Iteration: 466. Loss: 0.606240451335907\n",
      "Iteration: 467. Loss: 0.6520059704780579\n",
      "Iteration: 468. Loss: 0.6703117489814758\n",
      "Iteration: 469. Loss: 0.6424227952957153\n",
      "Iteration: 470. Loss: 0.5763952136039734\n",
      "Iteration: 471. Loss: 0.6846335530281067\n",
      "Iteration: 472. Loss: 0.6654855608940125\n",
      "Iteration: 473. Loss: 0.6616816520690918\n",
      "Iteration: 474. Loss: 0.603018581867218\n",
      "Iteration: 475. Loss: 0.6699280738830566\n",
      "Iteration: 476. Loss: 0.6239664554595947\n",
      "Iteration: 477. Loss: 0.6514025330543518\n",
      "Iteration: 478. Loss: 0.5963332056999207\n",
      "Iteration: 479. Loss: 0.6681435108184814\n",
      "Iteration: 480. Loss: 0.7159892916679382\n",
      "Iteration: 481. Loss: 0.6223917007446289\n",
      "Iteration: 482. Loss: 0.5633152723312378\n",
      "Iteration: 483. Loss: 0.6477136015892029\n",
      "Iteration: 484. Loss: 0.6063172817230225\n",
      "Iteration: 485. Loss: 0.5597788691520691\n",
      "Iteration: 486. Loss: 0.6838575005531311\n",
      "Iteration: 487. Loss: 0.6261902451515198\n",
      "Iteration: 488. Loss: 0.6015665531158447\n",
      "Iteration: 489. Loss: 0.5449921488761902\n",
      "Iteration: 490. Loss: 0.7244855761528015\n",
      "Iteration: 491. Loss: 0.5812220573425293\n",
      "Iteration: 492. Loss: 0.6660969257354736\n",
      "Iteration: 493. Loss: 0.7094040513038635\n",
      "Iteration: 494. Loss: 0.570086658000946\n",
      "Iteration: 495. Loss: 0.6742109656333923\n",
      "Iteration: 496. Loss: 0.5540538430213928\n",
      "Iteration: 497. Loss: 0.6046880483627319\n",
      "Iteration: 498. Loss: 0.5320767164230347\n",
      "Iteration: 499. Loss: 0.5634413957595825\n",
      "Iteration: 500. Loss: 0.6945168375968933\n",
      "Iteration: 501. Loss: 0.5811120271682739\n",
      "Iteration: 502. Loss: 0.6837701201438904\n",
      "Iteration: 503. Loss: 0.8058748841285706\n",
      "Iteration: 504. Loss: 0.533069372177124\n",
      "Iteration: 505. Loss: 0.5311933755874634\n",
      "Iteration: 506. Loss: 0.5395534634590149\n",
      "Iteration: 507. Loss: 0.568432092666626\n",
      "Iteration: 508. Loss: 0.4703957736492157\n",
      "Iteration: 509. Loss: 0.573350727558136\n",
      "Iteration: 510. Loss: 0.5987406969070435\n",
      "Iteration: 511. Loss: 0.7271461486816406\n",
      "Iteration: 512. Loss: 0.5535140037536621\n",
      "Iteration: 513. Loss: 0.6954824328422546\n",
      "Iteration: 514. Loss: 0.6227279901504517\n",
      "Iteration: 515. Loss: 0.5658160448074341\n",
      "Iteration: 516. Loss: 0.5362467169761658\n",
      "Iteration: 517. Loss: 0.5472743511199951\n",
      "Iteration: 518. Loss: 0.6113472580909729\n",
      "Iteration: 519. Loss: 0.4849274456501007\n",
      "Iteration: 520. Loss: 0.5658261775970459\n",
      "Iteration: 521. Loss: 0.5490624904632568\n",
      "Iteration: 522. Loss: 0.5760140419006348\n",
      "Iteration: 523. Loss: 0.5034770965576172\n",
      "Iteration: 524. Loss: 0.6333269476890564\n",
      "Iteration: 525. Loss: 0.5104890465736389\n",
      "Iteration: 526. Loss: 0.546455442905426\n",
      "Iteration: 527. Loss: 0.6137831211090088\n",
      "Iteration: 528. Loss: 0.5847750306129456\n",
      "Iteration: 529. Loss: 0.5255361795425415\n",
      "Iteration: 530. Loss: 0.5596696734428406\n",
      "Iteration: 531. Loss: 0.6215704679489136\n",
      "Iteration: 532. Loss: 0.44855278730392456\n",
      "Iteration: 533. Loss: 0.7027316093444824\n",
      "Iteration: 534. Loss: 0.6481165289878845\n",
      "Iteration: 535. Loss: 0.58316969871521\n",
      "Iteration: 536. Loss: 0.5895938277244568\n",
      "Iteration: 537. Loss: 0.6950724124908447\n",
      "Iteration: 538. Loss: 0.5964197516441345\n",
      "Iteration: 539. Loss: 0.6468251943588257\n",
      "Iteration: 540. Loss: 0.5942533016204834\n",
      "Iteration: 541. Loss: 0.692656934261322\n",
      "Iteration: 542. Loss: 0.5140054225921631\n",
      "Iteration: 543. Loss: 0.7479005455970764\n",
      "Iteration: 544. Loss: 0.5431681275367737\n",
      "Iteration: 545. Loss: 0.5507998466491699\n",
      "Iteration: 546. Loss: 0.5609700083732605\n",
      "Iteration: 547. Loss: 0.6727896332740784\n",
      "Iteration: 548. Loss: 0.7151772379875183\n",
      "Iteration: 549. Loss: 0.7184630036354065\n",
      "Iteration: 550. Loss: 0.5745444297790527\n",
      "Iteration: 551. Loss: 0.5949085354804993\n",
      "Iteration: 552. Loss: 0.5484313368797302\n",
      "Iteration: 553. Loss: 0.5973243117332458\n",
      "Iteration: 554. Loss: 0.5907066464424133\n",
      "Iteration: 555. Loss: 0.45542076230049133\n",
      "Iteration: 556. Loss: 0.6594376564025879\n",
      "Iteration: 557. Loss: 0.5260982513427734\n",
      "Iteration: 558. Loss: 0.5260878801345825\n",
      "Iteration: 559. Loss: 0.42705902457237244\n",
      "Iteration: 560. Loss: 0.4072684049606323\n",
      "Iteration: 561. Loss: 0.6052850484848022\n",
      "Iteration: 562. Loss: 0.5556184649467468\n",
      "Iteration: 563. Loss: 0.5311355590820312\n",
      "Iteration: 564. Loss: 0.6552192568778992\n",
      "Iteration: 565. Loss: 0.6656104922294617\n",
      "Iteration: 566. Loss: 0.5671256184577942\n",
      "Iteration: 567. Loss: 0.3835987150669098\n",
      "Iteration: 568. Loss: 0.571088969707489\n",
      "Iteration: 569. Loss: 0.589992105960846\n",
      "Iteration: 570. Loss: 0.562113881111145\n",
      "Iteration: 571. Loss: 0.6472830772399902\n",
      "Iteration: 572. Loss: 0.5548032522201538\n",
      "Iteration: 573. Loss: 0.5969964861869812\n",
      "Iteration: 574. Loss: 0.5774907469749451\n",
      "Iteration: 575. Loss: 0.5342434048652649\n",
      "Iteration: 576. Loss: 0.6256164908409119\n",
      "Iteration: 577. Loss: 0.5450745224952698\n",
      "Iteration: 578. Loss: 0.4851182699203491\n",
      "Iteration: 579. Loss: 0.49768656492233276\n",
      "Iteration: 580. Loss: 0.6486378312110901\n",
      "Iteration: 581. Loss: 0.5965392589569092\n",
      "Iteration: 582. Loss: 0.6100180149078369\n",
      "Iteration: 583. Loss: 0.500316858291626\n",
      "Iteration: 584. Loss: 0.4996883273124695\n",
      "Iteration: 585. Loss: 0.5539037585258484\n",
      "Iteration: 586. Loss: 0.597675085067749\n",
      "Iteration: 587. Loss: 0.4811551570892334\n",
      "Iteration: 588. Loss: 0.6240730881690979\n",
      "Iteration: 589. Loss: 0.46328288316726685\n",
      "Iteration: 590. Loss: 0.5443806648254395\n",
      "Iteration: 591. Loss: 0.6057747006416321\n",
      "Iteration: 592. Loss: 0.680985689163208\n",
      "Iteration: 593. Loss: 0.5208128094673157\n",
      "Iteration: 594. Loss: 0.5102012157440186\n",
      "Iteration: 595. Loss: 0.5855032205581665\n",
      "Iteration: 596. Loss: 0.542786717414856\n",
      "Iteration: 597. Loss: 0.6574850678443909\n",
      "Iteration: 598. Loss: 0.5752374529838562\n",
      "Iteration: 599. Loss: 0.5630691647529602\n",
      "Iteration: 600. Loss: 0.6137630343437195\n",
      "Iteration: 601. Loss: 0.4542396664619446\n",
      "Iteration: 602. Loss: 0.5543301105499268\n",
      "Iteration: 603. Loss: 0.5000424385070801\n",
      "Iteration: 604. Loss: 0.44686874747276306\n",
      "Iteration: 605. Loss: 0.4702445864677429\n",
      "Iteration: 606. Loss: 0.6206896305084229\n",
      "Iteration: 607. Loss: 0.4724162220954895\n",
      "Iteration: 608. Loss: 0.7621342539787292\n",
      "Iteration: 609. Loss: 0.589842677116394\n",
      "Iteration: 610. Loss: 0.5361288189888\n",
      "Iteration: 611. Loss: 0.5908451080322266\n",
      "Iteration: 612. Loss: 0.5947750210762024\n",
      "Iteration: 613. Loss: 0.6293751001358032\n",
      "Iteration: 614. Loss: 0.438704252243042\n",
      "Iteration: 615. Loss: 0.4373454749584198\n",
      "Iteration: 616. Loss: 0.7016908526420593\n",
      "Iteration: 617. Loss: 0.5103322863578796\n",
      "Iteration: 618. Loss: 0.4371764361858368\n",
      "Iteration: 619. Loss: 0.5599026083946228\n",
      "Iteration: 620. Loss: 0.4860450029373169\n",
      "Iteration: 621. Loss: 0.48490363359451294\n",
      "Iteration: 622. Loss: 0.5488513708114624\n",
      "Iteration: 623. Loss: 0.47856971621513367\n",
      "Iteration: 624. Loss: 0.6005098223686218\n",
      "Iteration: 625. Loss: 0.5783509612083435\n",
      "Iteration: 626. Loss: 0.5655258297920227\n",
      "Iteration: 627. Loss: 0.5506362318992615\n",
      "Iteration: 628. Loss: 0.5519707202911377\n",
      "Iteration: 629. Loss: 0.5239382386207581\n",
      "Iteration: 630. Loss: 0.6112822890281677\n",
      "Iteration: 631. Loss: 0.4758191406726837\n",
      "Iteration: 632. Loss: 0.5586960315704346\n",
      "Iteration: 633. Loss: 0.49097079038619995\n",
      "Iteration: 634. Loss: 0.5412151217460632\n",
      "Iteration: 635. Loss: 0.4519905745983124\n",
      "Iteration: 636. Loss: 0.5788622498512268\n",
      "Iteration: 637. Loss: 0.5500862002372742\n",
      "Iteration: 638. Loss: 0.5359220504760742\n",
      "Iteration: 639. Loss: 0.598569393157959\n",
      "Iteration: 640. Loss: 0.5072576403617859\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 641. Loss: 0.5520417094230652\n",
      "Iteration: 642. Loss: 0.46483856439590454\n",
      "Iteration: 643. Loss: 0.4979674220085144\n",
      "Iteration: 644. Loss: 0.6539208889007568\n",
      "Iteration: 645. Loss: 0.6079562902450562\n",
      "Iteration: 646. Loss: 0.43398529291152954\n",
      "Iteration: 647. Loss: 0.43895405530929565\n",
      "Iteration: 648. Loss: 0.4656504690647125\n",
      "Iteration: 649. Loss: 0.5718525052070618\n",
      "Iteration: 650. Loss: 0.5692428946495056\n",
      "Iteration: 651. Loss: 0.4884550869464874\n",
      "Iteration: 652. Loss: 0.4498145282268524\n",
      "Iteration: 653. Loss: 0.5453805327415466\n",
      "Iteration: 654. Loss: 0.505911648273468\n",
      "Iteration: 655. Loss: 0.44069114327430725\n",
      "Iteration: 656. Loss: 0.42022305727005005\n",
      "Iteration: 657. Loss: 0.4484058618545532\n",
      "Iteration: 658. Loss: 0.5837541818618774\n",
      "Iteration: 659. Loss: 0.5594027638435364\n",
      "Iteration: 660. Loss: 0.4887617826461792\n",
      "Iteration: 661. Loss: 0.4579312205314636\n",
      "Iteration: 662. Loss: 0.5253421068191528\n",
      "Iteration: 663. Loss: 0.6078764200210571\n",
      "Iteration: 664. Loss: 0.48078370094299316\n",
      "Iteration: 665. Loss: 0.5506759881973267\n",
      "Iteration: 666. Loss: 0.5108722448348999\n",
      "Iteration: 667. Loss: 0.476024329662323\n",
      "Iteration: 668. Loss: 0.4912620186805725\n",
      "Iteration: 669. Loss: 0.5376327633857727\n",
      "Iteration: 670. Loss: 0.5631481409072876\n",
      "Iteration: 671. Loss: 0.5257871150970459\n",
      "Iteration: 672. Loss: 0.49509942531585693\n",
      "Iteration: 673. Loss: 0.4879996180534363\n",
      "Iteration: 674. Loss: 0.5338708758354187\n",
      "Iteration: 675. Loss: 0.42643165588378906\n",
      "Iteration: 676. Loss: 0.36423081159591675\n",
      "Iteration: 677. Loss: 0.39471885561943054\n",
      "Iteration: 678. Loss: 0.440161794424057\n",
      "Iteration: 679. Loss: 0.4262019693851471\n",
      "Iteration: 680. Loss: 0.5071411728858948\n",
      "Iteration: 681. Loss: 0.5853496789932251\n",
      "Iteration: 682. Loss: 0.40562331676483154\n",
      "Iteration: 683. Loss: 0.5038101673126221\n",
      "Iteration: 684. Loss: 0.42874011397361755\n",
      "Iteration: 685. Loss: 0.5826281905174255\n",
      "Iteration: 686. Loss: 0.5151615738868713\n",
      "Iteration: 687. Loss: 0.5313157439231873\n",
      "Iteration: 688. Loss: 0.5083678960800171\n",
      "Iteration: 689. Loss: 0.5328356027603149\n",
      "Iteration: 690. Loss: 0.453974187374115\n",
      "Iteration: 691. Loss: 0.5175397992134094\n",
      "Iteration: 692. Loss: 0.6055542230606079\n",
      "Iteration: 693. Loss: 0.43958228826522827\n",
      "Iteration: 694. Loss: 0.5746471285820007\n",
      "Iteration: 695. Loss: 0.5443620681762695\n",
      "Iteration: 696. Loss: 0.49147269129753113\n",
      "Iteration: 697. Loss: 0.5581343770027161\n",
      "Iteration: 698. Loss: 0.45729488134384155\n",
      "Iteration: 699. Loss: 0.5860622525215149\n",
      "Iteration: 700. Loss: 0.5950541496276855\n",
      "Iteration: 701. Loss: 0.5570735335350037\n",
      "Iteration: 702. Loss: 0.4759856164455414\n",
      "Iteration: 703. Loss: 0.4700155556201935\n",
      "Iteration: 704. Loss: 0.5134176015853882\n",
      "Iteration: 705. Loss: 0.4454022943973541\n",
      "Iteration: 706. Loss: 0.5425857305526733\n",
      "Iteration: 707. Loss: 0.4981406331062317\n",
      "Iteration: 708. Loss: 0.5041546821594238\n",
      "Iteration: 709. Loss: 0.4897574186325073\n",
      "Iteration: 710. Loss: 0.3846844732761383\n",
      "Iteration: 711. Loss: 0.5061172246932983\n",
      "Iteration: 712. Loss: 0.3880334496498108\n",
      "Iteration: 713. Loss: 0.42590537667274475\n",
      "Iteration: 714. Loss: 0.5369272828102112\n",
      "Iteration: 715. Loss: 0.5468356013298035\n",
      "Iteration: 716. Loss: 0.4531210958957672\n",
      "Iteration: 717. Loss: 0.5287210941314697\n",
      "Iteration: 718. Loss: 0.48635032773017883\n",
      "Iteration: 719. Loss: 0.436367392539978\n",
      "Iteration: 720. Loss: 0.5873273611068726\n",
      "Iteration: 721. Loss: 0.531898021697998\n",
      "Iteration: 722. Loss: 0.5570996999740601\n",
      "Iteration: 723. Loss: 0.6433927416801453\n",
      "Iteration: 724. Loss: 0.3948037624359131\n",
      "Iteration: 725. Loss: 0.40108582377433777\n",
      "Iteration: 726. Loss: 0.3948993682861328\n",
      "Iteration: 727. Loss: 0.32390958070755005\n",
      "Iteration: 728. Loss: 0.5060604214668274\n",
      "Iteration: 729. Loss: 0.48109081387519836\n",
      "Iteration: 730. Loss: 0.5960752964019775\n",
      "Iteration: 731. Loss: 0.5502028465270996\n",
      "Iteration: 732. Loss: 0.4102845788002014\n",
      "Iteration: 733. Loss: 0.47943904995918274\n",
      "Iteration: 734. Loss: 0.460048645734787\n",
      "Iteration: 735. Loss: 0.44613754749298096\n",
      "Iteration: 736. Loss: 0.5401256680488586\n",
      "Iteration: 737. Loss: 0.4798568785190582\n",
      "Iteration: 738. Loss: 0.4671410620212555\n",
      "Iteration: 739. Loss: 0.5009206533432007\n",
      "Iteration: 740. Loss: 0.511527419090271\n",
      "Iteration: 741. Loss: 0.4341951012611389\n",
      "Iteration: 742. Loss: 0.4357106685638428\n",
      "Iteration: 743. Loss: 0.4264843463897705\n",
      "Iteration: 744. Loss: 0.5663444995880127\n",
      "Iteration: 745. Loss: 0.5200095176696777\n",
      "Iteration: 746. Loss: 0.6508617997169495\n",
      "Iteration: 747. Loss: 0.5542402863502502\n",
      "Iteration: 748. Loss: 0.4418259859085083\n",
      "Iteration: 749. Loss: 0.40760937333106995\n",
      "Iteration: 750. Loss: 0.5389865636825562\n",
      "Iteration: 751. Loss: 0.37263599038124084\n",
      "Iteration: 752. Loss: 0.4778306186199188\n",
      "Iteration: 753. Loss: 0.4591883420944214\n",
      "Iteration: 754. Loss: 0.4443916380405426\n",
      "Iteration: 755. Loss: 0.47824692726135254\n",
      "Iteration: 756. Loss: 0.3764667212963104\n",
      "Iteration: 757. Loss: 0.5389642119407654\n",
      "Iteration: 758. Loss: 0.5020288228988647\n",
      "Iteration: 759. Loss: 0.5079020857810974\n",
      "Iteration: 760. Loss: 0.4490683078765869\n",
      "Iteration: 761. Loss: 0.49788203835487366\n",
      "Iteration: 762. Loss: 0.5432456731796265\n",
      "Iteration: 763. Loss: 0.4024137854576111\n",
      "Iteration: 764. Loss: 0.37042608857154846\n",
      "Iteration: 765. Loss: 0.441791296005249\n",
      "Iteration: 766. Loss: 0.5480650067329407\n",
      "Iteration: 767. Loss: 0.4089709520339966\n",
      "Iteration: 768. Loss: 0.6273376941680908\n",
      "Iteration: 769. Loss: 0.3676888644695282\n",
      "Iteration: 770. Loss: 0.4166792333126068\n",
      "Iteration: 771. Loss: 0.3612591624259949\n",
      "Iteration: 772. Loss: 0.48234817385673523\n",
      "Iteration: 773. Loss: 0.446682333946228\n",
      "Iteration: 774. Loss: 0.49255865812301636\n",
      "Iteration: 775. Loss: 0.4661288857460022\n",
      "Iteration: 776. Loss: 0.43086886405944824\n",
      "Iteration: 777. Loss: 0.5393861532211304\n",
      "Iteration: 778. Loss: 0.4346896708011627\n",
      "Iteration: 779. Loss: 0.40458211302757263\n",
      "Iteration: 780. Loss: 0.4957816004753113\n",
      "Iteration: 781. Loss: 0.47148844599723816\n",
      "Iteration: 782. Loss: 0.4520244598388672\n",
      "Iteration: 783. Loss: 0.41691944003105164\n",
      "Iteration: 784. Loss: 0.557019829750061\n",
      "Iteration: 785. Loss: 0.4567241370677948\n",
      "Iteration: 786. Loss: 0.4943479895591736\n",
      "Iteration: 787. Loss: 0.5054009556770325\n",
      "Iteration: 788. Loss: 0.5865845680236816\n",
      "Iteration: 789. Loss: 0.45926719903945923\n",
      "Iteration: 790. Loss: 0.44537338614463806\n",
      "Iteration: 791. Loss: 0.5234318375587463\n",
      "Iteration: 792. Loss: 0.5612344145774841\n",
      "Iteration: 793. Loss: 0.5205216407775879\n",
      "Iteration: 794. Loss: 0.4962131083011627\n",
      "Iteration: 795. Loss: 0.3133336901664734\n",
      "Iteration: 796. Loss: 0.475040465593338\n",
      "Iteration: 797. Loss: 0.6951026916503906\n",
      "Iteration: 798. Loss: 0.4571387767791748\n",
      "Iteration: 799. Loss: 0.37820184230804443\n",
      "Iteration: 800. Loss: 0.43792492151260376\n",
      "Iteration: 801. Loss: 0.5702572464942932\n",
      "Iteration: 802. Loss: 0.5458955764770508\n",
      "Iteration: 803. Loss: 0.3843594491481781\n",
      "Iteration: 804. Loss: 0.44043803215026855\n",
      "Iteration: 805. Loss: 0.5802074670791626\n",
      "Iteration: 806. Loss: 0.5035852789878845\n",
      "Iteration: 807. Loss: 0.49580010771751404\n",
      "Iteration: 808. Loss: 0.48512816429138184\n",
      "Iteration: 809. Loss: 0.4465154707431793\n",
      "Iteration: 810. Loss: 0.41495347023010254\n",
      "Iteration: 811. Loss: 0.5585893392562866\n",
      "Iteration: 812. Loss: 0.48985427618026733\n",
      "Iteration: 813. Loss: 0.6441823840141296\n",
      "Iteration: 814. Loss: 0.5124390125274658\n",
      "Iteration: 815. Loss: 0.5786426663398743\n",
      "Iteration: 816. Loss: 0.40984660387039185\n",
      "Iteration: 817. Loss: 0.42993438243865967\n",
      "Iteration: 818. Loss: 0.5125144720077515\n",
      "Iteration: 819. Loss: 0.5277539491653442\n",
      "Iteration: 820. Loss: 0.5108295679092407\n",
      "Iteration: 821. Loss: 0.5387203097343445\n",
      "Iteration: 822. Loss: 0.4554358422756195\n",
      "Iteration: 823. Loss: 0.4411044716835022\n",
      "Iteration: 824. Loss: 0.49174633622169495\n",
      "Iteration: 825. Loss: 0.44713619351387024\n",
      "Iteration: 826. Loss: 0.4702528715133667\n",
      "Iteration: 827. Loss: 0.2984980642795563\n",
      "Iteration: 828. Loss: 0.44286635518074036\n",
      "Iteration: 829. Loss: 0.6430871486663818\n",
      "Iteration: 830. Loss: 0.4156029522418976\n",
      "Iteration: 831. Loss: 0.563605785369873\n",
      "Iteration: 832. Loss: 0.527519941329956\n",
      "Iteration: 833. Loss: 0.5246970653533936\n",
      "Iteration: 834. Loss: 0.4083999991416931\n",
      "Iteration: 835. Loss: 0.4528578817844391\n",
      "Iteration: 836. Loss: 0.7233336567878723\n",
      "Iteration: 837. Loss: 0.39792031049728394\n",
      "Iteration: 838. Loss: 0.33164775371551514\n",
      "Iteration: 839. Loss: 0.388283371925354\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 840. Loss: 0.4135713577270508\n",
      "Iteration: 841. Loss: 0.44181305170059204\n",
      "Iteration: 842. Loss: 0.42025822401046753\n",
      "Iteration: 843. Loss: 0.44848841428756714\n",
      "Iteration: 844. Loss: 0.3819803297519684\n",
      "Iteration: 845. Loss: 0.43844783306121826\n",
      "Iteration: 846. Loss: 0.36966028809547424\n",
      "Iteration: 847. Loss: 0.46626320481300354\n",
      "Iteration: 848. Loss: 0.43146035075187683\n",
      "Iteration: 849. Loss: 0.5598979592323303\n",
      "Iteration: 850. Loss: 0.38457393646240234\n",
      "Iteration: 851. Loss: 0.4286067485809326\n",
      "Iteration: 852. Loss: 0.43966734409332275\n",
      "Iteration: 853. Loss: 0.40419575572013855\n",
      "Iteration: 854. Loss: 0.3636784851551056\n",
      "Iteration: 855. Loss: 0.481658399105072\n",
      "Iteration: 856. Loss: 0.5988155603408813\n",
      "Iteration: 857. Loss: 0.5557394027709961\n",
      "Iteration: 858. Loss: 0.6153659224510193\n",
      "Iteration: 859. Loss: 0.3953007459640503\n",
      "Iteration: 860. Loss: 0.4458882808685303\n",
      "Iteration: 861. Loss: 0.405337929725647\n",
      "Iteration: 862. Loss: 0.5751714706420898\n",
      "Iteration: 863. Loss: 0.44716858863830566\n",
      "Iteration: 864. Loss: 0.4619944393634796\n",
      "Iteration: 865. Loss: 0.36948204040527344\n",
      "Iteration: 866. Loss: 0.3644427955150604\n",
      "Iteration: 867. Loss: 0.4350214898586273\n",
      "Iteration: 868. Loss: 0.4254648685455322\n",
      "Iteration: 869. Loss: 0.5289531946182251\n",
      "Iteration: 870. Loss: 0.37746337056159973\n",
      "Iteration: 871. Loss: 0.45354342460632324\n",
      "Iteration: 872. Loss: 0.4389953911304474\n",
      "Iteration: 873. Loss: 0.5111455321311951\n",
      "Iteration: 874. Loss: 0.3646987974643707\n",
      "Iteration: 875. Loss: 0.549709141254425\n",
      "Iteration: 876. Loss: 0.4258823096752167\n",
      "Iteration: 877. Loss: 0.5291972756385803\n",
      "Iteration: 878. Loss: 0.41670703887939453\n",
      "Iteration: 879. Loss: 0.43811142444610596\n",
      "Iteration: 880. Loss: 0.46629345417022705\n",
      "Iteration: 881. Loss: 0.4430781602859497\n",
      "Iteration: 882. Loss: 0.3606768548488617\n",
      "Iteration: 883. Loss: 0.5502741932868958\n",
      "Iteration: 884. Loss: 0.4202404022216797\n",
      "Iteration: 885. Loss: 0.4542069137096405\n",
      "Iteration: 886. Loss: 0.47698038816452026\n",
      "Iteration: 887. Loss: 0.3303411602973938\n",
      "Iteration: 888. Loss: 0.5881461501121521\n",
      "Iteration: 889. Loss: 0.454234778881073\n",
      "Iteration: 890. Loss: 0.42096835374832153\n",
      "Iteration: 891. Loss: 0.4227958917617798\n",
      "Iteration: 892. Loss: 0.33583343029022217\n",
      "Iteration: 893. Loss: 0.45391571521759033\n",
      "Iteration: 894. Loss: 0.5663446187973022\n",
      "Iteration: 895. Loss: 0.38899490237236023\n",
      "Iteration: 896. Loss: 0.4302193522453308\n",
      "Iteration: 897. Loss: 0.5036714673042297\n",
      "Iteration: 898. Loss: 0.41478896141052246\n",
      "Iteration: 899. Loss: 0.5403162240982056\n",
      "Iteration: 900. Loss: 0.4644152522087097\n",
      "Iteration: 901. Loss: 0.46021389961242676\n",
      "Iteration: 902. Loss: 0.4468733072280884\n",
      "Iteration: 903. Loss: 0.32513654232025146\n",
      "Iteration: 904. Loss: 0.35982850193977356\n",
      "Iteration: 905. Loss: 0.4918317496776581\n",
      "Iteration: 906. Loss: 0.4565369784832001\n",
      "Iteration: 907. Loss: 0.4155334234237671\n",
      "Iteration: 908. Loss: 0.43824565410614014\n",
      "Iteration: 909. Loss: 0.5379655361175537\n",
      "Iteration: 910. Loss: 0.505222499370575\n",
      "Iteration: 911. Loss: 0.5422770977020264\n",
      "Iteration: 912. Loss: 0.38564175367355347\n",
      "Iteration: 913. Loss: 0.4890805184841156\n",
      "Iteration: 914. Loss: 0.43005067110061646\n",
      "Iteration: 915. Loss: 0.3846222162246704\n",
      "Iteration: 916. Loss: 0.4434739649295807\n",
      "Iteration: 917. Loss: 0.44700074195861816\n",
      "Iteration: 918. Loss: 0.388839453458786\n",
      "Iteration: 919. Loss: 0.39721763134002686\n",
      "Iteration: 920. Loss: 0.4509086608886719\n",
      "Iteration: 921. Loss: 0.4960552155971527\n",
      "Iteration: 922. Loss: 0.5351402759552002\n",
      "Iteration: 923. Loss: 0.34942910075187683\n",
      "Iteration: 924. Loss: 0.4167163074016571\n",
      "Iteration: 925. Loss: 0.4009293019771576\n",
      "Iteration: 926. Loss: 0.44631001353263855\n",
      "Iteration: 927. Loss: 0.4117325246334076\n",
      "Iteration: 928. Loss: 0.5163993239402771\n",
      "Iteration: 929. Loss: 0.43239453434944153\n",
      "Iteration: 930. Loss: 0.44806286692619324\n",
      "Iteration: 931. Loss: 0.3894538879394531\n",
      "Iteration: 932. Loss: 0.49238118529319763\n",
      "Iteration: 933. Loss: 0.3772951364517212\n",
      "Iteration: 934. Loss: 0.43003490567207336\n",
      "Iteration: 935. Loss: 0.333530068397522\n",
      "Iteration: 936. Loss: 0.5126316547393799\n",
      "Iteration: 937. Loss: 0.46827852725982666\n",
      "Iteration: 938. Loss: 0.4054132103919983\n",
      "Iteration: 939. Loss: 0.4880504906177521\n",
      "Iteration: 940. Loss: 0.5541592240333557\n",
      "Iteration: 941. Loss: 0.42968884110450745\n",
      "Iteration: 942. Loss: 0.442426860332489\n",
      "Iteration: 943. Loss: 0.39166581630706787\n",
      "Iteration: 944. Loss: 0.40065184235572815\n",
      "Iteration: 945. Loss: 0.4042084515094757\n",
      "Iteration: 946. Loss: 0.4725649654865265\n",
      "Iteration: 947. Loss: 0.44490259885787964\n",
      "Iteration: 948. Loss: 0.3868318796157837\n",
      "Iteration: 949. Loss: 0.32316577434539795\n",
      "Iteration: 950. Loss: 0.3710811138153076\n",
      "Iteration: 951. Loss: 0.4725537896156311\n",
      "Iteration: 952. Loss: 0.4180605411529541\n",
      "Iteration: 953. Loss: 0.5042126774787903\n",
      "Iteration: 954. Loss: 0.44124579429626465\n",
      "Iteration: 955. Loss: 0.4512229859828949\n",
      "Iteration: 956. Loss: 0.5969215631484985\n",
      "Iteration: 957. Loss: 0.5869908928871155\n",
      "Iteration: 958. Loss: 0.3631242513656616\n",
      "Iteration: 959. Loss: 0.4760604202747345\n",
      "Iteration: 960. Loss: 0.46368664503097534\n",
      "Iteration: 961. Loss: 0.3364209830760956\n",
      "Iteration: 962. Loss: 0.4237883388996124\n",
      "Iteration: 963. Loss: 0.461582750082016\n",
      "Iteration: 964. Loss: 0.4395916759967804\n",
      "Iteration: 965. Loss: 0.32812052965164185\n",
      "Iteration: 966. Loss: 0.5465336441993713\n",
      "Iteration: 967. Loss: 0.6191294193267822\n",
      "Iteration: 968. Loss: 0.40933144092559814\n",
      "Iteration: 969. Loss: 0.4446611702442169\n",
      "Iteration: 970. Loss: 0.4118506610393524\n",
      "Iteration: 971. Loss: 0.3827711045742035\n",
      "Iteration: 972. Loss: 0.3369227945804596\n",
      "Iteration: 973. Loss: 0.3570481538772583\n",
      "Iteration: 974. Loss: 0.3294815123081207\n",
      "Iteration: 975. Loss: 0.4586673378944397\n",
      "Iteration: 976. Loss: 0.37841716408729553\n",
      "Iteration: 977. Loss: 0.3926069736480713\n",
      "Iteration: 978. Loss: 0.38222911953926086\n",
      "Iteration: 979. Loss: 0.57058185338974\n",
      "Iteration: 980. Loss: 0.3340556025505066\n",
      "Iteration: 981. Loss: 0.4239180386066437\n",
      "Iteration: 982. Loss: 0.4272955656051636\n",
      "Iteration: 983. Loss: 0.37771710753440857\n",
      "Iteration: 984. Loss: 0.32731688022613525\n",
      "Iteration: 985. Loss: 0.5226424932479858\n",
      "Iteration: 986. Loss: 0.4188688397407532\n",
      "Iteration: 987. Loss: 0.4181201159954071\n",
      "Iteration: 988. Loss: 0.524446427822113\n",
      "Iteration: 989. Loss: 0.3884662687778473\n",
      "Iteration: 990. Loss: 0.4577464163303375\n",
      "Iteration: 991. Loss: 0.5362929105758667\n",
      "Iteration: 992. Loss: 0.46291041374206543\n",
      "Iteration: 993. Loss: 0.39131414890289307\n",
      "Iteration: 994. Loss: 0.33805572986602783\n",
      "Iteration: 995. Loss: 0.43547236919403076\n",
      "Iteration: 996. Loss: 0.4060770869255066\n",
      "Iteration: 997. Loss: 0.42643219232559204\n",
      "Iteration: 998. Loss: 0.35116103291511536\n",
      "Iteration: 999. Loss: 0.4028010070323944\n",
      "Iteration: 1000. Loss: 0.43662071228027344\n",
      "Iteration: 1001. Loss: 0.45092132687568665\n",
      "Iteration: 1002. Loss: 0.4004281759262085\n",
      "Iteration: 1003. Loss: 0.38129928708076477\n",
      "Iteration: 1004. Loss: 0.42889854311943054\n",
      "Iteration: 1005. Loss: 0.4389609098434448\n",
      "Iteration: 1006. Loss: 0.3746810853481293\n",
      "Iteration: 1007. Loss: 0.45345786213874817\n",
      "Iteration: 1008. Loss: 0.42605409026145935\n",
      "Iteration: 1009. Loss: 0.294135183095932\n",
      "Iteration: 1010. Loss: 0.42619627714157104\n",
      "Iteration: 1011. Loss: 0.4889064431190491\n",
      "Iteration: 1012. Loss: 0.5137887001037598\n",
      "Iteration: 1013. Loss: 0.44364988803863525\n",
      "Iteration: 1014. Loss: 0.37149739265441895\n",
      "Iteration: 1015. Loss: 0.33966800570487976\n",
      "Iteration: 1016. Loss: 0.6374495029449463\n",
      "Iteration: 1017. Loss: 0.4712749719619751\n",
      "Iteration: 1018. Loss: 0.3121170103549957\n",
      "Iteration: 1019. Loss: 0.4220726490020752\n",
      "Iteration: 1020. Loss: 0.37312018871307373\n",
      "Iteration: 1021. Loss: 0.5042944550514221\n",
      "Iteration: 1022. Loss: 0.335261732339859\n",
      "Iteration: 1023. Loss: 0.39657044410705566\n",
      "Iteration: 1024. Loss: 0.37158218026161194\n",
      "Iteration: 1025. Loss: 0.20632916688919067\n",
      "Iteration: 1026. Loss: 0.4279146194458008\n",
      "Iteration: 1027. Loss: 0.4712708592414856\n",
      "Iteration: 1028. Loss: 0.33023616671562195\n",
      "Iteration: 1029. Loss: 0.3777649402618408\n",
      "Iteration: 1030. Loss: 0.3345126211643219\n",
      "Iteration: 1031. Loss: 0.4492267966270447\n",
      "Iteration: 1032. Loss: 0.36947154998779297\n",
      "Iteration: 1033. Loss: 0.4614217281341553\n",
      "Iteration: 1034. Loss: 0.4024163484573364\n",
      "Iteration: 1035. Loss: 0.37232810258865356\n",
      "Iteration: 1036. Loss: 0.3677269220352173\n",
      "Iteration: 1037. Loss: 0.43211737275123596\n",
      "Iteration: 1038. Loss: 0.37285909056663513\n",
      "Iteration: 1039. Loss: 0.34581801295280457\n",
      "Iteration: 1040. Loss: 0.4017521142959595\n",
      "Iteration: 1041. Loss: 0.3208695352077484\n",
      "Iteration: 1042. Loss: 0.5635378360748291\n",
      "Iteration: 1043. Loss: 0.43101266026496887\n",
      "Iteration: 1044. Loss: 0.3426291346549988\n",
      "Iteration: 1045. Loss: 0.31227076053619385\n",
      "Iteration: 1046. Loss: 0.3651409149169922\n",
      "Iteration: 1047. Loss: 0.41636258363723755\n",
      "Iteration: 1048. Loss: 0.41597479581832886\n",
      "Iteration: 1049. Loss: 0.3870101869106293\n",
      "Iteration: 1050. Loss: 0.424490749835968\n",
      "Iteration: 1051. Loss: 0.42710185050964355\n",
      "Iteration: 1052. Loss: 0.5046361684799194\n",
      "Iteration: 1053. Loss: 0.4850026071071625\n",
      "Iteration: 1054. Loss: 0.38927170634269714\n",
      "Iteration: 1055. Loss: 0.34580370783805847\n",
      "Iteration: 1056. Loss: 0.34600815176963806\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1057. Loss: 0.5394534468650818\n",
      "Iteration: 1058. Loss: 0.44552457332611084\n",
      "Iteration: 1059. Loss: 0.4889252483844757\n",
      "Iteration: 1060. Loss: 0.4104699194431305\n",
      "Iteration: 1061. Loss: 0.26344752311706543\n",
      "Iteration: 1062. Loss: 0.46992599964141846\n",
      "Iteration: 1063. Loss: 0.3196554183959961\n",
      "Iteration: 1064. Loss: 0.35979124903678894\n",
      "Iteration: 1065. Loss: 0.3754998743534088\n",
      "Iteration: 1066. Loss: 0.3315136730670929\n",
      "Iteration: 1067. Loss: 0.45215901732444763\n",
      "Iteration: 1068. Loss: 0.39608269929885864\n",
      "Iteration: 1069. Loss: 0.46283668279647827\n",
      "Iteration: 1070. Loss: 0.4404822289943695\n",
      "Iteration: 1071. Loss: 0.35250863432884216\n",
      "Iteration: 1072. Loss: 0.3457489013671875\n",
      "Iteration: 1073. Loss: 0.3392787277698517\n",
      "Iteration: 1074. Loss: 0.42192572355270386\n",
      "Iteration: 1075. Loss: 0.44728243350982666\n",
      "Iteration: 1076. Loss: 0.47996535897254944\n",
      "Iteration: 1077. Loss: 0.4817388951778412\n",
      "Iteration: 1078. Loss: 0.36604294180870056\n",
      "Iteration: 1079. Loss: 0.3060312569141388\n",
      "Iteration: 1080. Loss: 0.4825887680053711\n",
      "Iteration: 1081. Loss: 0.46941566467285156\n",
      "Iteration: 1082. Loss: 0.3652825653553009\n",
      "Iteration: 1083. Loss: 0.37774571776390076\n",
      "Iteration: 1084. Loss: 0.3601410984992981\n",
      "Iteration: 1085. Loss: 0.4268709421157837\n",
      "Iteration: 1086. Loss: 0.364182710647583\n",
      "Iteration: 1087. Loss: 0.31228107213974\n",
      "Iteration: 1088. Loss: 0.41330403089523315\n",
      "Iteration: 1089. Loss: 0.35094523429870605\n",
      "Iteration: 1090. Loss: 0.40509673953056335\n",
      "Iteration: 1091. Loss: 0.41548100113868713\n",
      "Iteration: 1092. Loss: 0.41668882966041565\n",
      "Iteration: 1093. Loss: 0.4420730471611023\n",
      "Iteration: 1094. Loss: 0.458051860332489\n",
      "Iteration: 1095. Loss: 0.32615306973457336\n",
      "Iteration: 1096. Loss: 0.5273826718330383\n",
      "Iteration: 1097. Loss: 0.4184025228023529\n",
      "Iteration: 1098. Loss: 0.4169389307498932\n",
      "Iteration: 1099. Loss: 0.36063435673713684\n",
      "Iteration: 1100. Loss: 0.3274345397949219\n",
      "Iteration: 1101. Loss: 0.4627642333507538\n",
      "Iteration: 1102. Loss: 0.40259087085723877\n",
      "Iteration: 1103. Loss: 0.31305286288261414\n",
      "Iteration: 1104. Loss: 0.44228920340538025\n",
      "Iteration: 1105. Loss: 0.4075357913970947\n",
      "Iteration: 1106. Loss: 0.38340553641319275\n",
      "Iteration: 1107. Loss: 0.35539448261260986\n",
      "Iteration: 1108. Loss: 0.3676767647266388\n",
      "Iteration: 1109. Loss: 0.3910940885543823\n",
      "Iteration: 1110. Loss: 0.30567437410354614\n",
      "Iteration: 1111. Loss: 0.36989250779151917\n",
      "Iteration: 1112. Loss: 0.43956834077835083\n",
      "Iteration: 1113. Loss: 0.3094653785228729\n",
      "Iteration: 1114. Loss: 0.3756812810897827\n",
      "Iteration: 1115. Loss: 0.3593077063560486\n",
      "Iteration: 1116. Loss: 0.47851893305778503\n",
      "Iteration: 1117. Loss: 0.36924174427986145\n",
      "Iteration: 1118. Loss: 0.3783023953437805\n",
      "Iteration: 1119. Loss: 0.250786691904068\n",
      "Iteration: 1120. Loss: 0.5817930698394775\n",
      "Iteration: 1121. Loss: 0.44630101323127747\n",
      "Iteration: 1122. Loss: 0.4553399384021759\n",
      "Iteration: 1123. Loss: 0.42297402024269104\n",
      "Iteration: 1124. Loss: 0.32486599683761597\n",
      "Iteration: 1125. Loss: 0.4745062589645386\n",
      "Iteration: 1126. Loss: 0.3877184987068176\n",
      "Iteration: 1127. Loss: 0.47101181745529175\n",
      "Iteration: 1128. Loss: 0.4667821526527405\n",
      "Iteration: 1129. Loss: 0.31142687797546387\n",
      "Iteration: 1130. Loss: 0.3218648433685303\n",
      "Iteration: 1131. Loss: 0.4063531756401062\n",
      "Iteration: 1132. Loss: 0.3204767107963562\n",
      "Iteration: 1133. Loss: 0.4687502682209015\n",
      "Iteration: 1134. Loss: 0.422600656747818\n",
      "Iteration: 1135. Loss: 0.42545729875564575\n",
      "Iteration: 1136. Loss: 0.4157409965991974\n",
      "Iteration: 1137. Loss: 0.42057108879089355\n",
      "Iteration: 1138. Loss: 0.5351075530052185\n",
      "Iteration: 1139. Loss: 0.4915166199207306\n",
      "Iteration: 1140. Loss: 0.3634702265262604\n",
      "Iteration: 1141. Loss: 0.24824516475200653\n",
      "Iteration: 1142. Loss: 0.35814499855041504\n",
      "Iteration: 1143. Loss: 0.41572096943855286\n",
      "Iteration: 1144. Loss: 0.5272242426872253\n",
      "Iteration: 1145. Loss: 0.4612868130207062\n",
      "Iteration: 1146. Loss: 0.26917126774787903\n",
      "Iteration: 1147. Loss: 0.28765997290611267\n",
      "Iteration: 1148. Loss: 0.5222223401069641\n",
      "Iteration: 1149. Loss: 0.5020487904548645\n",
      "Iteration: 1150. Loss: 0.5368912220001221\n",
      "Iteration: 1151. Loss: 0.5098526477813721\n",
      "Iteration: 1152. Loss: 0.29961463809013367\n",
      "Iteration: 1153. Loss: 0.4376881718635559\n",
      "Iteration: 1154. Loss: 0.4062660336494446\n",
      "Iteration: 1155. Loss: 0.33133047819137573\n",
      "Iteration: 1156. Loss: 0.49457859992980957\n",
      "Iteration: 1157. Loss: 0.34043288230895996\n",
      "Iteration: 1158. Loss: 0.3461054563522339\n",
      "Iteration: 1159. Loss: 0.34654709696769714\n",
      "Iteration: 1160. Loss: 0.29539787769317627\n",
      "Iteration: 1161. Loss: 0.2617592513561249\n",
      "Iteration: 1162. Loss: 0.4538455903530121\n",
      "Iteration: 1163. Loss: 0.31234776973724365\n",
      "Iteration: 1164. Loss: 0.45585280656814575\n",
      "Iteration: 1165. Loss: 0.46608230471611023\n",
      "Iteration: 1166. Loss: 0.3592957556247711\n",
      "Iteration: 1167. Loss: 0.4464760720729828\n",
      "Iteration: 1168. Loss: 0.34464961290359497\n",
      "Iteration: 1169. Loss: 0.4386972486972809\n",
      "Iteration: 1170. Loss: 0.3404122292995453\n",
      "Iteration: 1171. Loss: 0.3768491744995117\n",
      "Iteration: 1172. Loss: 0.3231040835380554\n",
      "Iteration: 1173. Loss: 0.34407782554626465\n",
      "Iteration: 1174. Loss: 0.3780796527862549\n",
      "Iteration: 1175. Loss: 0.3432370126247406\n",
      "Iteration: 1176. Loss: 0.3524465560913086\n",
      "Iteration: 1177. Loss: 0.34039196372032166\n",
      "Iteration: 1178. Loss: 0.34991687536239624\n",
      "Iteration: 1179. Loss: 0.3648271858692169\n",
      "Iteration: 1180. Loss: 0.41654568910598755\n",
      "Iteration: 1181. Loss: 0.29736030101776123\n",
      "Iteration: 1182. Loss: 0.4958064556121826\n",
      "Iteration: 1183. Loss: 0.42183226346969604\n",
      "Iteration: 1184. Loss: 0.34691867232322693\n",
      "Iteration: 1185. Loss: 0.40047532320022583\n",
      "Iteration: 1186. Loss: 0.4089474380016327\n",
      "Iteration: 1187. Loss: 0.43143269419670105\n",
      "Iteration: 1188. Loss: 0.4289962351322174\n",
      "Iteration: 1189. Loss: 0.43242859840393066\n",
      "Iteration: 1190. Loss: 0.3005775511264801\n",
      "Iteration: 1191. Loss: 0.3834730386734009\n",
      "Iteration: 1192. Loss: 0.3661826252937317\n",
      "Iteration: 1193. Loss: 0.28129512071609497\n",
      "Iteration: 1194. Loss: 0.4568600058555603\n",
      "Iteration: 1195. Loss: 0.5884996056556702\n",
      "Iteration: 1196. Loss: 0.4209224581718445\n",
      "Iteration: 1197. Loss: 0.352563738822937\n",
      "Iteration: 1198. Loss: 0.40891414880752563\n",
      "Iteration: 1199. Loss: 0.4568600356578827\n",
      "Iteration: 1200. Loss: 0.38856667280197144\n",
      "Iteration: 1201. Loss: 0.30559608340263367\n",
      "Iteration: 1202. Loss: 0.40298423171043396\n",
      "Iteration: 1203. Loss: 0.35944682359695435\n",
      "Iteration: 1204. Loss: 0.3970395624637604\n",
      "Iteration: 1205. Loss: 0.39298325777053833\n",
      "Iteration: 1206. Loss: 0.3427939713001251\n",
      "Iteration: 1207. Loss: 0.3460811376571655\n",
      "Iteration: 1208. Loss: 0.4258321523666382\n",
      "Iteration: 1209. Loss: 0.47994086146354675\n",
      "Iteration: 1210. Loss: 0.4463895857334137\n",
      "Iteration: 1211. Loss: 0.33979499340057373\n",
      "Iteration: 1212. Loss: 0.3496777415275574\n",
      "Iteration: 1213. Loss: 0.5769792199134827\n",
      "Iteration: 1214. Loss: 0.3769736588001251\n",
      "Iteration: 1215. Loss: 0.3839960992336273\n",
      "Iteration: 1216. Loss: 0.35423561930656433\n",
      "Iteration: 1217. Loss: 0.6009464859962463\n",
      "Iteration: 1218. Loss: 0.39921408891677856\n",
      "Iteration: 1219. Loss: 0.2929517924785614\n",
      "Iteration: 1220. Loss: 0.33479270339012146\n",
      "Iteration: 1221. Loss: 0.4236609637737274\n",
      "Iteration: 1222. Loss: 0.38870561122894287\n",
      "Iteration: 1223. Loss: 0.37926632165908813\n",
      "Iteration: 1224. Loss: 0.5199790000915527\n",
      "Iteration: 1225. Loss: 0.31988734006881714\n",
      "Iteration: 1226. Loss: 0.4285668134689331\n",
      "Iteration: 1227. Loss: 0.4869917035102844\n",
      "Iteration: 1228. Loss: 0.5280932188034058\n",
      "Iteration: 1229. Loss: 0.3156020939350128\n",
      "Iteration: 1230. Loss: 0.5312883853912354\n",
      "Iteration: 1231. Loss: 0.41591089963912964\n",
      "Iteration: 1232. Loss: 0.4755513668060303\n",
      "Iteration: 1233. Loss: 0.4324173629283905\n",
      "Iteration: 1234. Loss: 0.4923432171344757\n",
      "Iteration: 1235. Loss: 0.3628813624382019\n",
      "Iteration: 1236. Loss: 0.37030720710754395\n",
      "Iteration: 1237. Loss: 0.4575861990451813\n",
      "Iteration: 1238. Loss: 0.42232295870780945\n",
      "Iteration: 1239. Loss: 0.495296448469162\n",
      "Iteration: 1240. Loss: 0.4820943772792816\n",
      "Iteration: 1241. Loss: 0.413516640663147\n",
      "Iteration: 1242. Loss: 0.509800136089325\n",
      "Iteration: 1243. Loss: 0.38010352849960327\n",
      "Iteration: 1244. Loss: 0.2881268560886383\n",
      "Iteration: 1245. Loss: 0.42442071437835693\n",
      "Iteration: 1246. Loss: 0.32279807329177856\n",
      "Iteration: 1247. Loss: 0.5831369161605835\n",
      "Iteration: 1248. Loss: 0.39182761311531067\n",
      "Iteration: 1249. Loss: 0.45831841230392456\n",
      "Iteration: 1250. Loss: 0.27407875657081604\n",
      "Iteration: 1251. Loss: 0.3334228992462158\n",
      "Iteration: 1252. Loss: 0.358437180519104\n",
      "Iteration: 1253. Loss: 0.25970926880836487\n",
      "Iteration: 1254. Loss: 0.324677437543869\n",
      "Iteration: 1255. Loss: 0.39708423614501953\n",
      "Iteration: 1256. Loss: 0.36295029520988464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1257. Loss: 0.3854697048664093\n",
      "Iteration: 1258. Loss: 0.4290136694908142\n",
      "Iteration: 1259. Loss: 0.38870295882225037\n",
      "Iteration: 1260. Loss: 0.22633296251296997\n",
      "Iteration: 1261. Loss: 0.4738813042640686\n",
      "Iteration: 1262. Loss: 0.4139886796474457\n",
      "Iteration: 1263. Loss: 0.33417823910713196\n",
      "Iteration: 1264. Loss: 0.2808234691619873\n",
      "Iteration: 1265. Loss: 0.3323850929737091\n",
      "Iteration: 1266. Loss: 0.3153400123119354\n",
      "Iteration: 1267. Loss: 0.28224828839302063\n",
      "Iteration: 1268. Loss: 0.4098421037197113\n",
      "Iteration: 1269. Loss: 0.36751988530158997\n",
      "Iteration: 1270. Loss: 0.37278133630752563\n",
      "Iteration: 1271. Loss: 0.3025243878364563\n",
      "Iteration: 1272. Loss: 0.44045937061309814\n",
      "Iteration: 1273. Loss: 0.4356826841831207\n",
      "Iteration: 1274. Loss: 0.36790168285369873\n",
      "Iteration: 1275. Loss: 0.4016934633255005\n",
      "Iteration: 1276. Loss: 0.3513405919075012\n",
      "Iteration: 1277. Loss: 0.4048590064048767\n",
      "Iteration: 1278. Loss: 0.4168553948402405\n",
      "Iteration: 1279. Loss: 0.3112664520740509\n",
      "Iteration: 1280. Loss: 0.372706800699234\n",
      "Iteration: 1281. Loss: 0.27262958884239197\n",
      "Iteration: 1282. Loss: 0.36223191022872925\n",
      "Iteration: 1283. Loss: 0.3420787751674652\n",
      "Iteration: 1284. Loss: 0.25653961300849915\n",
      "Iteration: 1285. Loss: 0.41365906596183777\n",
      "Iteration: 1286. Loss: 0.33250388503074646\n",
      "Iteration: 1287. Loss: 0.4023529291152954\n",
      "Iteration: 1288. Loss: 0.28946754336357117\n",
      "Iteration: 1289. Loss: 0.34042251110076904\n",
      "Iteration: 1290. Loss: 0.3664306700229645\n",
      "Iteration: 1291. Loss: 0.38164466619491577\n",
      "Iteration: 1292. Loss: 0.31765127182006836\n",
      "Iteration: 1293. Loss: 0.2931053042411804\n",
      "Iteration: 1294. Loss: 0.37255650758743286\n",
      "Iteration: 1295. Loss: 0.4094606041908264\n",
      "Iteration: 1296. Loss: 0.3787623345851898\n",
      "Iteration: 1297. Loss: 0.34568727016448975\n",
      "Iteration: 1298. Loss: 0.399850994348526\n",
      "Iteration: 1299. Loss: 0.2249564677476883\n",
      "Iteration: 1300. Loss: 0.4403876066207886\n",
      "Iteration: 1301. Loss: 0.40644368529319763\n",
      "Iteration: 1302. Loss: 0.3316534161567688\n",
      "Iteration: 1303. Loss: 0.3375319242477417\n",
      "Iteration: 1304. Loss: 0.32445284724235535\n",
      "Iteration: 1305. Loss: 0.39799755811691284\n",
      "Iteration: 1306. Loss: 0.2863048315048218\n",
      "Iteration: 1307. Loss: 0.3960229158401489\n",
      "Iteration: 1308. Loss: 0.3952270448207855\n",
      "Iteration: 1309. Loss: 0.45998629927635193\n",
      "Iteration: 1310. Loss: 0.42415371537208557\n",
      "Iteration: 1311. Loss: 0.4212513267993927\n",
      "Iteration: 1312. Loss: 0.3791288435459137\n",
      "Iteration: 1313. Loss: 0.45947861671447754\n",
      "Iteration: 1314. Loss: 0.26879531145095825\n",
      "Iteration: 1315. Loss: 0.3258408010005951\n",
      "Iteration: 1316. Loss: 0.2673337459564209\n",
      "Iteration: 1317. Loss: 0.4196344017982483\n",
      "Iteration: 1318. Loss: 0.28736981749534607\n",
      "Iteration: 1319. Loss: 0.32685932517051697\n",
      "Iteration: 1320. Loss: 0.31722956895828247\n",
      "Iteration: 1321. Loss: 0.3872298300266266\n",
      "Iteration: 1322. Loss: 0.34632521867752075\n",
      "Iteration: 1323. Loss: 0.3762845993041992\n",
      "Iteration: 1324. Loss: 0.4766053855419159\n",
      "Iteration: 1325. Loss: 0.34488770365715027\n",
      "Iteration: 1326. Loss: 0.5438897013664246\n",
      "Iteration: 1327. Loss: 0.22519086301326752\n",
      "Iteration: 1328. Loss: 0.5028650760650635\n",
      "Iteration: 1329. Loss: 0.4190036356449127\n",
      "Iteration: 1330. Loss: 0.4097821116447449\n",
      "Iteration: 1331. Loss: 0.4630195200443268\n",
      "Iteration: 1332. Loss: 0.23733477294445038\n",
      "Iteration: 1333. Loss: 0.3893614709377289\n",
      "Iteration: 1334. Loss: 0.39974814653396606\n",
      "Iteration: 1335. Loss: 0.4957486093044281\n",
      "Iteration: 1336. Loss: 0.3091275990009308\n",
      "Iteration: 1337. Loss: 0.2810738682746887\n",
      "Iteration: 1338. Loss: 0.3255011737346649\n",
      "Iteration: 1339. Loss: 0.33931827545166016\n",
      "Iteration: 1340. Loss: 0.5512691736221313\n",
      "Iteration: 1341. Loss: 0.47045275568962097\n",
      "Iteration: 1342. Loss: 0.4861951768398285\n",
      "Iteration: 1343. Loss: 0.293499231338501\n",
      "Iteration: 1344. Loss: 0.2757640779018402\n",
      "Iteration: 1345. Loss: 0.32084187865257263\n",
      "Iteration: 1346. Loss: 0.24449487030506134\n",
      "Iteration: 1347. Loss: 0.26929789781570435\n",
      "Iteration: 1348. Loss: 0.5143142938613892\n",
      "Iteration: 1349. Loss: 0.3206441104412079\n",
      "Iteration: 1350. Loss: 0.3773954510688782\n",
      "Iteration: 1351. Loss: 0.3416685461997986\n",
      "Iteration: 1352. Loss: 0.3007139265537262\n",
      "Iteration: 1353. Loss: 0.388566255569458\n",
      "Iteration: 1354. Loss: 0.42969974875450134\n",
      "Iteration: 1355. Loss: 0.37213969230651855\n",
      "Iteration: 1356. Loss: 0.3638049066066742\n",
      "Iteration: 1357. Loss: 0.3339953124523163\n",
      "Iteration: 1358. Loss: 0.2384684532880783\n",
      "Iteration: 1359. Loss: 0.38683465123176575\n",
      "Iteration: 1360. Loss: 0.4241810142993927\n",
      "Iteration: 1361. Loss: 0.2995738685131073\n",
      "Iteration: 1362. Loss: 0.37355250120162964\n",
      "Iteration: 1363. Loss: 0.48552024364471436\n",
      "Iteration: 1364. Loss: 0.46620044112205505\n",
      "Iteration: 1365. Loss: 0.4650644361972809\n",
      "Iteration: 1366. Loss: 0.24484996497631073\n",
      "Iteration: 1367. Loss: 0.41448405385017395\n",
      "Iteration: 1368. Loss: 0.3370572030544281\n",
      "Iteration: 1369. Loss: 0.3674919009208679\n",
      "Iteration: 1370. Loss: 0.22168470919132233\n",
      "Iteration: 1371. Loss: 0.45303475856781006\n",
      "Iteration: 1372. Loss: 0.4599783420562744\n",
      "Iteration: 1373. Loss: 0.34116947650909424\n",
      "Iteration: 1374. Loss: 0.4532039761543274\n",
      "Iteration: 1375. Loss: 0.4133811891078949\n",
      "Iteration: 1376. Loss: 0.3157515525817871\n",
      "Iteration: 1377. Loss: 0.2614249885082245\n",
      "Iteration: 1378. Loss: 0.32894840836524963\n",
      "Iteration: 1379. Loss: 0.29943761229515076\n",
      "Iteration: 1380. Loss: 0.363064169883728\n",
      "Iteration: 1381. Loss: 0.3804551959037781\n",
      "Iteration: 1382. Loss: 0.4312652349472046\n",
      "Iteration: 1383. Loss: 0.40553924441337585\n",
      "Iteration: 1384. Loss: 0.4068949222564697\n",
      "Iteration: 1385. Loss: 0.323533296585083\n",
      "Iteration: 1386. Loss: 0.4840150475502014\n",
      "Iteration: 1387. Loss: 0.34444737434387207\n",
      "Iteration: 1388. Loss: 0.33438780903816223\n",
      "Iteration: 1389. Loss: 0.3728843629360199\n",
      "Iteration: 1390. Loss: 0.2996695637702942\n",
      "Iteration: 1391. Loss: 0.3525623679161072\n",
      "Iteration: 1392. Loss: 0.39620915055274963\n",
      "Iteration: 1393. Loss: 0.52495276927948\n",
      "Iteration: 1394. Loss: 0.42669355869293213\n",
      "Iteration: 1395. Loss: 0.4088735580444336\n",
      "Iteration: 1396. Loss: 0.3476155400276184\n",
      "Iteration: 1397. Loss: 0.3019757866859436\n",
      "Iteration: 1398. Loss: 0.2819289267063141\n",
      "Iteration: 1399. Loss: 0.33672723174095154\n",
      "Iteration: 1400. Loss: 0.2868674397468567\n",
      "Iteration: 1401. Loss: 0.3725724518299103\n",
      "Iteration: 1402. Loss: 0.3699358403682709\n",
      "Iteration: 1403. Loss: 0.37003573775291443\n",
      "Iteration: 1404. Loss: 0.4901474118232727\n",
      "Iteration: 1405. Loss: 0.30007973313331604\n",
      "Iteration: 1406. Loss: 0.4347081780433655\n",
      "Iteration: 1407. Loss: 0.35937607288360596\n",
      "Iteration: 1408. Loss: 0.3097061514854431\n",
      "Iteration: 1409. Loss: 0.33729878067970276\n",
      "Iteration: 1410. Loss: 0.31828609108924866\n",
      "Iteration: 1411. Loss: 0.4007877707481384\n",
      "Iteration: 1412. Loss: 0.3767116069793701\n",
      "Iteration: 1413. Loss: 0.4990993142127991\n",
      "Iteration: 1414. Loss: 0.4820532202720642\n",
      "Iteration: 1415. Loss: 0.40017440915107727\n",
      "Iteration: 1416. Loss: 0.3871375620365143\n",
      "Iteration: 1417. Loss: 0.3209415078163147\n",
      "Iteration: 1418. Loss: 0.256534218788147\n",
      "Iteration: 1419. Loss: 0.3341688811779022\n",
      "Iteration: 1420. Loss: 0.3104479908943176\n",
      "Iteration: 1421. Loss: 0.3296099901199341\n",
      "Iteration: 1422. Loss: 0.3564046621322632\n",
      "Iteration: 1423. Loss: 0.5049533247947693\n",
      "Iteration: 1424. Loss: 0.5316951274871826\n",
      "Iteration: 1425. Loss: 0.31946560740470886\n",
      "Iteration: 1426. Loss: 0.535760223865509\n",
      "Iteration: 1427. Loss: 0.36036789417266846\n",
      "Iteration: 1428. Loss: 0.34082886576652527\n",
      "Iteration: 1429. Loss: 0.29236114025115967\n",
      "Iteration: 1430. Loss: 0.34982913732528687\n",
      "Iteration: 1431. Loss: 0.44759732484817505\n",
      "Iteration: 1432. Loss: 0.3389121890068054\n",
      "Iteration: 1433. Loss: 0.4576154947280884\n",
      "Iteration: 1434. Loss: 0.2940497100353241\n",
      "Iteration: 1435. Loss: 0.28911206126213074\n",
      "Iteration: 1436. Loss: 0.6089816093444824\n",
      "Iteration: 1437. Loss: 0.48812851309776306\n",
      "Iteration: 1438. Loss: 0.3329770267009735\n",
      "Iteration: 1439. Loss: 0.30446192622184753\n",
      "Iteration: 1440. Loss: 0.28484416007995605\n",
      "Iteration: 1441. Loss: 0.4077100455760956\n",
      "Iteration: 1442. Loss: 0.32974669337272644\n",
      "Iteration: 1443. Loss: 0.37123799324035645\n",
      "Iteration: 1444. Loss: 0.5341914892196655\n",
      "Iteration: 1445. Loss: 0.26585474610328674\n",
      "Iteration: 1446. Loss: 0.40065938234329224\n",
      "Iteration: 1447. Loss: 0.3221660554409027\n",
      "Iteration: 1448. Loss: 0.37986984848976135\n",
      "Iteration: 1449. Loss: 0.2515562176704407\n",
      "Iteration: 1450. Loss: 0.44157132506370544\n",
      "Iteration: 1451. Loss: 0.34616878628730774\n",
      "Iteration: 1452. Loss: 0.47488078474998474\n",
      "Iteration: 1453. Loss: 0.36979228258132935\n",
      "Iteration: 1454. Loss: 0.3262433707714081\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1455. Loss: 0.2959161698818207\n",
      "Iteration: 1456. Loss: 0.3704148828983307\n",
      "Iteration: 1457. Loss: 0.2968137264251709\n",
      "Iteration: 1458. Loss: 0.38592633605003357\n",
      "Iteration: 1459. Loss: 0.298880934715271\n",
      "Iteration: 1460. Loss: 0.2701408565044403\n",
      "Iteration: 1461. Loss: 0.4562482535839081\n",
      "Iteration: 1462. Loss: 0.43439698219299316\n",
      "Iteration: 1463. Loss: 0.3830256760120392\n",
      "Iteration: 1464. Loss: 0.46645763516426086\n",
      "Iteration: 1465. Loss: 0.2969393730163574\n",
      "Iteration: 1466. Loss: 0.3346613347530365\n",
      "Iteration: 1467. Loss: 0.36408936977386475\n",
      "Iteration: 1468. Loss: 0.3263883888721466\n",
      "Iteration: 1469. Loss: 0.39352840185165405\n",
      "Iteration: 1470. Loss: 0.36592021584510803\n",
      "Iteration: 1471. Loss: 0.4214824438095093\n",
      "Iteration: 1472. Loss: 0.36582931876182556\n",
      "Iteration: 1473. Loss: 0.37773796916007996\n",
      "Iteration: 1474. Loss: 0.29600998759269714\n",
      "Iteration: 1475. Loss: 0.2827935516834259\n",
      "Iteration: 1476. Loss: 0.3019355833530426\n",
      "Iteration: 1477. Loss: 0.31764665246009827\n",
      "Iteration: 1478. Loss: 0.33096519112586975\n",
      "Iteration: 1479. Loss: 0.39676499366760254\n",
      "Iteration: 1480. Loss: 0.22961893677711487\n",
      "Iteration: 1481. Loss: 0.48326605558395386\n",
      "Iteration: 1482. Loss: 0.22174036502838135\n",
      "Iteration: 1483. Loss: 0.3166239559650421\n",
      "Iteration: 1484. Loss: 0.28688761591911316\n",
      "Iteration: 1485. Loss: 0.3305724859237671\n",
      "Iteration: 1486. Loss: 0.38871055841445923\n",
      "Iteration: 1487. Loss: 0.4174819588661194\n",
      "Iteration: 1488. Loss: 0.3194412887096405\n",
      "Iteration: 1489. Loss: 0.27702367305755615\n",
      "Iteration: 1490. Loss: 0.360469251871109\n",
      "Iteration: 1491. Loss: 0.421939492225647\n",
      "Iteration: 1492. Loss: 0.332861989736557\n",
      "Iteration: 1493. Loss: 0.3031198978424072\n",
      "Iteration: 1494. Loss: 0.3354948163032532\n",
      "Iteration: 1495. Loss: 0.5148200988769531\n",
      "Iteration: 1496. Loss: 0.3306163549423218\n",
      "Iteration: 1497. Loss: 0.3710950016975403\n",
      "Iteration: 1498. Loss: 0.23874090611934662\n",
      "Iteration: 1499. Loss: 0.39261844754219055\n",
      "Iteration: 1500. Loss: 0.3092832863330841\n",
      "Iteration: 1501. Loss: 0.41058510541915894\n",
      "Iteration: 1502. Loss: 0.3290766179561615\n",
      "Iteration: 1503. Loss: 0.4988771080970764\n",
      "Iteration: 1504. Loss: 0.35307061672210693\n",
      "Iteration: 1505. Loss: 0.34731608629226685\n",
      "Iteration: 1506. Loss: 0.2778949737548828\n",
      "Iteration: 1507. Loss: 0.41905713081359863\n",
      "Iteration: 1508. Loss: 0.4286938011646271\n",
      "Iteration: 1509. Loss: 0.35084807872772217\n",
      "Iteration: 1510. Loss: 0.37452325224876404\n",
      "Iteration: 1511. Loss: 0.34505757689476013\n",
      "Iteration: 1512. Loss: 0.33410751819610596\n",
      "Iteration: 1513. Loss: 0.4004085958003998\n",
      "Iteration: 1514. Loss: 0.2708689868450165\n",
      "Iteration: 1515. Loss: 0.4093264639377594\n",
      "Iteration: 1516. Loss: 0.4119158685207367\n",
      "Iteration: 1517. Loss: 0.38335981965065\n",
      "Iteration: 1518. Loss: 0.4362867474555969\n",
      "Iteration: 1519. Loss: 0.3782840073108673\n",
      "Iteration: 1520. Loss: 0.31896916031837463\n",
      "Iteration: 1521. Loss: 0.49313420057296753\n",
      "Iteration: 1522. Loss: 0.2947312295436859\n",
      "Iteration: 1523. Loss: 0.33310797810554504\n",
      "Iteration: 1524. Loss: 0.29074540734291077\n",
      "Iteration: 1525. Loss: 0.3038232922554016\n",
      "Iteration: 1526. Loss: 0.4830167293548584\n",
      "Iteration: 1527. Loss: 0.4020139276981354\n",
      "Iteration: 1528. Loss: 0.3252497911453247\n",
      "Iteration: 1529. Loss: 0.3906376361846924\n",
      "Iteration: 1530. Loss: 0.3676760792732239\n",
      "Iteration: 1531. Loss: 0.333841472864151\n",
      "Iteration: 1532. Loss: 0.2709485590457916\n",
      "Iteration: 1533. Loss: 0.3834115266799927\n",
      "Iteration: 1534. Loss: 0.327413946390152\n",
      "Iteration: 1535. Loss: 0.3979003131389618\n",
      "Iteration: 1536. Loss: 0.32072994112968445\n",
      "Iteration: 1537. Loss: 0.3154859244823456\n",
      "Iteration: 1538. Loss: 0.3687467873096466\n",
      "Iteration: 1539. Loss: 0.3196469843387604\n",
      "Iteration: 1540. Loss: 0.3763129413127899\n",
      "Iteration: 1541. Loss: 0.38096290826797485\n",
      "Iteration: 1542. Loss: 0.28686845302581787\n",
      "Iteration: 1543. Loss: 0.31336113810539246\n",
      "Iteration: 1544. Loss: 0.34228256344795227\n",
      "Iteration: 1545. Loss: 0.32602769136428833\n",
      "Iteration: 1546. Loss: 0.42218130826950073\n",
      "Iteration: 1547. Loss: 0.38093262910842896\n",
      "Iteration: 1548. Loss: 0.3451855778694153\n",
      "Iteration: 1549. Loss: 0.34552687406539917\n",
      "Iteration: 1550. Loss: 0.35903045535087585\n",
      "Iteration: 1551. Loss: 0.44283023476600647\n",
      "Iteration: 1552. Loss: 0.3994230628013611\n",
      "Iteration: 1553. Loss: 0.22685548663139343\n",
      "Iteration: 1554. Loss: 0.351119726896286\n",
      "Iteration: 1555. Loss: 0.4906090199947357\n",
      "Iteration: 1556. Loss: 0.27824464440345764\n",
      "Iteration: 1557. Loss: 0.34160315990448\n",
      "Iteration: 1558. Loss: 0.39947617053985596\n",
      "Iteration: 1559. Loss: 0.3787352442741394\n",
      "Iteration: 1560. Loss: 0.3395093083381653\n",
      "Iteration: 1561. Loss: 0.44258108735084534\n",
      "Iteration: 1562. Loss: 0.49506813287734985\n",
      "Iteration: 1563. Loss: 0.478305846452713\n",
      "Iteration: 1564. Loss: 0.4086647927761078\n",
      "Iteration: 1565. Loss: 0.39591076970100403\n",
      "Iteration: 1566. Loss: 0.4067400395870209\n",
      "Iteration: 1567. Loss: 0.38658297061920166\n",
      "Iteration: 1568. Loss: 0.30471354722976685\n",
      "Iteration: 1569. Loss: 0.5554303526878357\n",
      "Iteration: 1570. Loss: 0.3725632131099701\n",
      "Iteration: 1571. Loss: 0.3879167139530182\n",
      "Iteration: 1572. Loss: 0.28267136216163635\n",
      "Iteration: 1573. Loss: 0.4312395751476288\n",
      "Iteration: 1574. Loss: 0.31823399662971497\n",
      "Iteration: 1575. Loss: 0.35014137625694275\n",
      "Iteration: 1576. Loss: 0.4126395881175995\n",
      "Iteration: 1577. Loss: 0.39182618260383606\n",
      "Iteration: 1578. Loss: 0.31195080280303955\n",
      "Iteration: 1579. Loss: 0.3519650995731354\n",
      "Iteration: 1580. Loss: 0.3961111903190613\n",
      "Iteration: 1581. Loss: 0.30839669704437256\n",
      "Iteration: 1582. Loss: 0.36326584219932556\n",
      "Iteration: 1583. Loss: 0.3585513234138489\n",
      "Iteration: 1584. Loss: 0.4494326412677765\n",
      "Iteration: 1585. Loss: 0.40378338098526\n",
      "Iteration: 1586. Loss: 0.33790433406829834\n",
      "Iteration: 1587. Loss: 0.3263739049434662\n",
      "Iteration: 1588. Loss: 0.2535470724105835\n",
      "Iteration: 1589. Loss: 0.4872310161590576\n",
      "Iteration: 1590. Loss: 0.3356301784515381\n",
      "Iteration: 1591. Loss: 0.4104999303817749\n",
      "Iteration: 1592. Loss: 0.41458213329315186\n",
      "Iteration: 1593. Loss: 0.38524094223976135\n",
      "Iteration: 1594. Loss: 0.3309817910194397\n",
      "Iteration: 1595. Loss: 0.2974873483181\n",
      "Iteration: 1596. Loss: 0.3080347180366516\n",
      "Iteration: 1597. Loss: 0.2737138271331787\n",
      "Iteration: 1598. Loss: 0.29539579153060913\n",
      "Iteration: 1599. Loss: 0.29935699701309204\n",
      "Iteration: 1600. Loss: 0.34305495023727417\n",
      "Iteration: 1601. Loss: 0.4638138711452484\n",
      "Iteration: 1602. Loss: 0.3264876902103424\n",
      "Iteration: 1603. Loss: 0.3914707601070404\n",
      "Iteration: 1604. Loss: 0.3394005298614502\n",
      "Iteration: 1605. Loss: 0.33454689383506775\n",
      "Iteration: 1606. Loss: 0.42190733551979065\n",
      "Iteration: 1607. Loss: 0.3734823167324066\n",
      "Iteration: 1608. Loss: 0.330488920211792\n",
      "Iteration: 1609. Loss: 0.4298962354660034\n",
      "Iteration: 1610. Loss: 0.3737949728965759\n",
      "Iteration: 1611. Loss: 0.44245269894599915\n",
      "Iteration: 1612. Loss: 0.36391744017601013\n",
      "Iteration: 1613. Loss: 0.37215012311935425\n",
      "Iteration: 1614. Loss: 0.26857978105545044\n",
      "Iteration: 1615. Loss: 0.3570316731929779\n",
      "Iteration: 1616. Loss: 0.34184956550598145\n",
      "Iteration: 1617. Loss: 0.36186739802360535\n",
      "Iteration: 1618. Loss: 0.3989567160606384\n",
      "Iteration: 1619. Loss: 0.31565964221954346\n",
      "Iteration: 1620. Loss: 0.33199501037597656\n",
      "Iteration: 1621. Loss: 0.3236619532108307\n",
      "Iteration: 1622. Loss: 0.1944674700498581\n",
      "Iteration: 1623. Loss: 0.3452546298503876\n",
      "Iteration: 1624. Loss: 0.30022168159484863\n",
      "Iteration: 1625. Loss: 0.23288725316524506\n",
      "Iteration: 1626. Loss: 0.482601135969162\n",
      "Iteration: 1627. Loss: 0.4751538932323456\n",
      "Iteration: 1628. Loss: 0.39239656925201416\n",
      "Iteration: 1629. Loss: 0.36409762501716614\n",
      "Iteration: 1630. Loss: 0.3673689365386963\n",
      "Iteration: 1631. Loss: 0.3615594804286957\n",
      "Iteration: 1632. Loss: 0.33635666966438293\n",
      "Iteration: 1633. Loss: 0.3710155189037323\n",
      "Iteration: 1634. Loss: 0.281690388917923\n",
      "Iteration: 1635. Loss: 0.2999976873397827\n",
      "Iteration: 1636. Loss: 0.2588600814342499\n",
      "Iteration: 1637. Loss: 0.45753830671310425\n",
      "Iteration: 1638. Loss: 0.2727128863334656\n",
      "Iteration: 1639. Loss: 0.42307770252227783\n",
      "Iteration: 1640. Loss: 0.41185420751571655\n",
      "Iteration: 1641. Loss: 0.3135610818862915\n",
      "Iteration: 1642. Loss: 0.3151743412017822\n",
      "Iteration: 1643. Loss: 0.35831576585769653\n",
      "Iteration: 1644. Loss: 0.23191583156585693\n",
      "Iteration: 1645. Loss: 0.26814305782318115\n",
      "Iteration: 1646. Loss: 0.3353053331375122\n",
      "Iteration: 1647. Loss: 0.28999486565589905\n",
      "Iteration: 1648. Loss: 0.36055076122283936\n",
      "Iteration: 1649. Loss: 0.36274081468582153\n",
      "Iteration: 1650. Loss: 0.2725527882575989\n",
      "Iteration: 1651. Loss: 0.1684616655111313\n",
      "Iteration: 1652. Loss: 0.43926236033439636\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1653. Loss: 0.37343594431877136\n",
      "Iteration: 1654. Loss: 0.3449788987636566\n",
      "Iteration: 1655. Loss: 0.3415484130382538\n",
      "Iteration: 1656. Loss: 0.30292361974716187\n",
      "Iteration: 1657. Loss: 0.20590052008628845\n",
      "Iteration: 1658. Loss: 0.3928092122077942\n",
      "Iteration: 1659. Loss: 0.27405908703804016\n",
      "Iteration: 1660. Loss: 0.34442755579948425\n",
      "Iteration: 1661. Loss: 0.5328817963600159\n",
      "Iteration: 1662. Loss: 0.37186580896377563\n",
      "Iteration: 1663. Loss: 0.42166006565093994\n",
      "Iteration: 1664. Loss: 0.2929655611515045\n",
      "Iteration: 1665. Loss: 0.2951931059360504\n",
      "Iteration: 1666. Loss: 0.262233704328537\n",
      "Iteration: 1667. Loss: 0.35954749584198\n",
      "Iteration: 1668. Loss: 0.32925787568092346\n",
      "Iteration: 1669. Loss: 0.2707059383392334\n",
      "Iteration: 1670. Loss: 0.20416267216205597\n",
      "Iteration: 1671. Loss: 0.3657548427581787\n",
      "Iteration: 1672. Loss: 0.36378589272499084\n",
      "Iteration: 1673. Loss: 0.3351646363735199\n",
      "Iteration: 1674. Loss: 0.36287975311279297\n",
      "Iteration: 1675. Loss: 0.35118892788887024\n",
      "Iteration: 1676. Loss: 0.33323314785957336\n",
      "Iteration: 1677. Loss: 0.7606173753738403\n",
      "Iteration: 1678. Loss: 0.3492383062839508\n",
      "Iteration: 1679. Loss: 0.412759929895401\n",
      "Iteration: 1680. Loss: 0.19166283309459686\n",
      "Iteration: 1681. Loss: 0.3239027261734009\n",
      "Iteration: 1682. Loss: 0.2911733388900757\n",
      "Iteration: 1683. Loss: 0.2357579618692398\n",
      "Iteration: 1684. Loss: 0.37817493081092834\n",
      "Iteration: 1685. Loss: 0.2865113317966461\n",
      "Iteration: 1686. Loss: 0.25648242235183716\n",
      "Iteration: 1687. Loss: 0.36202767491340637\n",
      "Iteration: 1688. Loss: 0.3263864517211914\n",
      "Iteration: 1689. Loss: 0.3238953649997711\n",
      "Iteration: 1690. Loss: 0.3079560399055481\n",
      "Iteration: 1691. Loss: 0.3941306173801422\n",
      "Iteration: 1692. Loss: 0.3053344190120697\n",
      "Iteration: 1693. Loss: 0.26925724744796753\n",
      "Iteration: 1694. Loss: 0.29822686314582825\n",
      "Iteration: 1695. Loss: 0.3454357087612152\n",
      "Iteration: 1696. Loss: 0.39517441391944885\n",
      "Iteration: 1697. Loss: 0.41884124279022217\n",
      "Iteration: 1698. Loss: 0.414967805147171\n",
      "Iteration: 1699. Loss: 0.363475501537323\n",
      "Iteration: 1700. Loss: 0.3385944366455078\n",
      "Iteration: 1701. Loss: 0.2335163950920105\n",
      "Iteration: 1702. Loss: 0.26023221015930176\n",
      "Iteration: 1703. Loss: 0.3618948757648468\n",
      "Iteration: 1704. Loss: 0.26208266615867615\n",
      "Iteration: 1705. Loss: 0.26414215564727783\n",
      "Iteration: 1706. Loss: 0.40127575397491455\n",
      "Iteration: 1707. Loss: 0.2812504470348358\n",
      "Iteration: 1708. Loss: 0.428242027759552\n",
      "Iteration: 1709. Loss: 0.3319586217403412\n",
      "Iteration: 1710. Loss: 0.27734044194221497\n",
      "Iteration: 1711. Loss: 0.2071523517370224\n",
      "Iteration: 1712. Loss: 0.29656878113746643\n",
      "Iteration: 1713. Loss: 0.45444926619529724\n",
      "Iteration: 1714. Loss: 0.21890129148960114\n",
      "Iteration: 1715. Loss: 0.25885435938835144\n",
      "Iteration: 1716. Loss: 0.3894363343715668\n",
      "Iteration: 1717. Loss: 0.2586843967437744\n",
      "Iteration: 1718. Loss: 0.3237680196762085\n",
      "Iteration: 1719. Loss: 0.23967084288597107\n",
      "Iteration: 1720. Loss: 0.2763693332672119\n",
      "Iteration: 1721. Loss: 0.2263246774673462\n",
      "Iteration: 1722. Loss: 0.3539865016937256\n",
      "Iteration: 1723. Loss: 0.31496766209602356\n",
      "Iteration: 1724. Loss: 0.2551833391189575\n",
      "Iteration: 1725. Loss: 0.3279345631599426\n",
      "Iteration: 1726. Loss: 0.35719990730285645\n",
      "Iteration: 1727. Loss: 0.2633833587169647\n",
      "Iteration: 1728. Loss: 0.30048343539237976\n",
      "Iteration: 1729. Loss: 0.2190740704536438\n",
      "Iteration: 1730. Loss: 0.36275237798690796\n",
      "Iteration: 1731. Loss: 0.38326412439346313\n",
      "Iteration: 1732. Loss: 0.3827778995037079\n",
      "Iteration: 1733. Loss: 0.4388049840927124\n",
      "Iteration: 1734. Loss: 0.22281314432621002\n",
      "Iteration: 1735. Loss: 0.32562199234962463\n",
      "Iteration: 1736. Loss: 0.3320634365081787\n",
      "Iteration: 1737. Loss: 0.41704902052879333\n",
      "Iteration: 1738. Loss: 0.36005496978759766\n",
      "Iteration: 1739. Loss: 0.2645256519317627\n",
      "Iteration: 1740. Loss: 0.36385083198547363\n",
      "Iteration: 1741. Loss: 0.3068428933620453\n",
      "Iteration: 1742. Loss: 0.2463635951280594\n",
      "Iteration: 1743. Loss: 0.32067981362342834\n",
      "Iteration: 1744. Loss: 0.24720288813114166\n",
      "Iteration: 1745. Loss: 0.26142260432243347\n",
      "Iteration: 1746. Loss: 0.37151405215263367\n",
      "Iteration: 1747. Loss: 0.4554591476917267\n",
      "Iteration: 1748. Loss: 0.45942533016204834\n",
      "Iteration: 1749. Loss: 0.38729164004325867\n",
      "Iteration: 1750. Loss: 0.4668510854244232\n",
      "Iteration: 1751. Loss: 0.232282355427742\n",
      "Iteration: 1752. Loss: 0.33772000670433044\n",
      "Iteration: 1753. Loss: 0.4267752170562744\n",
      "Iteration: 1754. Loss: 0.4525882601737976\n",
      "Iteration: 1755. Loss: 0.29617345333099365\n",
      "Iteration: 1756. Loss: 0.2776865065097809\n",
      "Iteration: 1757. Loss: 0.2955659329891205\n",
      "Iteration: 1758. Loss: 0.4514249563217163\n",
      "Iteration: 1759. Loss: 0.43591731786727905\n",
      "Iteration: 1760. Loss: 0.4836617410182953\n",
      "Iteration: 1761. Loss: 0.32544684410095215\n",
      "Iteration: 1762. Loss: 0.22113248705863953\n",
      "Iteration: 1763. Loss: 0.32793429493904114\n",
      "Iteration: 1764. Loss: 0.34922751784324646\n",
      "Iteration: 1765. Loss: 0.35946136713027954\n",
      "Iteration: 1766. Loss: 0.4041479527950287\n",
      "Iteration: 1767. Loss: 0.38665148615837097\n",
      "Iteration: 1768. Loss: 0.35491788387298584\n",
      "Iteration: 1769. Loss: 0.29081490635871887\n",
      "Iteration: 1770. Loss: 0.3713904619216919\n",
      "Iteration: 1771. Loss: 0.3031509220600128\n",
      "Iteration: 1772. Loss: 0.2623556852340698\n",
      "Iteration: 1773. Loss: 0.3511430025100708\n",
      "Iteration: 1774. Loss: 0.344050794839859\n",
      "Iteration: 1775. Loss: 0.36442604660987854\n",
      "Iteration: 1776. Loss: 0.38968703150749207\n",
      "Iteration: 1777. Loss: 0.2545797824859619\n",
      "Iteration: 1778. Loss: 0.31504982709884644\n",
      "Iteration: 1779. Loss: 0.41449180245399475\n",
      "Iteration: 1780. Loss: 0.3455667495727539\n",
      "Iteration: 1781. Loss: 0.3327973186969757\n",
      "Iteration: 1782. Loss: 0.322828471660614\n",
      "Iteration: 1783. Loss: 0.3449713885784149\n",
      "Iteration: 1784. Loss: 0.38996219635009766\n",
      "Iteration: 1785. Loss: 0.2886596620082855\n",
      "Iteration: 1786. Loss: 0.379922091960907\n",
      "Iteration: 1787. Loss: 0.35549113154411316\n",
      "Iteration: 1788. Loss: 0.32765063643455505\n",
      "Iteration: 1789. Loss: 0.44418269395828247\n",
      "Iteration: 1790. Loss: 0.4046756327152252\n",
      "Iteration: 1791. Loss: 0.2828054428100586\n",
      "Iteration: 1792. Loss: 0.4115413427352905\n",
      "Iteration: 1793. Loss: 0.44326546788215637\n",
      "Iteration: 1794. Loss: 0.28741660714149475\n",
      "Iteration: 1795. Loss: 0.48665323853492737\n",
      "Iteration: 1796. Loss: 0.22005900740623474\n",
      "Iteration: 1797. Loss: 0.3410018980503082\n",
      "Iteration: 1798. Loss: 0.3074260652065277\n",
      "Iteration: 1799. Loss: 0.279988557100296\n",
      "Iteration: 1800. Loss: 0.35798221826553345\n",
      "Iteration: 1801. Loss: 0.23363332450389862\n",
      "Iteration: 1802. Loss: 0.22652770578861237\n",
      "Iteration: 1803. Loss: 0.22926099598407745\n",
      "Iteration: 1804. Loss: 0.22727441787719727\n",
      "Iteration: 1805. Loss: 0.22718273103237152\n",
      "Iteration: 1806. Loss: 0.3109116852283478\n",
      "Iteration: 1807. Loss: 0.40118205547332764\n",
      "Iteration: 1808. Loss: 0.2750219404697418\n",
      "Iteration: 1809. Loss: 0.29229235649108887\n",
      "Iteration: 1810. Loss: 0.29199573397636414\n",
      "Iteration: 1811. Loss: 0.3353949785232544\n",
      "Iteration: 1812. Loss: 0.3245197534561157\n",
      "Iteration: 1813. Loss: 0.4316675066947937\n",
      "Iteration: 1814. Loss: 0.35726672410964966\n",
      "Iteration: 1815. Loss: 0.4585932791233063\n",
      "Iteration: 1816. Loss: 0.4337621331214905\n",
      "Iteration: 1817. Loss: 0.2777855396270752\n",
      "Iteration: 1818. Loss: 0.3909466862678528\n",
      "Iteration: 1819. Loss: 0.22278495132923126\n",
      "Iteration: 1820. Loss: 0.2339116632938385\n",
      "Iteration: 1821. Loss: 0.3647819757461548\n",
      "Iteration: 1822. Loss: 0.28389307856559753\n",
      "Iteration: 1823. Loss: 0.45667487382888794\n",
      "Iteration: 1824. Loss: 0.3795934021472931\n",
      "Iteration: 1825. Loss: 0.30714476108551025\n",
      "Iteration: 1826. Loss: 0.258731871843338\n",
      "Iteration: 1827. Loss: 0.32284319400787354\n",
      "Iteration: 1828. Loss: 0.31387701630592346\n",
      "Iteration: 1829. Loss: 0.2647054195404053\n",
      "Iteration: 1830. Loss: 0.47022366523742676\n",
      "Iteration: 1831. Loss: 0.4671224355697632\n",
      "Iteration: 1832. Loss: 0.384872168302536\n",
      "Iteration: 1833. Loss: 0.30930835008621216\n",
      "Iteration: 1834. Loss: 0.366509348154068\n",
      "Iteration: 1835. Loss: 0.41412875056266785\n",
      "Iteration: 1836. Loss: 0.23707315325737\n",
      "Iteration: 1837. Loss: 0.26311537623405457\n",
      "Iteration: 1838. Loss: 0.27941426634788513\n",
      "Iteration: 1839. Loss: 0.29522326588630676\n",
      "Iteration: 1840. Loss: 0.3634093403816223\n",
      "Iteration: 1841. Loss: 0.28955259919166565\n",
      "Iteration: 1842. Loss: 0.3239293694496155\n",
      "Iteration: 1843. Loss: 0.2685052752494812\n",
      "Iteration: 1844. Loss: 0.2863313853740692\n",
      "Iteration: 1845. Loss: 0.3182132840156555\n",
      "Iteration: 1846. Loss: 0.3288906514644623\n",
      "Iteration: 1847. Loss: 0.304282009601593\n",
      "Iteration: 1848. Loss: 0.34016066789627075\n",
      "Iteration: 1849. Loss: 0.314693421125412\n",
      "Iteration: 1850. Loss: 0.33457377552986145\n",
      "Iteration: 1851. Loss: 0.3564925491809845\n",
      "Iteration: 1852. Loss: 0.27066662907600403\n",
      "Iteration: 1853. Loss: 0.2574847340583801\n",
      "Iteration: 1854. Loss: 0.22592328488826752\n",
      "Iteration: 1855. Loss: 0.3521137237548828\n",
      "Iteration: 1856. Loss: 0.39691856503486633\n",
      "Iteration: 1857. Loss: 0.2711560130119324\n",
      "Iteration: 1858. Loss: 0.23985610902309418\n",
      "Iteration: 1859. Loss: 0.5231999158859253\n",
      "Iteration: 1860. Loss: 0.40239888429641724\n",
      "Iteration: 1861. Loss: 0.3478057384490967\n",
      "Iteration: 1862. Loss: 0.28589117527008057\n",
      "Iteration: 1863. Loss: 0.25423863530158997\n",
      "Iteration: 1864. Loss: 0.3580532371997833\n",
      "Iteration: 1865. Loss: 0.494057834148407\n",
      "Iteration: 1866. Loss: 0.2120518535375595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1867. Loss: 0.2711459696292877\n",
      "Iteration: 1868. Loss: 0.2654101848602295\n",
      "Iteration: 1869. Loss: 0.3505314886569977\n",
      "Iteration: 1870. Loss: 0.2707112431526184\n",
      "Iteration: 1871. Loss: 0.4699013829231262\n",
      "Iteration: 1872. Loss: 0.3288533091545105\n",
      "Iteration: 1873. Loss: 0.3533875346183777\n",
      "Iteration: 1874. Loss: 0.3931267261505127\n",
      "Iteration: 1875. Loss: 0.27975162863731384\n",
      "Iteration: 1876. Loss: 0.39696186780929565\n",
      "Iteration: 1877. Loss: 0.4325062036514282\n",
      "Iteration: 1878. Loss: 0.4740581214427948\n",
      "Iteration: 1879. Loss: 0.37247562408447266\n",
      "Iteration: 1880. Loss: 0.40440404415130615\n",
      "Iteration: 1881. Loss: 0.3329533636569977\n",
      "Iteration: 1882. Loss: 0.3353099822998047\n",
      "Iteration: 1883. Loss: 0.2291007786989212\n",
      "Iteration: 1884. Loss: 0.4248652756214142\n",
      "Iteration: 1885. Loss: 0.2833453118801117\n",
      "Iteration: 1886. Loss: 0.31345418095588684\n",
      "Iteration: 1887. Loss: 0.410416841506958\n",
      "Iteration: 1888. Loss: 0.37446126341819763\n",
      "Iteration: 1889. Loss: 0.2894538938999176\n",
      "Iteration: 1890. Loss: 0.3757031559944153\n",
      "Iteration: 1891. Loss: 0.24954910576343536\n",
      "Iteration: 1892. Loss: 0.22182853519916534\n",
      "Iteration: 1893. Loss: 0.3540708124637604\n",
      "Iteration: 1894. Loss: 0.1954186111688614\n",
      "Iteration: 1895. Loss: 0.3408121168613434\n",
      "Iteration: 1896. Loss: 0.34379467368125916\n",
      "Iteration: 1897. Loss: 0.2950200140476227\n",
      "Iteration: 1898. Loss: 0.3606162667274475\n",
      "Iteration: 1899. Loss: 0.30023738741874695\n",
      "Iteration: 1900. Loss: 0.23714371025562286\n",
      "Iteration: 1901. Loss: 0.31814810633659363\n",
      "Iteration: 1902. Loss: 0.45168647170066833\n",
      "Iteration: 1903. Loss: 0.2687832713127136\n",
      "Iteration: 1904. Loss: 0.4459508955478668\n",
      "Iteration: 1905. Loss: 0.3144986927509308\n",
      "Iteration: 1906. Loss: 0.34369540214538574\n",
      "Iteration: 1907. Loss: 0.41955992579460144\n",
      "Iteration: 1908. Loss: 0.33331599831581116\n",
      "Iteration: 1909. Loss: 0.3534563481807709\n",
      "Iteration: 1910. Loss: 0.27962690591812134\n",
      "Iteration: 1911. Loss: 0.26636794209480286\n",
      "Iteration: 1912. Loss: 0.20038078725337982\n",
      "Iteration: 1913. Loss: 0.275846928358078\n",
      "Iteration: 1914. Loss: 0.3247646391391754\n",
      "Iteration: 1915. Loss: 0.2973659932613373\n",
      "Iteration: 1916. Loss: 0.2776297628879547\n",
      "Iteration: 1917. Loss: 0.32426583766937256\n",
      "Iteration: 1918. Loss: 0.45382651686668396\n",
      "Iteration: 1919. Loss: 0.36943379044532776\n",
      "Iteration: 1920. Loss: 0.3556337058544159\n",
      "Iteration: 1921. Loss: 0.35514596104621887\n",
      "Iteration: 1922. Loss: 0.24572768807411194\n",
      "Iteration: 1923. Loss: 0.3502996563911438\n",
      "Iteration: 1924. Loss: 0.40949881076812744\n",
      "Iteration: 1925. Loss: 0.4107455015182495\n",
      "Iteration: 1926. Loss: 0.2947191894054413\n",
      "Iteration: 1927. Loss: 0.3417205512523651\n",
      "Iteration: 1928. Loss: 0.29174017906188965\n",
      "Iteration: 1929. Loss: 0.33617088198661804\n",
      "Iteration: 1930. Loss: 0.2771683633327484\n",
      "Iteration: 1931. Loss: 0.29413047432899475\n",
      "Iteration: 1932. Loss: 0.3080352544784546\n",
      "Iteration: 1933. Loss: 0.34008848667144775\n",
      "Iteration: 1934. Loss: 0.3016221523284912\n",
      "Iteration: 1935. Loss: 0.36437058448791504\n",
      "Iteration: 1936. Loss: 0.23275375366210938\n",
      "Iteration: 1937. Loss: 0.306318074464798\n",
      "Iteration: 1938. Loss: 0.3620647192001343\n",
      "Iteration: 1939. Loss: 0.36191612482070923\n",
      "Iteration: 1940. Loss: 0.4352000951766968\n",
      "Iteration: 1941. Loss: 0.3748786449432373\n",
      "Iteration: 1942. Loss: 0.24490989744663239\n",
      "Iteration: 1943. Loss: 0.40277040004730225\n",
      "Iteration: 1944. Loss: 0.43713465332984924\n",
      "Iteration: 1945. Loss: 0.3573545217514038\n",
      "Iteration: 1946. Loss: 0.364114373922348\n",
      "Iteration: 1947. Loss: 0.19985921680927277\n",
      "Iteration: 1948. Loss: 0.23436608910560608\n",
      "Iteration: 1949. Loss: 0.3935420513153076\n",
      "Iteration: 1950. Loss: 0.27433958649635315\n",
      "Iteration: 1951. Loss: 0.31487467885017395\n",
      "Iteration: 1952. Loss: 0.2967934012413025\n",
      "Iteration: 1953. Loss: 0.3091496527194977\n",
      "Iteration: 1954. Loss: 0.42355239391326904\n",
      "Iteration: 1955. Loss: 0.35046085715293884\n",
      "Iteration: 1956. Loss: 0.29297783970832825\n",
      "Iteration: 1957. Loss: 0.2119290977716446\n",
      "Iteration: 1958. Loss: 0.2891652286052704\n",
      "Iteration: 1959. Loss: 0.4099810719490051\n",
      "Iteration: 1960. Loss: 0.1575281172990799\n",
      "Iteration: 1961. Loss: 0.3033403158187866\n",
      "Iteration: 1962. Loss: 0.25709474086761475\n",
      "Iteration: 1963. Loss: 0.3095274269580841\n",
      "Iteration: 1964. Loss: 0.31212177872657776\n",
      "Iteration: 1965. Loss: 0.3472389876842499\n",
      "Iteration: 1966. Loss: 0.35667774081230164\n",
      "Iteration: 1967. Loss: 0.28761398792266846\n",
      "Iteration: 1968. Loss: 0.39670971035957336\n",
      "Iteration: 1969. Loss: 0.34045469760894775\n",
      "Iteration: 1970. Loss: 0.27880412340164185\n",
      "Iteration: 1971. Loss: 0.4109210968017578\n",
      "Iteration: 1972. Loss: 0.3252013921737671\n",
      "Iteration: 1973. Loss: 0.2340608835220337\n",
      "Iteration: 1974. Loss: 0.31409379839897156\n",
      "Iteration: 1975. Loss: 0.371489018201828\n",
      "Iteration: 1976. Loss: 0.3616727888584137\n",
      "Iteration: 1977. Loss: 0.28563931584358215\n",
      "Iteration: 1978. Loss: 0.3450254499912262\n",
      "Iteration: 1979. Loss: 0.3311336040496826\n",
      "Iteration: 1980. Loss: 0.2859271168708801\n",
      "Iteration: 1981. Loss: 0.2200751155614853\n",
      "Iteration: 1982. Loss: 0.36390483379364014\n",
      "Iteration: 1983. Loss: 0.4348216652870178\n",
      "Iteration: 1984. Loss: 0.2473018318414688\n",
      "Iteration: 1985. Loss: 0.29859891533851624\n",
      "Iteration: 1986. Loss: 0.3913879096508026\n",
      "Iteration: 1987. Loss: 0.4049036502838135\n",
      "Iteration: 1988. Loss: 0.2982446253299713\n",
      "Iteration: 1989. Loss: 0.2863539755344391\n",
      "Iteration: 1990. Loss: 0.19596272706985474\n",
      "Iteration: 1991. Loss: 0.2564261257648468\n",
      "Iteration: 1992. Loss: 0.34527096152305603\n",
      "Iteration: 1993. Loss: 0.33532822132110596\n",
      "Iteration: 1994. Loss: 0.3974775969982147\n",
      "Iteration: 1995. Loss: 0.19059982895851135\n",
      "Iteration: 1996. Loss: 0.3314533531665802\n",
      "Iteration: 1997. Loss: 0.2613316774368286\n",
      "Iteration: 1998. Loss: 0.38040870428085327\n",
      "Iteration: 1999. Loss: 0.2834330201148987\n",
      "Iteration: 2000. Loss: 0.3593187630176544\n",
      "Iteration: 2001. Loss: 0.28994834423065186\n",
      "Iteration: 2002. Loss: 0.4701412320137024\n",
      "Iteration: 2003. Loss: 0.22994908690452576\n",
      "Iteration: 2004. Loss: 0.25969958305358887\n",
      "Iteration: 2005. Loss: 0.40924689173698425\n",
      "Iteration: 2006. Loss: 0.2657089829444885\n",
      "Iteration: 2007. Loss: 0.35258814692497253\n",
      "Iteration: 2008. Loss: 0.250142902135849\n",
      "Iteration: 2009. Loss: 0.29495248198509216\n",
      "Iteration: 2010. Loss: 0.31064265966415405\n",
      "Iteration: 2011. Loss: 0.40486663579940796\n",
      "Iteration: 2012. Loss: 0.3189617991447449\n",
      "Iteration: 2013. Loss: 0.2608106732368469\n",
      "Iteration: 2014. Loss: 0.3229444921016693\n",
      "Iteration: 2015. Loss: 0.23216472566127777\n",
      "Iteration: 2016. Loss: 0.34335196018218994\n",
      "Iteration: 2017. Loss: 0.3027133047580719\n",
      "Iteration: 2018. Loss: 0.4223636984825134\n",
      "Iteration: 2019. Loss: 0.3264130651950836\n",
      "Iteration: 2020. Loss: 0.3910103738307953\n",
      "Iteration: 2021. Loss: 0.3472251892089844\n",
      "Iteration: 2022. Loss: 0.3728410601615906\n",
      "Iteration: 2023. Loss: 0.33055630326271057\n",
      "Iteration: 2024. Loss: 0.3313365578651428\n",
      "Iteration: 2025. Loss: 0.29190224409103394\n",
      "Iteration: 2026. Loss: 0.2283361405134201\n",
      "Iteration: 2027. Loss: 0.25316178798675537\n",
      "Iteration: 2028. Loss: 0.21381472051143646\n",
      "Iteration: 2029. Loss: 0.24026735126972198\n",
      "Iteration: 2030. Loss: 0.2240944355726242\n",
      "Iteration: 2031. Loss: 0.226565420627594\n",
      "Iteration: 2032. Loss: 0.22262948751449585\n",
      "Iteration: 2033. Loss: 0.29298174381256104\n",
      "Iteration: 2034. Loss: 0.21278730034828186\n",
      "Iteration: 2035. Loss: 0.33992764353752136\n",
      "Iteration: 2036. Loss: 0.4028838276863098\n",
      "Iteration: 2037. Loss: 0.40362659096717834\n",
      "Iteration: 2038. Loss: 0.32736530900001526\n",
      "Iteration: 2039. Loss: 0.31015902757644653\n",
      "Iteration: 2040. Loss: 0.35393455624580383\n",
      "Iteration: 2041. Loss: 0.37343746423721313\n",
      "Iteration: 2042. Loss: 0.1772957295179367\n",
      "Iteration: 2043. Loss: 0.24882468581199646\n",
      "Iteration: 2044. Loss: 0.4165618419647217\n",
      "Iteration: 2045. Loss: 0.3369293212890625\n",
      "Iteration: 2046. Loss: 0.32410022616386414\n",
      "Iteration: 2047. Loss: 0.43731489777565\n",
      "Iteration: 2048. Loss: 0.23324626684188843\n",
      "Iteration: 2049. Loss: 0.20431718230247498\n",
      "Iteration: 2050. Loss: 0.31727689504623413\n",
      "Iteration: 2051. Loss: 0.3959643244743347\n",
      "Iteration: 2052. Loss: 0.2839670479297638\n",
      "Iteration: 2053. Loss: 0.2975429892539978\n",
      "Iteration: 2054. Loss: 0.35614514350891113\n",
      "Iteration: 2055. Loss: 0.4051136076450348\n",
      "Iteration: 2056. Loss: 0.2589656114578247\n",
      "Iteration: 2057. Loss: 0.461581826210022\n",
      "Iteration: 2058. Loss: 0.4422472417354584\n",
      "Iteration: 2059. Loss: 0.4361850321292877\n",
      "Iteration: 2060. Loss: 0.22968314588069916\n",
      "Iteration: 2061. Loss: 0.2835337817668915\n",
      "Iteration: 2062. Loss: 0.28630056977272034\n",
      "Iteration: 2063. Loss: 0.33521682024002075\n",
      "Iteration: 2064. Loss: 0.2753509283065796\n",
      "Iteration: 2065. Loss: 0.4696977138519287\n",
      "Iteration: 2066. Loss: 0.28399479389190674\n",
      "Iteration: 2067. Loss: 0.28343114256858826\n",
      "Iteration: 2068. Loss: 0.22939838469028473\n",
      "Iteration: 2069. Loss: 0.23466220498085022\n",
      "Iteration: 2070. Loss: 0.3516526520252228\n",
      "Iteration: 2071. Loss: 0.24535900354385376\n",
      "Iteration: 2072. Loss: 0.4402320384979248\n",
      "Iteration: 2073. Loss: 0.5336179137229919\n",
      "Iteration: 2074. Loss: 0.27796924114227295\n",
      "Iteration: 2075. Loss: 0.3314785063266754\n",
      "Iteration: 2076. Loss: 0.18209128081798553\n",
      "Iteration: 2077. Loss: 0.30404335260391235\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2078. Loss: 0.36961764097213745\n",
      "Iteration: 2079. Loss: 0.31677618622779846\n",
      "Iteration: 2080. Loss: 0.4368476867675781\n",
      "Iteration: 2081. Loss: 0.31272923946380615\n",
      "Iteration: 2082. Loss: 0.3579224646091461\n",
      "Iteration: 2083. Loss: 0.2237183004617691\n",
      "Iteration: 2084. Loss: 0.3621537387371063\n",
      "Iteration: 2085. Loss: 0.36381983757019043\n",
      "Iteration: 2086. Loss: 0.26515814661979675\n",
      "Iteration: 2087. Loss: 0.3104361891746521\n",
      "Iteration: 2088. Loss: 0.48016557097435\n",
      "Iteration: 2089. Loss: 0.6228853464126587\n",
      "Iteration: 2090. Loss: 0.2775813043117523\n",
      "Iteration: 2091. Loss: 0.2431110441684723\n",
      "Iteration: 2092. Loss: 0.4794829189777374\n",
      "Iteration: 2093. Loss: 0.27959078550338745\n",
      "Iteration: 2094. Loss: 0.38796278834342957\n",
      "Iteration: 2095. Loss: 0.4076635241508484\n",
      "Iteration: 2096. Loss: 0.348231703042984\n",
      "Iteration: 2097. Loss: 0.3699076175689697\n",
      "Iteration: 2098. Loss: 0.29012951254844666\n",
      "Iteration: 2099. Loss: 0.31395336985588074\n",
      "Iteration: 2100. Loss: 0.27506959438323975\n",
      "Iteration: 2101. Loss: 0.32765257358551025\n",
      "Iteration: 2102. Loss: 0.5408759117126465\n",
      "Iteration: 2103. Loss: 0.3783479928970337\n",
      "Iteration: 2104. Loss: 0.305532842874527\n",
      "Iteration: 2105. Loss: 0.39692768454551697\n",
      "Iteration: 2106. Loss: 0.1935449242591858\n",
      "Iteration: 2107. Loss: 0.16152727603912354\n",
      "Iteration: 2108. Loss: 0.3370342552661896\n",
      "Iteration: 2109. Loss: 0.2394675612449646\n",
      "Iteration: 2110. Loss: 0.343927800655365\n",
      "Iteration: 2111. Loss: 0.45020848512649536\n",
      "Iteration: 2112. Loss: 0.40428388118743896\n",
      "Iteration: 2113. Loss: 0.35973548889160156\n",
      "Iteration: 2114. Loss: 0.19642652571201324\n",
      "Iteration: 2115. Loss: 0.31053268909454346\n",
      "Iteration: 2116. Loss: 0.4343049228191376\n",
      "Iteration: 2117. Loss: 0.29295629262924194\n",
      "Iteration: 2118. Loss: 0.3953128159046173\n",
      "Iteration: 2119. Loss: 0.2845580577850342\n",
      "Iteration: 2120. Loss: 0.2896370589733124\n",
      "Iteration: 2121. Loss: 0.2785360813140869\n",
      "Iteration: 2122. Loss: 0.3221491277217865\n",
      "Iteration: 2123. Loss: 0.5613993406295776\n",
      "Iteration: 2124. Loss: 0.384609192609787\n",
      "Iteration: 2125. Loss: 0.35171839594841003\n",
      "Iteration: 2126. Loss: 0.3548022210597992\n",
      "Iteration: 2127. Loss: 0.2264930009841919\n",
      "Iteration: 2128. Loss: 0.34693580865859985\n",
      "Iteration: 2129. Loss: 0.4730784595012665\n",
      "Iteration: 2130. Loss: 0.3027246296405792\n",
      "Iteration: 2131. Loss: 0.303733766078949\n",
      "Iteration: 2132. Loss: 0.4198891818523407\n",
      "Iteration: 2133. Loss: 0.27312564849853516\n",
      "Iteration: 2134. Loss: 0.22752995789051056\n",
      "Iteration: 2135. Loss: 0.3854962885379791\n",
      "Iteration: 2136. Loss: 0.4168211817741394\n",
      "Iteration: 2137. Loss: 0.35349440574645996\n",
      "Iteration: 2138. Loss: 0.31532198190689087\n",
      "Iteration: 2139. Loss: 0.23984509706497192\n",
      "Iteration: 2140. Loss: 0.23457038402557373\n",
      "Iteration: 2141. Loss: 0.22669337689876556\n",
      "Iteration: 2142. Loss: 0.3315519392490387\n",
      "Iteration: 2143. Loss: 0.3572940528392792\n",
      "Iteration: 2144. Loss: 0.2636149227619171\n",
      "Iteration: 2145. Loss: 0.2691950798034668\n",
      "Iteration: 2146. Loss: 0.32919588685035706\n",
      "Iteration: 2147. Loss: 0.2778250575065613\n",
      "Iteration: 2148. Loss: 0.3125717043876648\n",
      "Iteration: 2149. Loss: 0.22055751085281372\n",
      "Iteration: 2150. Loss: 0.35910069942474365\n",
      "Iteration: 2151. Loss: 0.33151185512542725\n",
      "Iteration: 2152. Loss: 0.28379717469215393\n",
      "Iteration: 2153. Loss: 0.2862378656864166\n",
      "Iteration: 2154. Loss: 0.3250230848789215\n",
      "Iteration: 2155. Loss: 0.3635753393173218\n",
      "Iteration: 2156. Loss: 0.2643069922924042\n",
      "Iteration: 2157. Loss: 0.31957799196243286\n",
      "Iteration: 2158. Loss: 0.5003954172134399\n",
      "Iteration: 2159. Loss: 0.46479493379592896\n",
      "Iteration: 2160. Loss: 0.3967399597167969\n",
      "Iteration: 2161. Loss: 0.21375206112861633\n",
      "Iteration: 2162. Loss: 0.3158773183822632\n",
      "Iteration: 2163. Loss: 0.2622228264808655\n",
      "Iteration: 2164. Loss: 0.47474679350852966\n",
      "Iteration: 2165. Loss: 0.2572985589504242\n",
      "Iteration: 2166. Loss: 0.38959676027297974\n",
      "Iteration: 2167. Loss: 0.3336614966392517\n",
      "Iteration: 2168. Loss: 0.2868417203426361\n",
      "Iteration: 2169. Loss: 0.2765689790248871\n",
      "Iteration: 2170. Loss: 0.5007120370864868\n",
      "Iteration: 2171. Loss: 0.2742561995983124\n",
      "Iteration: 2172. Loss: 0.39603325724601746\n",
      "Iteration: 2173. Loss: 0.3815590739250183\n",
      "Iteration: 2174. Loss: 0.4540368318557739\n",
      "Iteration: 2175. Loss: 0.22108317911624908\n",
      "Iteration: 2176. Loss: 0.29548409581184387\n",
      "Iteration: 2177. Loss: 0.197295680642128\n",
      "Iteration: 2178. Loss: 0.28627118468284607\n",
      "Iteration: 2179. Loss: 0.3000911772251129\n",
      "Iteration: 2180. Loss: 0.35464155673980713\n",
      "Iteration: 2181. Loss: 0.3713609278202057\n",
      "Iteration: 2182. Loss: 0.2694662809371948\n",
      "Iteration: 2183. Loss: 0.2090974599123001\n",
      "Iteration: 2184. Loss: 0.3659631013870239\n",
      "Iteration: 2185. Loss: 0.3804131746292114\n",
      "Iteration: 2186. Loss: 0.2997172772884369\n",
      "Iteration: 2187. Loss: 0.23364515602588654\n",
      "Iteration: 2188. Loss: 0.3178739845752716\n",
      "Iteration: 2189. Loss: 0.437181293964386\n",
      "Iteration: 2190. Loss: 0.2885853052139282\n",
      "Iteration: 2191. Loss: 0.375299870967865\n",
      "Iteration: 2192. Loss: 0.4748353660106659\n",
      "Iteration: 2193. Loss: 0.18977341055870056\n",
      "Iteration: 2194. Loss: 0.4370601177215576\n",
      "Iteration: 2195. Loss: 0.38261497020721436\n",
      "Iteration: 2196. Loss: 0.1741381138563156\n",
      "Iteration: 2197. Loss: 0.3767307996749878\n",
      "Iteration: 2198. Loss: 0.2911536693572998\n",
      "Iteration: 2199. Loss: 0.4896293878555298\n",
      "Iteration: 2200. Loss: 0.32264384627342224\n",
      "Iteration: 2201. Loss: 0.38186630606651306\n",
      "Iteration: 2202. Loss: 0.3279428780078888\n",
      "Iteration: 2203. Loss: 0.32677119970321655\n",
      "Iteration: 2204. Loss: 0.42574089765548706\n",
      "Iteration: 2205. Loss: 0.30313706398010254\n",
      "Iteration: 2206. Loss: 0.43159258365631104\n",
      "Iteration: 2207. Loss: 0.26236793398857117\n",
      "Iteration: 2208. Loss: 0.20901548862457275\n",
      "Iteration: 2209. Loss: 0.2862156927585602\n",
      "Iteration: 2210. Loss: 0.3175351917743683\n",
      "Iteration: 2211. Loss: 0.2546435594558716\n",
      "Iteration: 2212. Loss: 0.45263469219207764\n",
      "Iteration: 2213. Loss: 0.3228432834148407\n",
      "Iteration: 2214. Loss: 0.31817060708999634\n",
      "Iteration: 2215. Loss: 0.3772337734699249\n",
      "Iteration: 2216. Loss: 0.31930282711982727\n",
      "Iteration: 2217. Loss: 0.3652506172657013\n",
      "Iteration: 2218. Loss: 0.3387874364852905\n",
      "Iteration: 2219. Loss: 0.3483726382255554\n",
      "Iteration: 2220. Loss: 0.4063183069229126\n",
      "Iteration: 2221. Loss: 0.3178032338619232\n",
      "Iteration: 2222. Loss: 0.2520037591457367\n",
      "Iteration: 2223. Loss: 0.4106884002685547\n",
      "Iteration: 2224. Loss: 0.2694534957408905\n",
      "Iteration: 2225. Loss: 0.3338586688041687\n",
      "Iteration: 2226. Loss: 0.33350226283073425\n",
      "Iteration: 2227. Loss: 0.28070172667503357\n",
      "Iteration: 2228. Loss: 0.3337874710559845\n",
      "Iteration: 2229. Loss: 0.22344867885112762\n",
      "Iteration: 2230. Loss: 0.25090014934539795\n",
      "Iteration: 2231. Loss: 0.3473643958568573\n",
      "Iteration: 2232. Loss: 0.44257378578186035\n",
      "Iteration: 2233. Loss: 0.299501895904541\n",
      "Iteration: 2234. Loss: 0.2613763213157654\n",
      "Iteration: 2235. Loss: 0.41070955991744995\n",
      "Iteration: 2236. Loss: 0.2815447151660919\n",
      "Iteration: 2237. Loss: 0.4004821479320526\n",
      "Iteration: 2238. Loss: 0.5271924734115601\n",
      "Iteration: 2239. Loss: 0.2974720299243927\n",
      "Iteration: 2240. Loss: 0.42450645565986633\n",
      "Iteration: 2241. Loss: 0.3317551910877228\n",
      "Iteration: 2242. Loss: 0.3494124710559845\n",
      "Iteration: 2243. Loss: 0.21536481380462646\n",
      "Iteration: 2244. Loss: 0.22629927098751068\n",
      "Iteration: 2245. Loss: 0.3201681077480316\n",
      "Iteration: 2246. Loss: 0.25541409850120544\n",
      "Iteration: 2247. Loss: 0.14634931087493896\n",
      "Iteration: 2248. Loss: 0.18710261583328247\n",
      "Iteration: 2249. Loss: 0.28142136335372925\n",
      "Iteration: 2250. Loss: 0.32066214084625244\n",
      "Iteration: 2251. Loss: 0.32385098934173584\n",
      "Iteration: 2252. Loss: 0.2601219415664673\n",
      "Iteration: 2253. Loss: 0.32737740874290466\n",
      "Iteration: 2254. Loss: 0.35589438676834106\n",
      "Iteration: 2255. Loss: 0.28719770908355713\n",
      "Iteration: 2256. Loss: 0.3031729757785797\n",
      "Iteration: 2257. Loss: 0.2724994421005249\n",
      "Iteration: 2258. Loss: 0.36440810561180115\n",
      "Iteration: 2259. Loss: 0.4071107804775238\n",
      "Iteration: 2260. Loss: 0.36752647161483765\n",
      "Iteration: 2261. Loss: 0.2970310151576996\n",
      "Iteration: 2262. Loss: 0.2149844616651535\n",
      "Iteration: 2263. Loss: 0.3409985303878784\n",
      "Iteration: 2264. Loss: 0.476217120885849\n",
      "Iteration: 2265. Loss: 0.4261859059333801\n",
      "Iteration: 2266. Loss: 0.27854597568511963\n",
      "Iteration: 2267. Loss: 0.3514167070388794\n",
      "Iteration: 2268. Loss: 0.24450618028640747\n",
      "Iteration: 2269. Loss: 0.3598974943161011\n",
      "Iteration: 2270. Loss: 0.2811623215675354\n",
      "Iteration: 2271. Loss: 0.24256347119808197\n",
      "Iteration: 2272. Loss: 0.35532665252685547\n",
      "Iteration: 2273. Loss: 0.37907809019088745\n",
      "Iteration: 2274. Loss: 0.284100741147995\n",
      "Iteration: 2275. Loss: 0.29543811082839966\n",
      "Iteration: 2276. Loss: 0.23955322802066803\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2277. Loss: 0.1674550175666809\n",
      "Iteration: 2278. Loss: 0.3037598133087158\n",
      "Iteration: 2279. Loss: 0.3193477690219879\n",
      "Iteration: 2280. Loss: 0.38434749841690063\n",
      "Iteration: 2281. Loss: 0.14721909165382385\n",
      "Iteration: 2282. Loss: 0.24777071177959442\n",
      "Iteration: 2283. Loss: 0.24655655026435852\n",
      "Iteration: 2284. Loss: 0.30291253328323364\n",
      "Iteration: 2285. Loss: 0.2217530459165573\n",
      "Iteration: 2286. Loss: 0.25336551666259766\n",
      "Iteration: 2287. Loss: 0.26935309171676636\n",
      "Iteration: 2288. Loss: 0.26950713992118835\n",
      "Iteration: 2289. Loss: 0.29431530833244324\n",
      "Iteration: 2290. Loss: 0.3108353316783905\n",
      "Iteration: 2291. Loss: 0.25324809551239014\n",
      "Iteration: 2292. Loss: 0.36036959290504456\n",
      "Iteration: 2293. Loss: 0.3768494129180908\n",
      "Iteration: 2294. Loss: 0.28511136770248413\n",
      "Iteration: 2295. Loss: 0.26731741428375244\n",
      "Iteration: 2296. Loss: 0.389940083026886\n",
      "Iteration: 2297. Loss: 0.2685624361038208\n",
      "Iteration: 2298. Loss: 0.27409541606903076\n",
      "Iteration: 2299. Loss: 0.30762845277786255\n",
      "Iteration: 2300. Loss: 0.2714356780052185\n",
      "Iteration: 2301. Loss: 0.39190518856048584\n",
      "Iteration: 2302. Loss: 0.38720741868019104\n",
      "Iteration: 2303. Loss: 0.2524489164352417\n",
      "Iteration: 2304. Loss: 0.4173820912837982\n",
      "Iteration: 2305. Loss: 0.4526395797729492\n",
      "Iteration: 2306. Loss: 0.25300338864326477\n",
      "Iteration: 2307. Loss: 0.37694084644317627\n",
      "Iteration: 2308. Loss: 0.24967730045318604\n",
      "Iteration: 2309. Loss: 0.23643475770950317\n",
      "Iteration: 2310. Loss: 0.23118919134140015\n",
      "Iteration: 2311. Loss: 0.3711817264556885\n",
      "Iteration: 2312. Loss: 0.19921539723873138\n",
      "Iteration: 2313. Loss: 0.22343122959136963\n",
      "Iteration: 2314. Loss: 0.24708586931228638\n",
      "Iteration: 2315. Loss: 0.3627921938896179\n",
      "Iteration: 2316. Loss: 0.3575180470943451\n",
      "Iteration: 2317. Loss: 0.1805572360754013\n",
      "Iteration: 2318. Loss: 0.4317888021469116\n",
      "Iteration: 2319. Loss: 0.4307304322719574\n",
      "Iteration: 2320. Loss: 0.18952928483486176\n",
      "Iteration: 2321. Loss: 0.2683371305465698\n",
      "Iteration: 2322. Loss: 0.2633568346500397\n",
      "Iteration: 2323. Loss: 0.34524139761924744\n",
      "Iteration: 2324. Loss: 0.21320368349552155\n",
      "Iteration: 2325. Loss: 0.2495316118001938\n",
      "Iteration: 2326. Loss: 0.3261924684047699\n",
      "Iteration: 2327. Loss: 0.2675360143184662\n",
      "Iteration: 2328. Loss: 0.24287566542625427\n",
      "Iteration: 2329. Loss: 0.37261322140693665\n",
      "Iteration: 2330. Loss: 0.3179551661014557\n",
      "Iteration: 2331. Loss: 0.22482334077358246\n",
      "Iteration: 2332. Loss: 0.2908276617527008\n",
      "Iteration: 2333. Loss: 0.2673571705818176\n",
      "Iteration: 2334. Loss: 0.3598196506500244\n",
      "Iteration: 2335. Loss: 0.42591872811317444\n",
      "Iteration: 2336. Loss: 0.2496735155582428\n",
      "Iteration: 2337. Loss: 0.40063342452049255\n",
      "Iteration: 2338. Loss: 0.456917405128479\n",
      "Iteration: 2339. Loss: 0.33835041522979736\n",
      "Iteration: 2340. Loss: 0.27524495124816895\n",
      "Iteration: 2341. Loss: 0.4469895660877228\n",
      "Iteration: 2342. Loss: 0.39823979139328003\n",
      "Iteration: 2343. Loss: 0.3111066222190857\n",
      "Iteration: 2344. Loss: 0.4843789339065552\n",
      "Iteration: 2345. Loss: 0.4592159688472748\n",
      "Iteration: 2346. Loss: 0.26971736550331116\n",
      "Iteration: 2347. Loss: 0.34713053703308105\n",
      "Iteration: 2348. Loss: 0.40498051047325134\n",
      "Iteration: 2349. Loss: 0.2551058828830719\n",
      "Iteration: 2350. Loss: 0.26681485772132874\n",
      "Iteration: 2351. Loss: 0.267507940530777\n",
      "Iteration: 2352. Loss: 0.3294532299041748\n",
      "Iteration: 2353. Loss: 0.2247738391160965\n",
      "Iteration: 2354. Loss: 0.2226988971233368\n",
      "Iteration: 2355. Loss: 0.3650068938732147\n",
      "Iteration: 2356. Loss: 0.3767603635787964\n",
      "Iteration: 2357. Loss: 0.3526880741119385\n",
      "Iteration: 2358. Loss: 0.3360595703125\n",
      "Iteration: 2359. Loss: 0.3788641691207886\n",
      "Iteration: 2360. Loss: 0.45228832960128784\n",
      "Iteration: 2361. Loss: 0.29728972911834717\n",
      "Iteration: 2362. Loss: 0.31689053773880005\n",
      "Iteration: 2363. Loss: 0.33265745639801025\n",
      "Iteration: 2364. Loss: 0.40728676319122314\n",
      "Iteration: 2365. Loss: 0.2502056062221527\n",
      "Iteration: 2366. Loss: 0.3298763632774353\n",
      "Iteration: 2367. Loss: 0.22165007889270782\n",
      "Iteration: 2368. Loss: 0.4243188202381134\n",
      "Iteration: 2369. Loss: 0.39517760276794434\n",
      "Iteration: 2370. Loss: 0.27317240834236145\n",
      "Iteration: 2371. Loss: 0.36319831013679504\n",
      "Iteration: 2372. Loss: 0.44611087441444397\n",
      "Iteration: 2373. Loss: 0.35928359627723694\n",
      "Iteration: 2374. Loss: 0.2624284625053406\n",
      "Iteration: 2375. Loss: 0.39883747696876526\n",
      "Iteration: 2376. Loss: 0.3403979539871216\n",
      "Iteration: 2377. Loss: 0.2925374507904053\n",
      "Iteration: 2378. Loss: 0.19953937828540802\n",
      "Iteration: 2379. Loss: 0.325743705034256\n",
      "Iteration: 2380. Loss: 0.32564330101013184\n",
      "Iteration: 2381. Loss: 0.2848745584487915\n",
      "Iteration: 2382. Loss: 0.4498494267463684\n",
      "Iteration: 2383. Loss: 0.2529440224170685\n",
      "Iteration: 2384. Loss: 0.19686202704906464\n",
      "Iteration: 2385. Loss: 0.3051607012748718\n",
      "Iteration: 2386. Loss: 0.32189396023750305\n",
      "Iteration: 2387. Loss: 0.5007774829864502\n",
      "Iteration: 2388. Loss: 0.33589836955070496\n",
      "Iteration: 2389. Loss: 0.3788946568965912\n",
      "Iteration: 2390. Loss: 0.3198586702346802\n",
      "Iteration: 2391. Loss: 0.28258639574050903\n",
      "Iteration: 2392. Loss: 0.2540493607521057\n",
      "Iteration: 2393. Loss: 0.2187454104423523\n",
      "Iteration: 2394. Loss: 0.3089170753955841\n",
      "Iteration: 2395. Loss: 0.34277480840682983\n",
      "Iteration: 2396. Loss: 0.37440070509910583\n",
      "Iteration: 2397. Loss: 0.28292784094810486\n",
      "Iteration: 2398. Loss: 0.3298281729221344\n",
      "Iteration: 2399. Loss: 0.310481995344162\n",
      "Iteration: 2400. Loss: 0.29113858938217163\n",
      "Iteration: 2401. Loss: 0.28906330466270447\n",
      "Iteration: 2402. Loss: 0.28808602690696716\n",
      "Iteration: 2403. Loss: 0.22691544890403748\n",
      "Iteration: 2404. Loss: 0.39473778009414673\n",
      "Iteration: 2405. Loss: 0.3313743472099304\n",
      "Iteration: 2406. Loss: 0.30779746174812317\n",
      "Iteration: 2407. Loss: 0.2997640073299408\n",
      "Iteration: 2408. Loss: 0.2590288817882538\n",
      "Iteration: 2409. Loss: 0.27289530634880066\n",
      "Iteration: 2410. Loss: 0.25439324975013733\n",
      "Iteration: 2411. Loss: 0.2755258083343506\n",
      "Iteration: 2412. Loss: 0.3370073735713959\n",
      "Iteration: 2413. Loss: 0.2921220362186432\n",
      "Iteration: 2414. Loss: 0.26929140090942383\n",
      "Iteration: 2415. Loss: 0.27991053462028503\n",
      "Iteration: 2416. Loss: 0.2970256507396698\n",
      "Iteration: 2417. Loss: 0.3239024877548218\n",
      "Iteration: 2418. Loss: 0.281621515750885\n",
      "Iteration: 2419. Loss: 0.2412455976009369\n",
      "Iteration: 2420. Loss: 0.26546356081962585\n",
      "Iteration: 2421. Loss: 0.2246917337179184\n",
      "Iteration: 2422. Loss: 0.44492438435554504\n",
      "Iteration: 2423. Loss: 0.3647478222846985\n",
      "Iteration: 2424. Loss: 0.2457789182662964\n",
      "Iteration: 2425. Loss: 0.3252663016319275\n",
      "Iteration: 2426. Loss: 0.26318562030792236\n",
      "Iteration: 2427. Loss: 0.2414340376853943\n",
      "Iteration: 2428. Loss: 0.36661091446876526\n",
      "Iteration: 2429. Loss: 0.17420825362205505\n",
      "Iteration: 2430. Loss: 0.3017948269844055\n",
      "Iteration: 2431. Loss: 0.23118704557418823\n",
      "Iteration: 2432. Loss: 0.2633785605430603\n",
      "Iteration: 2433. Loss: 0.2660723626613617\n",
      "Iteration: 2434. Loss: 0.2716982662677765\n",
      "Iteration: 2435. Loss: 0.3913067579269409\n",
      "Iteration: 2436. Loss: 0.2790060043334961\n",
      "Iteration: 2437. Loss: 0.29957064986228943\n",
      "Iteration: 2438. Loss: 0.23064760863780975\n",
      "Iteration: 2439. Loss: 0.2661189138889313\n",
      "Iteration: 2440. Loss: 0.24409954249858856\n",
      "Iteration: 2441. Loss: 0.38614216446876526\n",
      "Iteration: 2442. Loss: 0.30928683280944824\n",
      "Iteration: 2443. Loss: 0.32821837067604065\n",
      "Iteration: 2444. Loss: 0.25380218029022217\n",
      "Iteration: 2445. Loss: 0.38089829683303833\n",
      "Iteration: 2446. Loss: 0.16678284108638763\n",
      "Iteration: 2447. Loss: 0.22444623708724976\n",
      "Iteration: 2448. Loss: 0.35752981901168823\n",
      "Iteration: 2449. Loss: 0.41683128476142883\n",
      "Iteration: 2450. Loss: 0.25272712111473083\n",
      "Iteration: 2451. Loss: 0.2607467770576477\n",
      "Iteration: 2452. Loss: 0.27052339911460876\n",
      "Iteration: 2453. Loss: 0.44794005155563354\n",
      "Iteration: 2454. Loss: 0.23284630477428436\n",
      "Iteration: 2455. Loss: 0.48948967456817627\n",
      "Iteration: 2456. Loss: 0.4485352039337158\n",
      "Iteration: 2457. Loss: 0.3630700707435608\n",
      "Iteration: 2458. Loss: 0.16728155314922333\n",
      "Iteration: 2459. Loss: 0.3795062303543091\n",
      "Iteration: 2460. Loss: 0.2858676612377167\n",
      "Iteration: 2461. Loss: 0.36047571897506714\n",
      "Iteration: 2462. Loss: 0.22311484813690186\n",
      "Iteration: 2463. Loss: 0.37671810388565063\n",
      "Iteration: 2464. Loss: 0.2605437934398651\n",
      "Iteration: 2465. Loss: 0.35219642519950867\n",
      "Iteration: 2466. Loss: 0.23721741139888763\n",
      "Iteration: 2467. Loss: 0.21831122040748596\n",
      "Iteration: 2468. Loss: 0.3901636004447937\n",
      "Iteration: 2469. Loss: 0.3286901116371155\n",
      "Iteration: 2470. Loss: 0.23312564194202423\n",
      "Iteration: 2471. Loss: 0.29998645186424255\n",
      "Iteration: 2472. Loss: 0.4253782331943512\n",
      "Iteration: 2473. Loss: 0.24313503503799438\n",
      "Iteration: 2474. Loss: 0.3303910195827484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2475. Loss: 0.3914056718349457\n",
      "Iteration: 2476. Loss: 0.17490415275096893\n",
      "Iteration: 2477. Loss: 0.45467203855514526\n",
      "Iteration: 2478. Loss: 0.23914474248886108\n",
      "Iteration: 2479. Loss: 0.3664010167121887\n",
      "Iteration: 2480. Loss: 0.27422112226486206\n",
      "Iteration: 2481. Loss: 0.23651760816574097\n",
      "Iteration: 2482. Loss: 0.3156583905220032\n",
      "Iteration: 2483. Loss: 0.2628858983516693\n",
      "Iteration: 2484. Loss: 0.4810916781425476\n",
      "Iteration: 2485. Loss: 0.3492995500564575\n",
      "Iteration: 2486. Loss: 0.35735023021698\n",
      "Iteration: 2487. Loss: 0.39133220911026\n",
      "Iteration: 2488. Loss: 0.4842740595340729\n",
      "Iteration: 2489. Loss: 0.4706918001174927\n",
      "Iteration: 2490. Loss: 0.4141179621219635\n",
      "Iteration: 2491. Loss: 0.2636478543281555\n",
      "Iteration: 2492. Loss: 0.18650022149085999\n",
      "Iteration: 2493. Loss: 0.2538665533065796\n",
      "Iteration: 2494. Loss: 0.2592374086380005\n",
      "Iteration: 2495. Loss: 0.2263849675655365\n",
      "Iteration: 2496. Loss: 0.27241846919059753\n",
      "Iteration: 2497. Loss: 0.29315292835235596\n",
      "Iteration: 2498. Loss: 0.19134113192558289\n",
      "Iteration: 2499. Loss: 0.31539595127105713\n",
      "Iteration: 2500. Loss: 0.20703943073749542\n",
      "Iteration: 2501. Loss: 0.3254077434539795\n",
      "Iteration: 2502. Loss: 0.2619750499725342\n",
      "Iteration: 2503. Loss: 0.25453129410743713\n",
      "Iteration: 2504. Loss: 0.26576992869377136\n",
      "Iteration: 2505. Loss: 0.3445255756378174\n",
      "Iteration: 2506. Loss: 0.39175233244895935\n",
      "Iteration: 2507. Loss: 0.30658021569252014\n",
      "Iteration: 2508. Loss: 0.3705442547798157\n",
      "Iteration: 2509. Loss: 0.34213438630104065\n",
      "Iteration: 2510. Loss: 0.2601858079433441\n",
      "Iteration: 2511. Loss: 0.3520141839981079\n",
      "Iteration: 2512. Loss: 0.29420825839042664\n",
      "Iteration: 2513. Loss: 0.35716208815574646\n",
      "Iteration: 2514. Loss: 0.4646461606025696\n",
      "Iteration: 2515. Loss: 0.46880412101745605\n",
      "Iteration: 2516. Loss: 0.28685441613197327\n",
      "Iteration: 2517. Loss: 0.32991114258766174\n",
      "Iteration: 2518. Loss: 0.5359602570533752\n",
      "Iteration: 2519. Loss: 0.30438536405563354\n",
      "Iteration: 2520. Loss: 0.28614211082458496\n",
      "Iteration: 2521. Loss: 0.30756068229675293\n",
      "Iteration: 2522. Loss: 0.33108606934547424\n",
      "Iteration: 2523. Loss: 0.29761263728141785\n",
      "Iteration: 2524. Loss: 0.37125420570373535\n",
      "Iteration: 2525. Loss: 0.3733283579349518\n",
      "Iteration: 2526. Loss: 0.1752289980649948\n",
      "Iteration: 2527. Loss: 0.3267405033111572\n",
      "Iteration: 2528. Loss: 0.3698238730430603\n",
      "Iteration: 2529. Loss: 0.35945895314216614\n",
      "Iteration: 2530. Loss: 0.3492010235786438\n",
      "Iteration: 2531. Loss: 0.27634134888648987\n",
      "Iteration: 2532. Loss: 0.3198656737804413\n",
      "Iteration: 2533. Loss: 0.4077681005001068\n",
      "Iteration: 2534. Loss: 0.2482692003250122\n",
      "Iteration: 2535. Loss: 0.34858158230781555\n",
      "Iteration: 2536. Loss: 0.2855997085571289\n",
      "Iteration: 2537. Loss: 0.2294161170721054\n",
      "Iteration: 2538. Loss: 0.29652824997901917\n",
      "Iteration: 2539. Loss: 0.30989691615104675\n",
      "Iteration: 2540. Loss: 0.45270827412605286\n",
      "Iteration: 2541. Loss: 0.41837650537490845\n",
      "Iteration: 2542. Loss: 0.268034428358078\n",
      "Iteration: 2543. Loss: 0.24847738444805145\n",
      "Iteration: 2544. Loss: 0.3116030693054199\n",
      "Iteration: 2545. Loss: 0.3680647611618042\n",
      "Iteration: 2546. Loss: 0.3937465250492096\n",
      "Iteration: 2547. Loss: 0.27788764238357544\n",
      "Iteration: 2548. Loss: 0.31488147377967834\n",
      "Iteration: 2549. Loss: 0.31297022104263306\n",
      "Iteration: 2550. Loss: 0.24620553851127625\n",
      "Iteration: 2551. Loss: 0.3775157630443573\n",
      "Iteration: 2552. Loss: 0.37400394678115845\n",
      "Iteration: 2553. Loss: 0.45890045166015625\n",
      "Iteration: 2554. Loss: 0.45836716890335083\n",
      "Iteration: 2555. Loss: 0.36120185256004333\n",
      "Iteration: 2556. Loss: 0.5046160221099854\n",
      "Iteration: 2557. Loss: 0.30206671357154846\n",
      "Iteration: 2558. Loss: 0.42799389362335205\n",
      "Iteration: 2559. Loss: 0.25469741225242615\n",
      "Iteration: 2560. Loss: 0.29516226053237915\n",
      "Iteration: 2561. Loss: 0.3855932652950287\n",
      "Iteration: 2562. Loss: 0.29241591691970825\n",
      "Iteration: 2563. Loss: 0.2402673363685608\n",
      "Iteration: 2564. Loss: 0.22452642023563385\n",
      "Iteration: 2565. Loss: 0.2806396782398224\n",
      "Iteration: 2566. Loss: 0.17981019616127014\n",
      "Iteration: 2567. Loss: 0.2940364181995392\n",
      "Iteration: 2568. Loss: 0.24042613804340363\n",
      "Iteration: 2569. Loss: 0.3348504304885864\n",
      "Iteration: 2570. Loss: 0.2575451731681824\n",
      "Iteration: 2571. Loss: 0.34645870327949524\n",
      "Iteration: 2572. Loss: 0.3398490250110626\n",
      "Iteration: 2573. Loss: 0.4102259576320648\n",
      "Iteration: 2574. Loss: 0.29582005739212036\n",
      "Iteration: 2575. Loss: 0.33425673842430115\n",
      "Iteration: 2576. Loss: 0.38546401262283325\n",
      "Iteration: 2577. Loss: 0.25777411460876465\n",
      "Iteration: 2578. Loss: 0.27870461344718933\n",
      "Iteration: 2579. Loss: 0.17692427337169647\n",
      "Iteration: 2580. Loss: 0.2635161876678467\n",
      "Iteration: 2581. Loss: 0.2908953130245209\n",
      "Iteration: 2582. Loss: 0.4538786709308624\n",
      "Iteration: 2583. Loss: 0.33807089924812317\n",
      "Iteration: 2584. Loss: 0.4707918167114258\n",
      "Iteration: 2585. Loss: 0.4294934570789337\n",
      "Iteration: 2586. Loss: 0.17811276018619537\n",
      "Iteration: 2587. Loss: 0.255550742149353\n",
      "Iteration: 2588. Loss: 0.35108157992362976\n",
      "Iteration: 2589. Loss: 0.2813689410686493\n",
      "Iteration: 2590. Loss: 0.2714524567127228\n",
      "Iteration: 2591. Loss: 0.1815853863954544\n",
      "Iteration: 2592. Loss: 0.2544260323047638\n",
      "Iteration: 2593. Loss: 0.4808793365955353\n",
      "Iteration: 2594. Loss: 0.23774349689483643\n",
      "Iteration: 2595. Loss: 0.37051695585250854\n",
      "Iteration: 2596. Loss: 0.3512563705444336\n",
      "Iteration: 2597. Loss: 0.24546247720718384\n",
      "Iteration: 2598. Loss: 0.35473114252090454\n",
      "Iteration: 2599. Loss: 0.27600207924842834\n",
      "Iteration: 2600. Loss: 0.34304946660995483\n",
      "Iteration: 2601. Loss: 0.24476270377635956\n",
      "Iteration: 2602. Loss: 0.23394156992435455\n",
      "Iteration: 2603. Loss: 0.1798001527786255\n",
      "Iteration: 2604. Loss: 0.5042257308959961\n",
      "Iteration: 2605. Loss: 0.3221711814403534\n",
      "Iteration: 2606. Loss: 0.2642049491405487\n",
      "Iteration: 2607. Loss: 0.25840216875076294\n",
      "Iteration: 2608. Loss: 0.2792738974094391\n",
      "Iteration: 2609. Loss: 0.39322203397750854\n",
      "Iteration: 2610. Loss: 0.2133425921201706\n",
      "Iteration: 2611. Loss: 0.39864951372146606\n",
      "Iteration: 2612. Loss: 0.2368582785129547\n",
      "Iteration: 2613. Loss: 0.4445393681526184\n",
      "Iteration: 2614. Loss: 0.2841942310333252\n",
      "Iteration: 2615. Loss: 0.4160360097885132\n",
      "Iteration: 2616. Loss: 0.2381238490343094\n",
      "Iteration: 2617. Loss: 0.4233959913253784\n",
      "Iteration: 2618. Loss: 0.5176730155944824\n",
      "Iteration: 2619. Loss: 0.21015052497386932\n",
      "Iteration: 2620. Loss: 0.266514390707016\n",
      "Iteration: 2621. Loss: 0.2393425554037094\n",
      "Iteration: 2622. Loss: 0.40892237424850464\n",
      "Iteration: 2623. Loss: 0.32017582654953003\n",
      "Iteration: 2624. Loss: 0.15062269568443298\n",
      "Iteration: 2625. Loss: 0.27691105008125305\n",
      "Iteration: 2626. Loss: 0.26933449506759644\n",
      "Iteration: 2627. Loss: 0.2985756993293762\n",
      "Iteration: 2628. Loss: 0.3949863910675049\n",
      "Iteration: 2629. Loss: 0.30356332659721375\n",
      "Iteration: 2630. Loss: 0.34482038021087646\n",
      "Iteration: 2631. Loss: 0.34702908992767334\n",
      "Iteration: 2632. Loss: 0.30903586745262146\n",
      "Iteration: 2633. Loss: 0.397190660238266\n",
      "Iteration: 2634. Loss: 0.3107844293117523\n",
      "Iteration: 2635. Loss: 0.5132614970207214\n",
      "Iteration: 2636. Loss: 0.26191461086273193\n",
      "Iteration: 2637. Loss: 0.3063899874687195\n",
      "Iteration: 2638. Loss: 0.288824200630188\n",
      "Iteration: 2639. Loss: 0.19423063099384308\n",
      "Iteration: 2640. Loss: 0.2977982461452484\n",
      "Iteration: 2641. Loss: 0.24468502402305603\n",
      "Iteration: 2642. Loss: 0.29108527302742004\n",
      "Iteration: 2643. Loss: 0.3365780711174011\n",
      "Iteration: 2644. Loss: 0.15455922484397888\n",
      "Iteration: 2645. Loss: 0.20942097902297974\n",
      "Iteration: 2646. Loss: 0.28466492891311646\n",
      "Iteration: 2647. Loss: 0.44841551780700684\n",
      "Iteration: 2648. Loss: 0.3341917097568512\n",
      "Iteration: 2649. Loss: 0.2802594304084778\n",
      "Iteration: 2650. Loss: 0.245270773768425\n",
      "Iteration: 2651. Loss: 0.21695713698863983\n",
      "Iteration: 2652. Loss: 0.30343374609947205\n",
      "Iteration: 2653. Loss: 0.27427971363067627\n",
      "Iteration: 2654. Loss: 0.24857273697853088\n",
      "Iteration: 2655. Loss: 0.33696219325065613\n",
      "Iteration: 2656. Loss: 0.3097549080848694\n",
      "Iteration: 2657. Loss: 0.3637642562389374\n",
      "Iteration: 2658. Loss: 0.18055622279644012\n",
      "Iteration: 2659. Loss: 0.28901463747024536\n",
      "Iteration: 2660. Loss: 0.32183781266212463\n",
      "Iteration: 2661. Loss: 0.24410146474838257\n",
      "Iteration: 2662. Loss: 0.24188973009586334\n",
      "Iteration: 2663. Loss: 0.23859819769859314\n",
      "Iteration: 2664. Loss: 0.43475788831710815\n",
      "Iteration: 2665. Loss: 0.3294885754585266\n",
      "Iteration: 2666. Loss: 0.19916030764579773\n",
      "Iteration: 2667. Loss: 0.27356600761413574\n",
      "Iteration: 2668. Loss: 0.3273850977420807\n",
      "Iteration: 2669. Loss: 0.29091307520866394\n",
      "Iteration: 2670. Loss: 0.2510242164134979\n",
      "Iteration: 2671. Loss: 0.16813252866268158\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2672. Loss: 0.35843127965927124\n",
      "Iteration: 2673. Loss: 0.2904261648654938\n",
      "Iteration: 2674. Loss: 0.32956787943840027\n",
      "Iteration: 2675. Loss: 0.3871360123157501\n",
      "Iteration: 2676. Loss: 0.3662886321544647\n",
      "Iteration: 2677. Loss: 0.18077851831912994\n",
      "Iteration: 2678. Loss: 0.3260897696018219\n",
      "Iteration: 2679. Loss: 0.3989192247390747\n",
      "Iteration: 2680. Loss: 0.464405357837677\n",
      "Iteration: 2681. Loss: 0.27213191986083984\n",
      "Iteration: 2682. Loss: 0.3255651891231537\n",
      "Iteration: 2683. Loss: 0.3070930242538452\n",
      "Iteration: 2684. Loss: 0.3555421829223633\n",
      "Iteration: 2685. Loss: 0.21577636897563934\n",
      "Iteration: 2686. Loss: 0.3634416460990906\n",
      "Iteration: 2687. Loss: 0.21460431814193726\n",
      "Iteration: 2688. Loss: 0.37809914350509644\n",
      "Iteration: 2689. Loss: 0.36593472957611084\n",
      "Iteration: 2690. Loss: 0.49985024333000183\n",
      "Iteration: 2691. Loss: 0.2595117688179016\n",
      "Iteration: 2692. Loss: 0.24973121285438538\n",
      "Iteration: 2693. Loss: 0.42682212591171265\n",
      "Iteration: 2694. Loss: 0.2842774987220764\n",
      "Iteration: 2695. Loss: 0.243003249168396\n",
      "Iteration: 2696. Loss: 0.3811488449573517\n",
      "Iteration: 2697. Loss: 0.3507906198501587\n",
      "Iteration: 2698. Loss: 0.33921077847480774\n",
      "Iteration: 2699. Loss: 0.28855976462364197\n",
      "Iteration: 2700. Loss: 0.33139264583587646\n",
      "Iteration: 2701. Loss: 0.32407572865486145\n",
      "Iteration: 2702. Loss: 0.40367579460144043\n",
      "Iteration: 2703. Loss: 0.32082444429397583\n",
      "Iteration: 2704. Loss: 0.2039124071598053\n",
      "Iteration: 2705. Loss: 0.2573082447052002\n",
      "Iteration: 2706. Loss: 0.2719155251979828\n",
      "Iteration: 2707. Loss: 0.27005892992019653\n",
      "Iteration: 2708. Loss: 0.345729798078537\n",
      "Iteration: 2709. Loss: 0.36158981919288635\n",
      "Iteration: 2710. Loss: 0.3392481207847595\n",
      "Iteration: 2711. Loss: 0.2651694416999817\n",
      "Iteration: 2712. Loss: 0.3846222758293152\n",
      "Iteration: 2713. Loss: 0.5580412745475769\n",
      "Iteration: 2714. Loss: 0.44506192207336426\n",
      "Iteration: 2715. Loss: 0.2560696601867676\n",
      "Iteration: 2716. Loss: 0.30473485589027405\n",
      "Iteration: 2717. Loss: 0.2497953176498413\n",
      "Iteration: 2718. Loss: 0.2978249490261078\n",
      "Iteration: 2719. Loss: 0.2457253783941269\n",
      "Iteration: 2720. Loss: 0.422585129737854\n",
      "Iteration: 2721. Loss: 0.33342283964157104\n",
      "Iteration: 2722. Loss: 0.14031066000461578\n",
      "Iteration: 2723. Loss: 0.2957971692085266\n",
      "Iteration: 2724. Loss: 0.27066177129745483\n",
      "Iteration: 2725. Loss: 0.4526746869087219\n",
      "Iteration: 2726. Loss: 0.32455894351005554\n",
      "Iteration: 2727. Loss: 0.2764981985092163\n",
      "Iteration: 2728. Loss: 0.37420403957366943\n",
      "Iteration: 2729. Loss: 0.2202073037624359\n",
      "Iteration: 2730. Loss: 0.2639094889163971\n",
      "Iteration: 2731. Loss: 0.21711891889572144\n",
      "Iteration: 2732. Loss: 0.20002864301204681\n",
      "Iteration: 2733. Loss: 0.3078845143318176\n",
      "Iteration: 2734. Loss: 0.3813677132129669\n",
      "Iteration: 2735. Loss: 0.4511469602584839\n",
      "Iteration: 2736. Loss: 0.3081552982330322\n",
      "Iteration: 2737. Loss: 0.28907322883605957\n",
      "Iteration: 2738. Loss: 0.3140876293182373\n",
      "Iteration: 2739. Loss: 0.4643000066280365\n",
      "Iteration: 2740. Loss: 0.21277713775634766\n",
      "Iteration: 2741. Loss: 0.22471891343593597\n",
      "Iteration: 2742. Loss: 0.3274478614330292\n",
      "Iteration: 2743. Loss: 0.1982460916042328\n",
      "Iteration: 2744. Loss: 0.2771339416503906\n",
      "Iteration: 2745. Loss: 0.1746540069580078\n",
      "Iteration: 2746. Loss: 0.2969385087490082\n",
      "Iteration: 2747. Loss: 0.25825706124305725\n",
      "Iteration: 2748. Loss: 0.4277880787849426\n",
      "Iteration: 2749. Loss: 0.13219492137432098\n",
      "Iteration: 2750. Loss: 0.48785072565078735\n",
      "Iteration: 2751. Loss: 0.16078267991542816\n",
      "Iteration: 2752. Loss: 0.35219353437423706\n",
      "Iteration: 2753. Loss: 0.2778874337673187\n",
      "Iteration: 2754. Loss: 0.3524368405342102\n",
      "Iteration: 2755. Loss: 0.21904882788658142\n",
      "Iteration: 2756. Loss: 0.30090758204460144\n",
      "Iteration: 2757. Loss: 0.4213138818740845\n",
      "Iteration: 2758. Loss: 0.36768031120300293\n",
      "Iteration: 2759. Loss: 0.2403036504983902\n",
      "Iteration: 2760. Loss: 0.3782704472541809\n",
      "Iteration: 2761. Loss: 0.3694295585155487\n",
      "Iteration: 2762. Loss: 0.21180815994739532\n",
      "Iteration: 2763. Loss: 0.22020216286182404\n",
      "Iteration: 2764. Loss: 0.2558092176914215\n",
      "Iteration: 2765. Loss: 0.30045104026794434\n",
      "Iteration: 2766. Loss: 0.4040062725543976\n",
      "Iteration: 2767. Loss: 0.40786129236221313\n",
      "Iteration: 2768. Loss: 0.2657542824745178\n",
      "Iteration: 2769. Loss: 0.35676687955856323\n",
      "Iteration: 2770. Loss: 0.38631701469421387\n",
      "Iteration: 2771. Loss: 0.3543882369995117\n",
      "Iteration: 2772. Loss: 0.3057498633861542\n",
      "Iteration: 2773. Loss: 0.2855116128921509\n",
      "Iteration: 2774. Loss: 0.35287198424339294\n",
      "Iteration: 2775. Loss: 0.4048629403114319\n",
      "Iteration: 2776. Loss: 0.33956313133239746\n",
      "Iteration: 2777. Loss: 0.23990868031978607\n",
      "Iteration: 2778. Loss: 0.29969489574432373\n",
      "Iteration: 2779. Loss: 0.2287633717060089\n",
      "Iteration: 2780. Loss: 0.3294815123081207\n",
      "Iteration: 2781. Loss: 0.432792067527771\n",
      "Iteration: 2782. Loss: 0.41803690791130066\n",
      "Iteration: 2783. Loss: 0.2920750379562378\n",
      "Iteration: 2784. Loss: 0.3093034327030182\n",
      "Iteration: 2785. Loss: 0.30238816142082214\n",
      "Iteration: 2786. Loss: 0.25165581703186035\n",
      "Iteration: 2787. Loss: 0.20446757972240448\n",
      "Iteration: 2788. Loss: 0.24226944148540497\n",
      "Iteration: 2789. Loss: 0.25513729453086853\n",
      "Iteration: 2790. Loss: 0.2530786395072937\n",
      "Iteration: 2791. Loss: 0.21344798803329468\n",
      "Iteration: 2792. Loss: 0.2406633198261261\n",
      "Iteration: 2793. Loss: 0.21441249549388885\n",
      "Iteration: 2794. Loss: 0.26345884799957275\n",
      "Iteration: 2795. Loss: 0.2627072036266327\n",
      "Iteration: 2796. Loss: 0.39353203773498535\n",
      "Iteration: 2797. Loss: 0.26995956897735596\n",
      "Iteration: 2798. Loss: 0.23767197132110596\n",
      "Iteration: 2799. Loss: 0.32579776644706726\n",
      "Iteration: 2800. Loss: 0.2745683491230011\n",
      "Iteration: 2801. Loss: 0.2539442479610443\n",
      "Iteration: 2802. Loss: 0.2672627568244934\n",
      "Iteration: 2803. Loss: 0.2700347602367401\n",
      "Iteration: 2804. Loss: 0.2430974394083023\n",
      "Iteration: 2805. Loss: 0.318452924489975\n",
      "Iteration: 2806. Loss: 0.2847707271575928\n",
      "Iteration: 2807. Loss: 0.2698005437850952\n",
      "Iteration: 2808. Loss: 0.16657932102680206\n",
      "Iteration: 2809. Loss: 0.2492389678955078\n",
      "Iteration: 2810. Loss: 0.22928161919116974\n",
      "Iteration: 2811. Loss: 0.194292813539505\n",
      "Iteration: 2812. Loss: 0.20372138917446136\n",
      "Iteration: 2813. Loss: 0.3055853843688965\n",
      "Iteration: 2814. Loss: 0.31793639063835144\n",
      "Iteration: 2815. Loss: 0.2884785234928131\n",
      "Iteration: 2816. Loss: 0.29646971821784973\n",
      "Iteration: 2817. Loss: 0.3734554052352905\n",
      "Iteration: 2818. Loss: 0.3914202153682709\n",
      "Iteration: 2819. Loss: 0.27609026432037354\n",
      "Iteration: 2820. Loss: 0.2775121033191681\n",
      "Iteration: 2821. Loss: 0.22779598832130432\n",
      "Iteration: 2822. Loss: 0.26008614897727966\n",
      "Iteration: 2823. Loss: 0.4072151482105255\n",
      "Iteration: 2824. Loss: 0.2185794860124588\n",
      "Iteration: 2825. Loss: 0.37440910935401917\n",
      "Iteration: 2826. Loss: 0.2258652299642563\n",
      "Iteration: 2827. Loss: 0.16288091242313385\n",
      "Iteration: 2828. Loss: 0.260972797870636\n",
      "Iteration: 2829. Loss: 0.34787794947624207\n",
      "Iteration: 2830. Loss: 0.3207767903804779\n",
      "Iteration: 2831. Loss: 0.20824086666107178\n",
      "Iteration: 2832. Loss: 0.2830778956413269\n",
      "Iteration: 2833. Loss: 0.11620301008224487\n",
      "Iteration: 2834. Loss: 0.24686235189437866\n",
      "Iteration: 2835. Loss: 0.26701194047927856\n",
      "Iteration: 2836. Loss: 0.3377380669116974\n",
      "Iteration: 2837. Loss: 0.22782140970230103\n",
      "Iteration: 2838. Loss: 0.46919602155685425\n",
      "Iteration: 2839. Loss: 0.28691184520721436\n",
      "Iteration: 2840. Loss: 0.42058563232421875\n",
      "Iteration: 2841. Loss: 0.3006593585014343\n",
      "Iteration: 2842. Loss: 0.26096436381340027\n",
      "Iteration: 2843. Loss: 0.14809729158878326\n",
      "Iteration: 2844. Loss: 0.32876667380332947\n",
      "Iteration: 2845. Loss: 0.2606407403945923\n",
      "Iteration: 2846. Loss: 0.2703237235546112\n",
      "Iteration: 2847. Loss: 0.3616252839565277\n",
      "Iteration: 2848. Loss: 0.3020170331001282\n",
      "Iteration: 2849. Loss: 0.34547659754753113\n",
      "Iteration: 2850. Loss: 0.1545463353395462\n",
      "Iteration: 2851. Loss: 0.21975740790367126\n",
      "Iteration: 2852. Loss: 0.40864166617393494\n",
      "Iteration: 2853. Loss: 0.314780592918396\n",
      "Iteration: 2854. Loss: 0.36062467098236084\n",
      "Iteration: 2855. Loss: 0.21749067306518555\n",
      "Iteration: 2856. Loss: 0.24390220642089844\n",
      "Iteration: 2857. Loss: 0.32987186312675476\n",
      "Iteration: 2858. Loss: 0.35652533173561096\n",
      "Iteration: 2859. Loss: 0.1730511337518692\n",
      "Iteration: 2860. Loss: 0.2861660122871399\n",
      "Iteration: 2861. Loss: 0.23232044279575348\n",
      "Iteration: 2862. Loss: 0.387424498796463\n",
      "Iteration: 2863. Loss: 0.37198710441589355\n",
      "Iteration: 2864. Loss: 0.3994150161743164\n",
      "Iteration: 2865. Loss: 0.37421944737434387\n",
      "Iteration: 2866. Loss: 0.30661606788635254\n",
      "Iteration: 2867. Loss: 0.20813940465450287\n",
      "Iteration: 2868. Loss: 0.2648252248764038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2869. Loss: 0.39206817746162415\n",
      "Iteration: 2870. Loss: 0.2558484971523285\n",
      "Iteration: 2871. Loss: 0.3825396001338959\n",
      "Iteration: 2872. Loss: 0.30061739683151245\n",
      "Iteration: 2873. Loss: 0.28320834040641785\n",
      "Iteration: 2874. Loss: 0.19884587824344635\n",
      "Iteration: 2875. Loss: 0.2898329198360443\n",
      "Iteration: 2876. Loss: 0.31142300367355347\n",
      "Iteration: 2877. Loss: 0.34352126717567444\n",
      "Iteration: 2878. Loss: 0.213687464594841\n",
      "Iteration: 2879. Loss: 0.31334683299064636\n",
      "Iteration: 2880. Loss: 0.2539511024951935\n",
      "Iteration: 2881. Loss: 0.19010141491889954\n",
      "Iteration: 2882. Loss: 0.16403864324092865\n",
      "Iteration: 2883. Loss: 0.2793671488761902\n",
      "Iteration: 2884. Loss: 0.2089172601699829\n",
      "Iteration: 2885. Loss: 0.11987635493278503\n",
      "Iteration: 2886. Loss: 0.26790520548820496\n",
      "Iteration: 2887. Loss: 0.2256034016609192\n",
      "Iteration: 2888. Loss: 0.25386807322502136\n",
      "Iteration: 2889. Loss: 0.2719871699810028\n",
      "Iteration: 2890. Loss: 0.34889933466911316\n",
      "Iteration: 2891. Loss: 0.23438875377178192\n",
      "Iteration: 2892. Loss: 0.1829955130815506\n",
      "Iteration: 2893. Loss: 0.340791791677475\n",
      "Iteration: 2894. Loss: 0.4583057463169098\n",
      "Iteration: 2895. Loss: 0.2489454448223114\n",
      "Iteration: 2896. Loss: 0.17964449524879456\n",
      "Iteration: 2897. Loss: 0.3621881902217865\n",
      "Iteration: 2898. Loss: 0.13817982375621796\n",
      "Iteration: 2899. Loss: 0.2334166020154953\n",
      "Iteration: 2900. Loss: 0.2726536691188812\n",
      "Iteration: 2901. Loss: 0.3660646677017212\n",
      "Iteration: 2902. Loss: 0.24141159653663635\n",
      "Iteration: 2903. Loss: 0.2709122598171234\n",
      "Iteration: 2904. Loss: 0.3301534354686737\n",
      "Iteration: 2905. Loss: 0.23778919875621796\n",
      "Iteration: 2906. Loss: 0.30191782116889954\n",
      "Iteration: 2907. Loss: 0.2334488332271576\n",
      "Iteration: 2908. Loss: 0.3334665298461914\n",
      "Iteration: 2909. Loss: 0.2570321261882782\n",
      "Iteration: 2910. Loss: 0.24868221580982208\n",
      "Iteration: 2911. Loss: 0.30031657218933105\n",
      "Iteration: 2912. Loss: 0.2795089781284332\n",
      "Iteration: 2913. Loss: 0.32058581709861755\n",
      "Iteration: 2914. Loss: 0.1830015927553177\n",
      "Iteration: 2915. Loss: 0.17003461718559265\n",
      "Iteration: 2916. Loss: 0.17066192626953125\n",
      "Iteration: 2917. Loss: 0.3363312780857086\n",
      "Iteration: 2918. Loss: 0.3641479015350342\n",
      "Iteration: 2919. Loss: 0.3466476500034332\n",
      "Iteration: 2920. Loss: 0.19319063425064087\n",
      "Iteration: 2921. Loss: 0.26016709208488464\n",
      "Iteration: 2922. Loss: 0.393520712852478\n",
      "Iteration: 2923. Loss: 0.22498482465744019\n",
      "Iteration: 2924. Loss: 0.27901551127433777\n",
      "Iteration: 2925. Loss: 0.18281841278076172\n",
      "Iteration: 2926. Loss: 0.34222546219825745\n",
      "Iteration: 2927. Loss: 0.28844407200813293\n",
      "Iteration: 2928. Loss: 0.44692543148994446\n",
      "Iteration: 2929. Loss: 0.3088724911212921\n",
      "Iteration: 2930. Loss: 0.21962998807430267\n",
      "Iteration: 2931. Loss: 0.2365090548992157\n",
      "Iteration: 2932. Loss: 0.37229999899864197\n",
      "Iteration: 2933. Loss: 0.3112364709377289\n",
      "Iteration: 2934. Loss: 0.4029753804206848\n",
      "Iteration: 2935. Loss: 0.16552020609378815\n",
      "Iteration: 2936. Loss: 0.17930704355239868\n",
      "Iteration: 2937. Loss: 0.38344284892082214\n",
      "Iteration: 2938. Loss: 0.2833661139011383\n",
      "Iteration: 2939. Loss: 0.3699873089790344\n",
      "Iteration: 2940. Loss: 0.3057901859283447\n",
      "Iteration: 2941. Loss: 0.266755610704422\n",
      "Iteration: 2942. Loss: 0.2586195170879364\n",
      "Iteration: 2943. Loss: 0.2396865338087082\n",
      "Iteration: 2944. Loss: 0.2124129682779312\n",
      "Iteration: 2945. Loss: 0.2590075731277466\n",
      "Iteration: 2946. Loss: 0.2079371213912964\n",
      "Iteration: 2947. Loss: 0.18924422562122345\n",
      "Iteration: 2948. Loss: 0.30170416831970215\n",
      "Iteration: 2949. Loss: 0.267093688249588\n",
      "Iteration: 2950. Loss: 0.2564575672149658\n",
      "Iteration: 2951. Loss: 0.26456186175346375\n",
      "Iteration: 2952. Loss: 0.2773880660533905\n",
      "Iteration: 2953. Loss: 0.25307953357696533\n",
      "Iteration: 2954. Loss: 0.13471515476703644\n",
      "Iteration: 2955. Loss: 0.2992674708366394\n",
      "Iteration: 2956. Loss: 0.3374691307544708\n",
      "Iteration: 2957. Loss: 0.3276234567165375\n",
      "Iteration: 2958. Loss: 0.33711302280426025\n",
      "Iteration: 2959. Loss: 0.33709093928337097\n",
      "Iteration: 2960. Loss: 0.31504306197166443\n",
      "Iteration: 2961. Loss: 0.2651652991771698\n",
      "Iteration: 2962. Loss: 0.2343536764383316\n",
      "Iteration: 2963. Loss: 0.3848581314086914\n",
      "Iteration: 2964. Loss: 0.37414616346359253\n",
      "Iteration: 2965. Loss: 0.34862685203552246\n",
      "Iteration: 2966. Loss: 0.25162944197654724\n",
      "Iteration: 2967. Loss: 0.3514338433742523\n",
      "Iteration: 2968. Loss: 0.3068166971206665\n",
      "Iteration: 2969. Loss: 0.31378915905952454\n",
      "Iteration: 2970. Loss: 0.2221289426088333\n",
      "Iteration: 2971. Loss: 0.25641071796417236\n",
      "Iteration: 2972. Loss: 0.38260364532470703\n",
      "Iteration: 2973. Loss: 0.21969638764858246\n",
      "Iteration: 2974. Loss: 0.24031314253807068\n",
      "Iteration: 2975. Loss: 0.38737568259239197\n",
      "Iteration: 2976. Loss: 0.3347519338130951\n",
      "Iteration: 2977. Loss: 0.17676597833633423\n",
      "Iteration: 2978. Loss: 0.21299847960472107\n",
      "Iteration: 2979. Loss: 0.3565685749053955\n",
      "Iteration: 2980. Loss: 0.4079540967941284\n",
      "Iteration: 2981. Loss: 0.286891371011734\n",
      "Iteration: 2982. Loss: 0.3586376905441284\n",
      "Iteration: 2983. Loss: 0.3471231162548065\n",
      "Iteration: 2984. Loss: 0.4706380069255829\n",
      "Iteration: 2985. Loss: 0.315243124961853\n",
      "Iteration: 2986. Loss: 0.3942705988883972\n",
      "Iteration: 2987. Loss: 0.2854034900665283\n",
      "Iteration: 2988. Loss: 0.31645679473876953\n",
      "Iteration: 2989. Loss: 0.29557132720947266\n",
      "Iteration: 2990. Loss: 0.18493270874023438\n",
      "Iteration: 2991. Loss: 0.2982841730117798\n",
      "Iteration: 2992. Loss: 0.27204227447509766\n",
      "Iteration: 2993. Loss: 0.24697397649288177\n",
      "Iteration: 2994. Loss: 0.3192397356033325\n",
      "Iteration: 2995. Loss: 0.19952505826950073\n",
      "Iteration: 2996. Loss: 0.19755050539970398\n",
      "Iteration: 2997. Loss: 0.19686183333396912\n",
      "Iteration: 2998. Loss: 0.30193984508514404\n",
      "Iteration: 2999. Loss: 0.21568289399147034\n",
      "Iteration: 3000. Loss: 0.2841082513332367\n"
     ]
    }
   ],
   "source": [
    "# Training \n",
    "# convert input into variable \n",
    "iter=0 \n",
    "for epochs in range(num_epochs):\n",
    "    for i,(images,labels) in enumerate (train_loader): \n",
    "        images = Variable(images.view(-1,28*28))\n",
    "        labels = Variable(labels)\n",
    "        \n",
    "    # clear the gradients\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    # foward pass to obtain the outputs\n",
    "        outputs = model(images)\n",
    "    \n",
    "    #loss \n",
    "        loss = criterion(outputs,labels)\n",
    "    \n",
    "    #backpropagation \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    \n",
    "        iter+=1\n",
    "\n",
    "        print('Iteration: {}. Loss: {}'.format(iter, loss.data[0]))\n",
    "\n",
    "        \n",
    "        images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%Iteration: 3000. Loss: 0.2841082513332367. Accuracy: 91.96\n"
     ]
    }
   ],
   "source": [
    "if iter %500 == 0:\n",
    "            # Calculate Accuracy         \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through test dataset\n",
    "            for images, labels in test_loader:\n",
    "               \n",
    "                images = Variable(images.view(-1, 28*28))\n",
    "                \n",
    "                # Forward pass only to get logits/output\n",
    "                outputs = model(images)\n",
    "                \n",
    "                # Get predictions from the maximum value\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                \n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "                \n",
    "                correct += (predicted.cpu() == labels.cpu()).sum()\n",
    "            \n",
    "            accuracy = 100 * correct / total\n",
    "        \n",
    "            # Print Loss\n",
    "            print('%Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.data[0], accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
